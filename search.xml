<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Django初学教程（一）</title>
    <url>/2024/01/03/Django%E5%88%9D%E5%AD%A6%E6%95%99%E7%A8%8B%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"><h1 id="创建项目">创建项目</h1>
<ol type="1">
<li>创建虚拟环境</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create -n testDjango python=3.8</span><br><span class="line">conda activate testDjango</span><br><span class="line">pip install Django # 注意这里Django首字母大写</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>创建项目</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd D:\PythonCode</span><br><span class="line">django-admin startproject mysite</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<p>创建成功的项目结构为：</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysite/</span><br><span class="line">    manage.py</span><br><span class="line">    mysite/</span><br><span class="line">        __init__.py</span><br><span class="line">        settings.py</span><br><span class="line">        urls.py</span><br><span class="line">        asgi.py</span><br><span class="line">        wsgi.py</span><br></pre></td></tr></table></figure>
</blockquote>
<p>启动服务器：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd mysite</span><br><span class="line">python manage.py runserver</span><br></pre></td></tr></table></figure>
<h1 id="创建应用">创建应用</h1>
<p>项目=（一个及以上的）应用程序 + 配置</p>
<h2 id="创建应用-1">创建应用</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 项目名称为&quot;polls&quot;</span><br><span class="line">python manage.py startapp polls</span><br></pre></td></tr></table></figure>
<p>创建成功的应用结构为：</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">polls/</span><br><span class="line">    __init__.py</span><br><span class="line">    admin.py</span><br><span class="line">    apps.py</span><br><span class="line">    migrations/</span><br><span class="line">        __init__.py</span><br><span class="line">    models.py</span><br><span class="line">    tests.py</span><br><span class="line">    views.py</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="创建首页">创建首页</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 修改polls/views.py，创建index view</span><br><span class="line"></span><br><span class="line">from django.http import HttpResponse</span><br><span class="line"></span><br><span class="line">def index(request):</span><br><span class="line">    return HttpResponse(&quot;Hello, world. You&#x27;re at the polls index.&quot;)</span><br></pre></td></tr></table></figure>
<h2 id="配置url">配置url</h2>
<h3 id="应用的url配置文件">应用的url配置文件</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 新增polls/urls.py, 用于配置应用&quot;polls&quot;下的url</span><br><span class="line"></span><br><span class="line">from django.urls import path</span><br><span class="line"></span><br><span class="line">from . import views</span><br><span class="line"></span><br><span class="line"># 设置应用程序命名空间</span><br><span class="line"># 在真正的 Django 项目中，可能有五个、十个、二十个或更多应用程序。Django 如何区分它们之间的 URL 名称？——即将命名空间app_name = &quot;polls&quot;添加到 URLconf 中</span><br><span class="line"></span><br><span class="line">app_name = &quot;polls&quot;</span><br><span class="line">urlpatterns = [</span><br><span class="line">    path(&quot;&quot;, views.index, name=&quot;index&quot;),</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h3 id="项目的url配置文件">项目的url配置文件</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 修改mysite/urls.py，将应用&quot;polls&quot;下的urls.py引入</span><br><span class="line"></span><br><span class="line">from django.contrib import admin</span><br><span class="line">from django.urls import include, path</span><br><span class="line"></span><br><span class="line">urlpatterns = [</span><br><span class="line">    path(&quot;polls/&quot;, include(&quot;polls.urls&quot;)),</span><br><span class="line">    path(&quot;admin/&quot;, admin.site.urls),</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>启动服务器：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python manage.py runserver</span><br></pre></td></tr></table></figure>
<p>启动服务器后访问链接：http://127.0.0.1:8000/polls/，即可访问到index
view</p>
<h1 id="连接database">连接Database</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 在项目的settings.py(即mysite/settings.py)中修改数据库配置</span><br><span class="line"></span><br><span class="line"># 修改数据库配置——更改为mysql数据库</span><br><span class="line">DATABASES = &#123;</span><br><span class="line">    &#x27;default&#x27;: &#123;</span><br><span class="line">        &#x27;ENGINE&#x27;: &#x27;django.db.backends.mysql&#x27;,</span><br><span class="line">        &#x27;NAME&#x27;: &#x27;db_test&#x27;,</span><br><span class="line">        &quot;USER&quot;: &quot;root&quot;,</span><br><span class="line">        &quot;PASSWORD&quot;: &quot;123456xjy&quot;,</span><br><span class="line">        &quot;HOST&quot;: &quot;127.0.0.1&quot;,</span><br><span class="line">        &quot;PORT&quot;: &quot;3306&quot;,</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 首次连接mysql数据库需要在虚拟环境导入以下2个包</span><br><span class="line">pip install pymysql</span><br><span class="line">pip install mysqlclient</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 执行数据迁移——将&quot;mysite/settings.py&quot;中INSTALLED_APPS对应的数据表都创建出来（创建到上述的&quot;db_test&quot;数据库中）</span><br><span class="line">python manage.py migrate</span><br></pre></td></tr></table></figure>
<h1 id="建立model">建立Model</h1>
<h2 id="步骤">步骤</h2>
<ul>
<li>Change your models (in <code>models.py</code>).</li>
<li>Run <code>python manage.py makemigrations</code>to create migrations
for those changes</li>
<li>Run <code>python manage.py migrate</code>to apply those changes to
the database.</li>
</ul>
<h2 id="change-your-models">Change your models</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 修改polls/models.py，创建类</span><br><span class="line"> </span><br><span class="line">from django.db import models</span><br><span class="line"></span><br><span class="line">class Question(models.Model):</span><br><span class="line">    question_text = models.CharField(max_length=200)</span><br><span class="line">    pub_date = models.DateTimeField(&quot;date published&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Choice(models.Model):</span><br><span class="line">    question = models.ForeignKey(Question, on_delete=models.CASCADE)</span><br><span class="line">    choice_text = models.CharField(max_length=200)</span><br><span class="line">    votes = models.IntegerField(default=0)</span><br></pre></td></tr></table></figure>
<h2 id="make-migrations">make migrations</h2>
<ol type="1">
<li>将应用"polls"放到项目的setting.py（即mysite/settings.py）的INSTALLED_APPS中</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 修改mysite/settings.py，增加一行&quot;polls.apps.PollsConfig&quot;,</span><br><span class="line"></span><br><span class="line">INSTALLED_APPS = [</span><br><span class="line">    &quot;polls.apps.PollsConfig&quot;,</span><br><span class="line">    &quot;django.contrib.admin&quot;,</span><br><span class="line">    &quot;django.contrib.auth&quot;,</span><br><span class="line">    &quot;django.contrib.contenttypes&quot;,</span><br><span class="line">    &quot;django.contrib.sessions&quot;,</span><br><span class="line">    &quot;django.contrib.messages&quot;,</span><br><span class="line">    &quot;django.contrib.staticfiles&quot;,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>make migrations</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python manage.py makemigrations polls</span><br></pre></td></tr></table></figure>
<ol start="3" type="1">
<li>migrate</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 运行以下代码，即可将models.py中新创建的类Question和类Choice，在数据库中创建出对应的数据表</span><br><span class="line">python manage.py migrate</span><br></pre></td></tr></table></figure>
<ol start="4" type="1">
<li>进入Python环境</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python manage.py shell</span><br></pre></td></tr></table></figure>
<p>We’re using this instead of simply typing “python”, because
<code>manage.py</code> sets the <a
href="https://docs.djangoproject.com/en/4.2/topics/settings/#envvar-DJANGO_SETTINGS_MODULE"><code>DJANGO_SETTINGS_MODULE</code></a>
environment variable, which gives Django the Python import path to your
<code>mysite/settings.py</code> file.</p>
<h1 id="管理后台">管理后台</h1>
<ol type="1">
<li>创建超级管理员</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python manage.py createsuperuser</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>运行服务器</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python manage.py runserver</span><br></pre></td></tr></table></figure>
<ol start="3" type="1">
<li>将自定义的类注册到管理后台</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 修改polls/admin.py，将新注册的类Question注册到管理后台</span><br><span class="line"></span><br><span class="line">from django.contrib import admin</span><br><span class="line">from .models import Question</span><br><span class="line"></span><br><span class="line">admin.site.register(Question)</span><br></pre></td></tr></table></figure>
<h1 id="增加views">增加Views</h1>
<ol type="1">
<li>新增视图</li>
</ol>
<p>每一个View就干两件事中的一件：要么返回一个HttpResponse，要么返回一个Exception（如Http404）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 在polls/views.py新增以下函数（视图）</span><br><span class="line"># 以下均为虚拟实现</span><br><span class="line"></span><br><span class="line">def detail(request, question_id):</span><br><span class="line">    return HttpResponse(&quot;You&#x27;re looking at question %s.&quot; % question_id)</span><br><span class="line"></span><br><span class="line">def results(request, question_id):</span><br><span class="line">    response = &quot;You&#x27;re looking at the results of question %s.&quot;</span><br><span class="line">    return HttpResponse(response % question_id)</span><br><span class="line"></span><br><span class="line">def vote(request, question_id):</span><br><span class="line">    return HttpResponse(&quot;You&#x27;re voting on question %s.&quot; % question_id)</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>配置应用url</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 将新增的view写入pools/urls.py</span><br><span class="line"></span><br><span class="line">from django.urls import path</span><br><span class="line"></span><br><span class="line">from . import views</span><br><span class="line"></span><br><span class="line">urlpatterns = [</span><br><span class="line">    # ex: /polls/</span><br><span class="line">    path(&quot;&quot;, views.index, name=&quot;index&quot;),</span><br><span class="line">    # ex: /polls/5/</span><br><span class="line">    path(&quot;&lt;int:question_id&gt;/&quot;, views.detail, name=&quot;detail&quot;),</span><br><span class="line">    # ex: /polls/5/results/</span><br><span class="line">    path(&quot;&lt;int:question_id&gt;/results/&quot;, views.results, name=&quot;results&quot;),</span><br><span class="line">    # ex: /polls/5/vote/</span><br><span class="line">    path(&quot;&lt;int:question_id&gt;/vote/&quot;, views.vote, name=&quot;vote&quot;),</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<ol start="3" type="1">
<li>用户访问视图的运行流程</li>
</ol>
<p>请求页面"/polls/6/"=&gt;加载mysite.urls=&gt;找到变量"polls/"命名的urlpatterns，即<code>path("polls/", include("polls.urls"))</code>=&gt;去掉"polls/"，剩余"6/"=&gt;把剩余的"6/"发送给"<code>polls.urls</code>"
URLconf=&gt;匹配到了<code>path("&lt;int:question_id&gt;/", views.detail, name="detail")</code>=&gt;导致调用了<code>detail()</code>
视图(<code>views.py中的detail()</code>)，具体调用如<code>detail(request=&lt;HttpRequest object&gt;, question_id=34)</code></p>
<h1 id="修改views">修改Views</h1>
<h2 id="index-view">index view</h2>
<p>上述增加视图中的视图使用了虚拟实现，并没有从数据库中读取记录，或完成其他有实际意义的任务，因此在这一板块将编写实际执行某些操作的视图。</p>
<ol type="1">
<li>修改视图(index view)</li>
</ol>
<p>法1：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># polls/views.py</span><br><span class="line"># 修改index view (index函数)，使用模板(利用HttpResponse和loader)，读取数据库数据</span><br><span class="line"># 加载调用的模板 polls/index.html并向其传递context。context是将模板变量名称映射到 Python 对象的字典。</span><br><span class="line">from django.http import HttpResponse</span><br><span class="line">from django.template import loader</span><br><span class="line"></span><br><span class="line">from .models import Question</span><br><span class="line"></span><br><span class="line">def index(request):</span><br><span class="line">    latest_question_list = Question.objects.order_by(&quot;-pub_date&quot;)[:5]</span><br><span class="line">    template = loader.get_template(&quot;polls/index.html&quot;)</span><br><span class="line">    context = &#123;</span><br><span class="line">        &quot;latest_question_list&quot;: latest_question_list,</span><br><span class="line">    &#125;</span><br><span class="line">    return HttpResponse(template.render(context, request))</span><br></pre></td></tr></table></figure>
<p>法2：</p>
<p>利用render()函数代替HttpResponse和loader，简化代码</p>
<p><code>render()</code>函数第一个参数是请求对象，第二个参数是模板名称，第三个参数是可选的字典。它返回一个<code>HttpResponse</code>对象，这是一个使用给定context的模板对象。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># polls/views.py</span><br><span class="line"></span><br><span class="line">from django.shortcuts import render</span><br><span class="line"></span><br><span class="line">from .models import Question</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def index(request):</span><br><span class="line">    latest_question_list = Question.objects.order_by(&quot;-pub_date&quot;)[:5]</span><br><span class="line">    context = &#123;&quot;latest_question_list&quot;: latest_question_list&#125;</span><br><span class="line">    return render(request, &quot;polls/index.html&quot;, context)</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>创建模板(index.html)</li>
</ol>
<p>如果页面的设计硬编码在视图中，那么当我们想要更改页面的外观时，必须编辑此
Python
代码（即views.py中的函数）。因此我们使用Django的模板系统，通过创建模板，使得页面设计与Python代码分离。</p>
<p>在应用的目录下创建templates目录（即polls/templates/），Django会默认在这个目录下寻找模板。</p>
<p>创建好模板文件后的目录结构如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 这里还在templates目录下创建了一个polls子目录，在子目录下才放置了模板文件index.html</span><br><span class="line"># 这里新建的polls子目录起到划分命名空间的作用。因为如果在不同的应用程序中具有相同名称的模板，Django 将无法区分它们，所以需要设置命名空间。</span><br><span class="line"></span><br><span class="line">polls/</span><br><span class="line">    __init__.py</span><br><span class="line">    admin.py</span><br><span class="line">    models.py</span><br><span class="line">    views.py</span><br><span class="line">    templates</span><br><span class="line">    	polls</span><br><span class="line">    		index.html</span><br></pre></td></tr></table></figure>
<p>index.html的文件内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># polls/templates/polls/index.html</span><br><span class="line"></span><br><span class="line">&#123;% if latest_question_list %&#125;</span><br><span class="line">    &lt;ul&gt;</span><br><span class="line">    &#123;% for question in latest_question_list %&#125;</span><br><span class="line">        &lt;li&gt;&lt;a href=&quot;/polls/&#123;&#123; question.id &#125;&#125;/&quot;&gt;&#123;&#123; question.question_text &#125;&#125;&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">    &#123;% endfor %&#125;</span><br><span class="line">    &lt;/ul&gt;</span><br><span class="line">&#123;% else %&#125;</span><br><span class="line">    &lt;p&gt;No polls are available.&lt;/p&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
<ol start="3" type="1">
<li>移除模板文件中的硬编码URL</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 修改前</span><br><span class="line">&lt;li&gt;&lt;a href=&quot;/polls/&#123;&#123; question.id &#125;&#125;/&quot;&gt;&#123;&#123; question.question_text &#125;&#125;&lt;/a&gt;&lt;/li&gt;</span><br><span class="line"></span><br><span class="line"># 修改后</span><br><span class="line">&lt;li&gt;&lt;a href=&quot;&#123;% url &#x27;detail&#x27; question.id %&#125;&quot;&gt;&#123;&#123; question.question_text &#125;&#125;&lt;/a&gt;&lt;/li&gt;</span><br><span class="line"></span><br><span class="line"># 修改原因：在polls/urls.py中，views.py的detail()函数路径被注册为</span><br><span class="line">path(&quot;&lt;int:question_id&gt;/&quot;, views.detail, name=&quot;detail&quot;),其中name属性为&quot;detail&quot;</span><br><span class="line"></span><br><span class="line"># 继续修改：在polls/urls.py中设置应用程序命名空间(app_name = &quot;polls&quot;)后,继续修改为指向命名空间的详细视图</span><br><span class="line">&lt;li&gt;&lt;a href=&quot;&#123;% url &#x27;polls:detail&#x27; question.id %&#125;&quot;&gt;&#123;&#123; question.question_text &#125;&#125;&lt;/a&gt;&lt;/li&gt;</span><br></pre></td></tr></table></figure>
<h2 id="detail-view">detail view</h2>
<ol type="1">
<li>修改视图(detail view)</li>
</ol>
<p>使用get方法特别容易产生404错误，因此产生了一个快捷方式，即使用<code>get_object_or_404()</code>来进行查询。<code>get_object_or_404()</code>第一个参数是a
Django
model，第二个参数是任意数量的关键词参数。如果查询对象存在，则返回该模型对象；如果对象不存在，则返回Http404。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># polls/views.py</span><br><span class="line"></span><br><span class="line">from django.shortcuts import get_object_or_404, render</span><br><span class="line"></span><br><span class="line">from .models import Question</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def detail(request, question_id):</span><br><span class="line">    question = get_object_or_404(Question, pk=question_id)</span><br><span class="line">    return render(request, &quot;polls/detail.html&quot;, &#123;&quot;question&quot;: question&#125;)</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>创建模板(detail.html)</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;form action=&quot;&#123;% url &#x27;polls:vote&#x27; question.id %&#125;&quot; method=&quot;post&quot;&gt;</span><br><span class="line">&#123;% csrf_token %&#125;</span><br><span class="line">&lt;fieldset&gt;</span><br><span class="line">    &lt;legend&gt;&lt;h1&gt;&#123;&#123; question.question_text &#125;&#125;&lt;/h1&gt;&lt;/legend&gt;</span><br><span class="line">    &#123;% if error_message %&#125;&lt;p&gt;&lt;strong&gt;&#123;&#123; error_message &#125;&#125;&lt;/strong&gt;&lt;/p&gt;&#123;% endif %&#125;</span><br><span class="line">    &#123;% for choice in question.choice_set.all %&#125;</span><br><span class="line">        &lt;input type=&quot;radio&quot; name=&quot;choice&quot; id=&quot;choice&#123;&#123; forloop.counter &#125;&#125;&quot; value=&quot;&#123;&#123; choice.id &#125;&#125;&quot;&gt;</span><br><span class="line">        &lt;label for=&quot;choice&#123;&#123; forloop.counter &#125;&#125;&quot;&gt;&#123;&#123; choice.choice_text &#125;&#125;&lt;/label&gt;&lt;br&gt;</span><br><span class="line">    &#123;% endfor %&#125;</span><br><span class="line">&lt;/fieldset&gt;</span><br><span class="line">&lt;input type=&quot;submit&quot; value=&quot;Vote&quot;&gt;</span><br><span class="line">&lt;/form&gt;</span><br></pre></td></tr></table></figure>
<ol start="3" type="1">
<li>2中的vote()函数还需要具体实现</li>
</ol>
<p>后续内容可以参考官方文档https://docs.djangoproject.com/en/5.0/intro/tutorial04/</p>
]]></content>
      <categories>
        <category>开发技能</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title>Django搭建敏感图片检测系统（二）</title>
    <url>/2024/01/04/Django%E6%90%AD%E5%BB%BA%E6%95%8F%E6%84%9F%E5%9B%BE%E7%89%87%E6%A3%80%E6%B5%8B%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"><h1 id="项目需求">项目需求</h1>
<p>目前，我已经实现了一个函数<code>detect_sensitive(img_path)</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def detect_sensitive(img_path):</span><br><span class="line">	# 1.加载训练好的检测和识别模型，获取图片中的文字，保存在sentence中，并设置初始敏感概率为0.12</span><br><span class="line">	# 2.初始化trie树，检查sentence中是否包含敏感词，若包含，将敏感概率设置为0.4592</span><br><span class="line">	# 3.加载TextCNN模型，获取textCNN模型得出的sentence的预测结果，若预测为敏感信息，将敏感概率增加0.3</span><br><span class="line">	# 3.1加载 word_2_index 字典和词嵌入层</span><br><span class="line">	# 3.2加载 TextCNN 模型</span><br><span class="line">	# 3.3对文本进行预处理，并使用 TextCNN 模型进行预测</span><br><span class="line">	# 4.最终判断该图片包含的文本是否为敏感信息isSensitive</span><br><span class="line">	# 5.返回sentence，isSensitive</span><br><span class="line">	return sentence, isSensitive</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<p>它的作用是：传入一张图片，先利用训练好的检测和识别模型识别出图片中的文字，再判断文字是否为敏感信息。最终的返回值有2个，分别为<code>sentence</code>和<code>isSensitive</code>，<code>sentence</code>代表图片中的文字，<code>isSensitive</code>代表这张图片所含的文字是否为敏感信息。</p>
<p>我希望借助这个函数搭建一个”敏感图片检测系统“，系统的需求为</p>
<ul>
<li>上传一张图片</li>
<li>首先，在”白名单表“中查询这张图片是否存在，如果存在，直接将”白名单表“中的<code>statement</code>和<code>isSensitive</code>字段返回；如果不存在，就将图片传入<code>detect_sensitive()</code>进行检查，得到返回值<code>statement</code>和<code>isSensitive</code></li>
<li>其次，如果<code>isSensitive</code>为True，返回告警信息，并将告警信息存入一张”告警信息表“，同时将这张图片和它对应的敏感类型存入”白名单表“中；如果<code>isSensitive</code>为False，返回正常信息，并将这张图片和它对应的敏感类型存入”白名单表“中。</li>
</ul>
<p>数据库的物理结构设计如下：</p>
<p>”白名单表“用于存储用户上传的图片，白名单里的图片无需通过敏感信息检测模块，表内容包含编号、图片MD5值、<code>statement</code>、<code>isSensitive</code>。</p>
<p>表1 白名单表结构</p>
<table>
<thead>
<tr class="header">
<th>列名</th>
<th>数据类型</th>
<th>主键</th>
<th>注释</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>id</td>
<td>bigint</td>
<td>Primary Key</td>
<td>记录ID</td>
</tr>
<tr class="even">
<td>img_md5</td>
<td>varchar(32)</td>
<td></td>
<td>图片MD5值</td>
</tr>
<tr class="odd">
<td>isSensitive</td>
<td>tinyint(1)</td>
<td></td>
<td>图片是否敏感</td>
</tr>
<tr class="even">
<td>statement</td>
<td>longtext</td>
<td></td>
<td>图片中的文字</td>
</tr>
</tbody>
</table>
<p>”告警信息表“包含以下字段：信息编号、图片路径、图片MD5值、<code>statement</code>、<code>isSensitive</code>。</p>
<p>表2 告警信息表结构</p>
<table>
<thead>
<tr class="header">
<th>列名</th>
<th>数据类型</th>
<th>主键</th>
<th>注释</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>id</td>
<td>bigint</td>
<td>Primary Key</td>
<td>信息ID</td>
</tr>
<tr class="even">
<td>img_path</td>
<td>varchar(255)</td>
<td></td>
<td>图片路径</td>
</tr>
<tr class="odd">
<td>img_md5</td>
<td>varchar(32)</td>
<td></td>
<td>图片MD5值</td>
</tr>
<tr class="even">
<td>statement</td>
<td>longtext</td>
<td></td>
<td>图片中的文字</td>
</tr>
<tr class="odd">
<td>isSensitive</td>
<td>tinyint(1)</td>
<td></td>
<td>图片是否敏感</td>
</tr>
</tbody>
</table>
<p>我计划后端使用Django框架实现，前端使用html、css及JavaScript实现。</p>
<h1 id="创建项目">创建项目</h1>
<ol type="1">
<li>创建虚拟环境</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create -n sensitiveFilter python=3.8</span><br><span class="line">conda activate sensitiveFilter</span><br><span class="line">pip install Django # 注意这里Django首字母大写</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>创建项目</li>
</ol>
<p>项目名为<code>sensi_img_filter</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd D:\PythonCode</span><br><span class="line">django-admin startproject sensi_img_filter</span><br></pre></td></tr></table></figure>
<p>创建成功的项目结构为：</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sensi_img_filter/</span><br><span class="line"> manage.py</span><br><span class="line"> sensi_img_filter/</span><br><span class="line">     __init__.py</span><br><span class="line">     settings.py</span><br><span class="line">     urls.py</span><br><span class="line">     asgi.py</span><br><span class="line">     wsgi.py</span><br></pre></td></tr></table></figure>
</blockquote>
<p>启动服务器：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd sensi_img_filter # cd到外层的项目文件夹下</span><br><span class="line">python manage.py runserver # 启动服务器</span><br></pre></td></tr></table></figure>
<p>项目=（一个及以上的）应用程序 + 配置</p>
<h1 id="创建应用">创建应用</h1>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 应用名称为&quot;detector&quot;</span><br><span class="line">python manage.py startapp detector</span><br></pre></td></tr></table></figure>
<p>创建成功的应用结构为：</p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">detector/</span><br><span class="line"> __init__.py</span><br><span class="line"> admin.py</span><br><span class="line"> apps.py</span><br><span class="line"> migrations/</span><br><span class="line">     __init__.py</span><br><span class="line"> models.py</span><br><span class="line"> tests.py</span><br><span class="line"> views.py</span><br></pre></td></tr></table></figure>
</blockquote>
<h1 id="注册应用">注册应用</h1>
<p>在创建应用之后，我们需要将应用注册到项目中，让项目知道自己又多了一个应用，同时方便后期的数据迁移。将应用”<code>detector</code>”放到项目的setting.py（即<code>sensi_img_filter/settings.py</code>）的<code>INSTALLED_APPS</code>中</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 修改mysite/settings.py，增加一行&#x27;detector.apps.DetectorConfig&#x27;,完成注册</span><br><span class="line"></span><br><span class="line">INSTALLED_APPS = [</span><br><span class="line">    &#x27;detector.apps.DetectorConfig&#x27;,</span><br><span class="line">    &#x27;django.contrib.admin&#x27;,</span><br><span class="line">    &#x27;django.contrib.auth&#x27;,</span><br><span class="line">    &#x27;django.contrib.contenttypes&#x27;,</span><br><span class="line">    &#x27;django.contrib.sessions&#x27;,</span><br><span class="line">    &#x27;django.contrib.messages&#x27;,</span><br><span class="line">    &#x27;django.contrib.staticfiles&#x27;,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h1 id="初始配置">初始配置</h1>
<h2
id="引入应用配置文件到项目配置文件中">引入应用配置文件到项目配置文件中</h2>
<h3
id="新建应用配置文件用于配置应用detector下的url">新建应用配置文件，用于配置应用"detector"下的url</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 新增detector/urls.py, 用于配置应用&quot;detector&quot;下的url</span><br><span class="line"></span><br><span class="line">from django.urls import path</span><br><span class="line"></span><br><span class="line"># 设置应用程序命名空间</span><br><span class="line"># 在真正的 Django 项目中，可能有五个、十个、二十个或更多应用程序。Django 如何区分它们之间的 URL 名称？——即将命名空间app_name = &quot;detector&quot;添加到 URLconf 中</span><br><span class="line"></span><br><span class="line">app_name = &quot;detector&quot;</span><br><span class="line">urlpatterns = [</span><br><span class="line">    path(),</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h3
id="修改项目配置文件引入应用配置文件">修改项目配置文件，引入应用配置文件</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 修改sensi_img_filter/urls.py，将应用&quot;polls&quot;下的urls.py引入</span><br><span class="line"></span><br><span class="line">from django.contrib import admin</span><br><span class="line">from django.urls import include, path</span><br><span class="line"></span><br><span class="line">urlpatterns = [</span><br><span class="line">    path(&quot;detector/&quot;, include(&quot;detector.urls&quot;)),</span><br><span class="line">    path(&quot;admin/&quot;, admin.site.urls),</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<h3 id="测试配置文件是否正确引入">测试配置文件是否正确引入</h3>
<ol type="1">
<li><p>创建index view</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 修改detecor/views.py，创建index view(仅为虚拟实现)</span><br><span class="line"></span><br><span class="line">from django.http import HttpResponse</span><br><span class="line"></span><br><span class="line">def index(request):</span><br><span class="line">    return HttpResponse(&quot;Hello, world. You&#x27;re at the polls index.&quot;)</span><br></pre></td></tr></table></figure></li>
<li><p>在应用配置文件<code>detector/urls.py</code>中添加url配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 在detector/urls.py中配置index view的url</span><br><span class="line"></span><br><span class="line">from django.urls import path</span><br><span class="line"></span><br><span class="line">from . import views</span><br><span class="line"></span><br><span class="line">app_name = &quot;detector&quot;</span><br><span class="line">urlpatterns = [</span><br><span class="line">    # ex:/detector/</span><br><span class="line">    # path(&quot;&quot;, views.index, name=&quot;index&quot;),</span><br><span class="line">]</span><br></pre></td></tr></table></figure></li>
<li><p>启动服务器后访问链接：http://127.0.0.1:8000/detector/，即可访问到index
view，说明应用的配置文件已经正确引入到项目配置文件中</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python manage.py runserver # 启动服务器</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="连接mysql数据库">连接MySQL数据库</h2>
<ol type="1">
<li><p>导入依赖</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 首次连接mysql数据库需要在虚拟环境导入以下2个包</span><br><span class="line">pip install pymysql</span><br><span class="line">pip install mysqlclient</span><br></pre></td></tr></table></figure></li>
<li><p>修改项目的数据库配置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 在项目的settings.py(即sensi_img_filter/settings.py)中修改数据库配置</span><br><span class="line"></span><br><span class="line"># 修改数据库配置——更改为mysql数据库</span><br><span class="line">DATABASES = &#123;</span><br><span class="line">    &#x27;default&#x27;: &#123;</span><br><span class="line">        &#x27;ENGINE&#x27;: &#x27;django.db.backends.mysql&#x27;,</span><br><span class="line">        &#x27;NAME&#x27;: &#x27;info_filter&#x27;, # 修改为数据库的名字</span><br><span class="line">        &quot;USER&quot;: &quot;root&quot;,</span><br><span class="line">        &quot;PASSWORD&quot;: &quot;123456xjy&quot;,</span><br><span class="line">        &quot;HOST&quot;: &quot;127.0.0.1&quot;,</span><br><span class="line">        &quot;PORT&quot;: &quot;3306&quot;,</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>数据迁移</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 执行数据迁移——将&quot;sensi_img_filter/settings.py&quot;中INSTALLED_APPS对应的数据表都创建出来（创建到上述的&quot;info_filter&quot;数据库中）</span><br><span class="line">python manage.py migrate</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="修改静态文件默认位置">修改静态文件默认位置</h2>
<p>默认我们会将整个项目（可能包含多个应用）的静态文件（如css、js、images等）都放到项目路径下的static目录下，因此修改项目的<code>setting.py</code>（即<code>sensi_img_filter/setting.py</code>）中对静态文件默认路径的配置.</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 修改前</span><br><span class="line"># STATIC_URL = &#x27;static/&#x27;</span><br><span class="line"></span><br><span class="line"># 修改后</span><br><span class="line">STATIC_URL = &#x27;/static/&#x27;</span><br><span class="line">STATICFILES_DIRS = [BASE_DIR / &#x27;static&#x27;]</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZoQ7a.png" alt="OZoQ7a.png" />
<figcaption aria-hidden="true">OZoQ7a.png</figcaption>
</figure>
<h1 id="具体实现">具体实现</h1>
<h2 id="创建模型">创建模型</h2>
<h3 id="创建模型-1">创建模型</h3>
<p>在<code>detector/models.py</code>中创建模型</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from django.db import models</span><br><span class="line"></span><br><span class="line"># Create your models here.</span><br><span class="line">class WhiteTableModel(models.Model):</span><br><span class="line">    img_md5 = models.CharField(max_length=32)</span><br><span class="line">    isSensitive = models.BooleanField()</span><br><span class="line">    statement = models.TextField(max_length=500)</span><br><span class="line"></span><br><span class="line">class InfoTableModel(models.Model):</span><br><span class="line">    img_path = models.CharField(max_length=255)</span><br><span class="line">    img_md5 = models.CharField(max_length=32)</span><br><span class="line">    statement = models.TextField(max_length=500)</span><br><span class="line">    isSensitive = models.BooleanField()</span><br></pre></td></tr></table></figure>
<h3 id="数据迁移">数据迁移</h3>
<ol type="1">
<li><p>产生迁移文件（可以类比为生成SQL代码）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python manage.py makemigrations detector # 这里能执行成功多亏已经在项目的setting.py的INSTALLED_APPS中注册了新应用detector</span><br></pre></td></tr></table></figure></li>
<li><p>执行迁移（类比为执行SQL代码）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 运行以下代码，即可将models.py中新创建的类，在数据库中创建出对应的数据表</span><br><span class="line">python manage.py migrate</span><br></pre></td></tr></table></figure></li>
<li><p>如果在后期某张数据表需要发生变动，那么直接修改models.py中该类的定义，再次执行步骤1和2即可在数据表同步这一变动</p></li>
</ol>
<h2 id="创建视图">创建视图</h2>
<h3 id="创建视图函数">创建视图函数</h3>
<p>在这里创建了<code>form_view()</code>和<code>handle_upload()</code>两个视图函数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from django.shortcuts import render</span><br><span class="line">from django.http import JsonResponse</span><br><span class="line">from .models import ImageModel, WhiteTableModel, InfoTableModel</span><br><span class="line">from .utils.utils import detect_sensitive, calculate_md5, save_image</span><br><span class="line"></span><br><span class="line">def form_view(request):</span><br><span class="line">    return render(request, &#x27;detector/form.html&#x27;)</span><br><span class="line"></span><br><span class="line">def handle_upload(request):</span><br><span class="line">    if request.method == &quot;POST&quot;:</span><br><span class="line">        img_file = request.FILES[&#x27;image&#x27;]</span><br><span class="line">        if img_file:</span><br><span class="line">            img_md5 = calculate_md5(img_file)</span><br><span class="line"></span><br><span class="line">            white_obj = WhiteTableModel.objects.filter(img_md5=img_md5).first()</span><br><span class="line">            if white_obj:</span><br><span class="line">                context = &#123;&#x27;message&#x27;: white_obj.statement, &#x27;isSensitive&#x27;: white_obj.isSensitive&#125;</span><br><span class="line">                # return render(request, &quot;detector/upload.html&quot;, &#123;&#x27;context&#x27;: context&#125;)</span><br><span class="line">            else:</span><br><span class="line">                # 将图片保存至某个路径供detect_sensitive使用</span><br><span class="line">                img_path = save_image(img_file, img_md5)</span><br><span class="line"></span><br><span class="line">                statement, isSensitive = detect_sensitive(img_path)</span><br><span class="line"></span><br><span class="line">                # 如果被识别为敏感图片</span><br><span class="line">                if isSensitive == True:</span><br><span class="line">                    # 创建一个信息记录并保存</span><br><span class="line">                    InfoTableModel.objects.create(</span><br><span class="line">                        img_path=img_path,</span><br><span class="line">                        img_md5=img_md5,</span><br><span class="line">                        statement=statement,</span><br><span class="line">                        isSensitive=isSensitive</span><br><span class="line">                    )</span><br><span class="line"></span><br><span class="line">                # 不论是敏感图片还是非敏感图片，都要加入白名单</span><br><span class="line">                WhiteTableModel.objects.create(</span><br><span class="line">                    img_md5=img_md5,</span><br><span class="line">                    isSensitive=isSensitive,</span><br><span class="line">                    statement=statement</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">                # 如果被识别为敏感图片，则返回告警信息，否则返回正常信息</span><br><span class="line">                # if isSensitive == True:</span><br><span class="line">                #     context = &#123;&#x27;message&#x27;: statement, &#x27;isSensitive&#x27;: isSensitive&#125;</span><br><span class="line">                # else:</span><br><span class="line">                #     context = &#123;&#x27;message&#x27;: statement, &#x27;isSensitive&#x27;: isSensitive&#125;</span><br><span class="line">                # return JsonResponse(context)</span><br><span class="line">                context = &#123;&#x27;message&#x27;: statement, &#x27;isSensitive&#x27;: isSensitive&#125;</span><br><span class="line">            return render(request, &quot;detector/upload.html&quot;, &#123;&#x27;context&#x27;: context&#125;)</span><br><span class="line">        else:</span><br><span class="line">            error_message = &#x27;图片上传出错啦！&#x27;</span><br><span class="line">            return render(request, &#x27;detector/error.html&#x27;, &#123;&#x27;error_message&#x27;: error_message&#125;)</span><br><span class="line">    else:</span><br><span class="line">        error_message = &#x27;只接受POST请求！&#x27;</span><br><span class="line">        return render(request, &#x27;detector/error.html&#x27;, &#123;&#x27;error_message&#x27;: error_message&#125;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># test index view</span><br><span class="line"># from django.http import HttpResponse</span><br><span class="line"># # 访问链接：http://127.0.0.1:8000/detector/</span><br><span class="line"># def index(request):</span><br><span class="line">#     return HttpResponse(&quot;Hello, world. You&#x27;re at the polls index.&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>上述函数中用到的三个工具函数detect_sensitive, calculate_md5,
save_image在utils目录下的utils.py文件夹。该文件夹内容较多，此处不做赘述。</p>
<h3 id="配置视图url">配置视图url</h3>
<p>要想请求上述创建的两个视图函数，需要现在应用的<code>urls.py(detector/urls.py)</code>中配置这两个视图的url</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># detector/settings.py</span><br><span class="line"></span><br><span class="line">from django.urls import path</span><br><span class="line"></span><br><span class="line">from . import views</span><br><span class="line"></span><br><span class="line">app_name = &quot;detector&quot;</span><br><span class="line">urlpatterns = [</span><br><span class="line">    # /detector/</span><br><span class="line">    # path(&quot;&quot;, views.index, name=&quot;index&quot;),</span><br><span class="line">    # /detector/handle_upload/</span><br><span class="line">    path(&quot;handle_upload/&quot;, views.handle_upload, name=&quot;handle_upload&quot;),</span><br><span class="line">    # /detector/form/</span><br><span class="line">    path(&quot;form/&quot;, views.form_view, name=&quot;form&quot;),</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>要想访问<code>form_view()</code>函数，在启动服务器后访问链接
http://127.0.0.1:8000/detector/form/，同理要想访问<code>handle_upload()</code>函数，只需访问http://127.0.0.1:8000/detector/handle_upload/（不过handle_upload()要求使用POST请求访问，直接采取GET请求访问会返回一条错误信息，因此我们会在访问form_view()后渲染出<code>detector/form.html</code>，在这个表单上传文件，就可以采用POST请求正确访问<code>handle_upload()</code>函数，获得对应的响应）</p>
<h3 id="渲染模板文件">渲染模板文件</h3>
<p>我们会将一个应用使用的模板文件全部放入应用目录下的<code>templates</code>目录下（即<code>detector/templates/</code>），Django会默认在这个目录下寻找模板。</p>
<p>创建好模板文件后的目录结构如下：</p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZoGMK.png" alt="OZoGMK.png" />
<figcaption aria-hidden="true">OZoGMK.png</figcaption>
</figure>
<blockquote>
<p>这里还在templates目录下创建了一个detector子目录，在子目录下才放置了模板文件<code>form.html</code>
这里新建的detector子目录起到划分命名空间的作用。因为如果在不同的应用程序中具有相同名称的模板，Django
将无法区分它们，所以需要设置命名空间。</p>
</blockquote>
<p>那么，模板文件form.html的内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;!-- form.html --&gt;</span><br><span class="line">&#123;% load static %&#125;</span><br><span class="line">&lt;html lang=&quot;en&quot;&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">    &lt;meta charset=&quot;UTF-8&quot;&gt;</span><br><span class="line">    &lt;title&gt;Upload Image&lt;/title&gt;</span><br><span class="line">    &lt;link rel=&quot;stylesheet&quot; href=&quot;&#123;% static &#x27;css/form_style.css&#x27; %&#125;&quot;&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line"></span><br><span class="line">    &lt;form method=&quot;POST&quot; action=&quot;&#123;% url &#x27;detector:handle_upload&#x27; %&#125;&quot;  enctype=&quot;multipart/form-data&quot;&gt;</span><br><span class="line">      &#123;% csrf_token %&#125;</span><br><span class="line">      &lt;label for=&quot;imageUpload&quot;&gt;上传图片&lt;/label&gt;&lt;br&gt;</span><br><span class="line">      &lt;input type=&quot;file&quot; id=&quot;imageUpload&quot; name=&quot;image&quot;&gt;</span><br><span class="line">      &lt;input type=&quot;submit&quot; value=&quot;上传&quot;&gt;</span><br><span class="line">    &lt;/form&gt;</span><br><span class="line"></span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：</p>
<ol type="1">
<li>需要在<code>&lt;form&gt;</code>表单内部增加这行代码：<code>&#123;% csrf_token %&#125;</code>，否则会报错<code>Forbidden (CSRF token missing.): /detector/form/</code></li>
<li><code>action</code>属性值表示点击<code>submit</code>时会将表单提交到什么位置，这里用了软编码<code>"&#123;% url 'detector:handle_upload' %&#125;"</code>，其中detector:表示程序命名空间，与<code>detector/urls.py</code>中的(<code>app_name="detector"</code>)相对应；<code>handle_upload</code>表示要将提交到<code>handle_upload</code>这个url路径，与<code>detector/urls.py</code>中的
<code>path("handle_upload/", views.handle_upload, name="handle_upload"),</code>中的<code>name</code>相对应</li>
<li>最头部的<code>&#123;% load static %&#125;</code>需要添加上，只有这样才能加载到项目的setting.py中配置的static默认路径，才能加载到我们的css和images静态文件</li>
</ol>
</blockquote>
<p>当通过<code>form.html</code>提交表单后，表单信息被正确提交到<code>upload_handle()</code>视图，经过<code>handle_upload()</code>函数处理，会将响应的<code>context</code>传递给模板文件<code>upload.html</code>进行渲染，这部分的对应代码为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">context = &#123;&#x27;message&#x27;: statement, &#x27;isSensitive&#x27;: isSensitive&#125;</span><br><span class="line">return render(request, &quot;detector/upload.html&quot;, &#123;&#x27;context&#x27;: context&#125;)</span><br></pre></td></tr></table></figure>
<p>模板文件<code>upload.html</code>的内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% load static %&#125;</span><br><span class="line">&lt;html lang=&quot;en&quot;&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">    &lt;meta charset=&quot;UTF-8&quot;&gt;</span><br><span class="line">    &lt;title&gt;Upload Image&lt;/title&gt;</span><br><span class="line">    &lt;link rel=&quot;stylesheet&quot; href=&quot;&#123;% static &#x27;css/upload_style.css&#x27; %&#125;&quot;&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line"></span><br><span class="line">    &#123;% if context.isSensitive %&#125;</span><br><span class="line">        &lt;p class=&quot;sensitive&quot;&gt;图片中的文字为:&lt;/p&gt;</span><br><span class="line">        &lt;p class=&quot;sensitive&quot;&gt;&#123;&#123; context.message &#125;&#125;&lt;/p&gt;</span><br><span class="line">        &lt;p class=&quot;sensitive&quot;&gt;检查结果为：&lt;/p&gt;</span><br><span class="line">        &lt;p class=&quot;sensitive&quot;&gt;敏感信息，需要屏蔽&lt;/p&gt;</span><br><span class="line">    &#123;% else %&#125;</span><br><span class="line">        &lt;p class=&quot;normal&quot;&gt;图片中的文字为: &lt;/p&gt;</span><br><span class="line">        &lt;p class=&quot;normal&quot;&gt;&#123;&#123; context.message &#125;&#125;&lt;/p&gt;</span><br><span class="line">        &lt;p class=&quot;normal&quot;&gt;检查结果为：&lt;/p&gt;</span><br><span class="line">        &lt;p class=&quot;normal&quot;&gt;正常信息，允许通过&lt;/p&gt;</span><br><span class="line">    &#123;% endif %&#125;</span><br><span class="line"></span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure>
<p>至此，项目的主体内容就已经完成。</p>
<h1 id="项目演示">项目演示</h1>
<p>上传图片：</p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZodxC.png" alt="OZodxC.png" />
<figcaption aria-hidden="true">OZodxC.png</figcaption>
</figure>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZo0XN.png" alt="OZo0XN.png" />
<figcaption aria-hidden="true">OZo0XN.png</figcaption>
</figure>
<p>结果展示：</p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZosJL.png" alt="OZosJL.png" />
<figcaption aria-hidden="true">OZosJL.png</figcaption>
</figure>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZoVli.png" alt="OZoVli.png" />
<figcaption aria-hidden="true">OZoVli.png</figcaption>
</figure>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZofBX.png" alt="OZofBX.png" />
<figcaption aria-hidden="true">OZofBX.png</figcaption>
</figure>
<p>查看告警信息表</p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZoSHS.png" alt="OZoSHS.png" />
<figcaption aria-hidden="true">OZoSHS.png</figcaption>
</figure>
]]></content>
      <categories>
        <category>开发技能</category>
      </categories>
      <tags>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title>Fast RCNN论文理解</title>
    <url>/2023/12/29/Fast-RCNN%E8%AE%BA%E6%96%87%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"><h1 id="fast-rcnn论文理解">Fast RCNN论文理解</h1>
<p>在Fast
RCNN提出前，目标检测领域效果较好的算法是RCNN和SPPnet，因此本文主要将Fast
RCNN的改进与这两种算法进行对比。</p>
<h2 id="rcnn和sppnet的缺点">RCNN和SPPnet的缺点</h2>
<h3 id="rcnn的缺点">RCNN的缺点</h3>
<ol type="1">
<li><p>训练是多阶段的</p>
<span id="more"></span>
<ol type="1">
<li>首先，在候选区域上微调ConvNet</li>
<li>然后，对ConvNet产生的特征训练SVMs分类器，用来代替微调的CNN网络学到的softmax分类器</li>
<li>最后，学习边界框回归网络</li>
</ol></li>
<li><p>训练耗费时间和空间</p>
<ol type="1">
<li>为了训练SVM分类器和边界框回归网络，从ConvNet抽取到的候选区域的特征要先被写入磁盘</li>
<li>耗时：<strong>RCNN对每个候选区域都执行ConvNet前向传播来提取特征，没有共享计算</strong></li>
</ol></li>
<li><p>测试很慢</p>
<ol type="1">
<li>每张测试图片要先获取2000个候选区域，再对每个区域抽取特征，耗时很长</li>
</ol></li>
</ol>
<h3 id="sppnet的缺点">SPPnet的缺点</h3>
<h4 id="sppnet对rcnn的改进">SPPnet对RCNN的改进</h4>
<p>SPPnet对RCNN进行了改进，引入了一个空间金字塔池化层(SPP
layer)，这个层的本质是：通过采用动态的池化核尺寸，来限制最终特征输出尺寸的最大池化层(max
pooling
layer)。从而使网络借助该层可以把不同大小的候选区域特征图转换成特定大小的输出。</p>
<p>因此该网络的运行流程大致如下：</p>
<p>输入整张图片进行前向传播，计算一个卷积特征图=&gt;从这个共享特征图抠出每个候选区域的特征图=&gt;通过SPP
layer把不同大小的候选区域特征图转换成特定大小的输出=&gt;对特征向量进行分类。</p>
<p>这种做法只需要对整张图片做一次前向传播，得到整张图片的特征图，再从中抠出不同候选区域的特征图即可，不用像RCNN一样对2000张候选区域图像做2000次前向传播，实现了通过共享计算来加速，解决了RCNN预测时速度慢的问题</p>
<h4 id="sppnet的缺点-1">SPPnet的缺点</h4>
<ol type="1">
<li>训练是多阶段的：抽取特征==》微调网络==》训练SVMs分类器==》训练边界框回归</li>
<li>耗费时间和空间：因为要额外训练SVMs分类器和边界框回归网络，所以CNN得出的特征仍然要被写入磁盘</li>
<li>SPPnet的微调算法部分不能更新卷积层的参数，限制了深层网络的精度</li>
</ol>
<h2 id="主要贡献">主要贡献</h2>
<ol type="1">
<li>精度更高（比RCNN、SPPnet）</li>
<li>训练使用多任务损失函数(multi-task loss)实现单阶段的训练</li>
<li>训练可以更新所有的网络层</li>
<li>不需要磁盘进行特征缓存</li>
</ol>
<h2 id="模型结构">模型结构</h2>
<h3 id="总体流程">总体流程</h3>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYAJv.png" alt="OKYAJv.png" />
<figcaption aria-hidden="true">OKYAJv.png</figcaption>
</figure>
<p>输入一整张图片和一套候选区域（也叫ROIs）=&gt;Deep
ConvNet（卷积+池化）得到整体的特征图=&gt;根据候选区域在原图中的位置，使用ROI投影，获取到候选区域的特征图=&gt;针对每个候选区域，用ROI池化层把尺寸不固定的候选区域特征图转换成特定尺寸的特征图（如7
x
7）=&gt;接上2个全连接层，得到每个候选区域的特征向量=&gt;再接上两个并列的全连接层，获得两个输出：</p>
<ol type="1">
<li>利用softmax预测类别（对K+1个类别）</li>
<li>产生每个类别对应的边界框坐标（(K+1)*4）</li>
</ol>
<h3 id="与sppnet的区别">与SPPnet的区别</h3>
<ol type="1">
<li>SPPnet在ConvNet之后接上了SPP
layer（空间金字塔池化层），用来把不同尺寸的候选区域特征图转换为特定大小的输出；而Fast
RCNN在ConvNet之后接上了ROI pooling
layer（ROI池化层），用于把不同尺寸的候选区域特征图转换成特定尺寸的特征图</li>
<li>SPPnet在提取到图像的CNN特征后，又额外训练SVM进行分类和回归；而Fast
RCNN就是直接接了两个并行的全连接层做分类和回归</li>
</ol>
<h3 id="组成部分及实现步骤">组成部分及实现步骤</h3>
<h4 id="roi池化层">ROI池化层</h4>
<p>用最大池化把感兴趣区域的特征转换成特定大小的特征图（如7 x
7），是spatial pyramid pooling layer只有一个金字塔层时的特例</p>
<h4 id="初始化fast-rcnn">初始化Fast RCNN</h4>
<p>从在ImageNet上预训练好的图像分类模型（AlexNet）上初始化一个Fast
RCNN，改造步骤为;</p>
<p>（1）首先，把Conv Net之后最后一层的最大池化层替换为一个ROI池化层</p>
<p>（2）其次，网络的最后一个全连接层和softmax层替换为两个并行的全连接层（一个分支用来预测K+1个类别，一个分支用来预测边界框回归）</p>
<p>（3）最后，网络要接受两个数据输入：a list of images和 a list of ROIs
in those images</p>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYrxY.png" alt="OKYrxY.png" />
<figcaption aria-hidden="true">OKYrxY.png</figcaption>
</figure>
<h4 id="微调模型">微调模型</h4>
<p>即：通过反向传播来训练整个网络的权重</p>
<p>（1）为什么SPPnet不能更新空间金字塔池化层前的权重？</p>
<p>根本原因：当训练样本（ROIs）来自于不同的图片时，通过空间金字塔池化层（SPP
layer）的反向传播是无效的。（至于为啥无效，没看懂解释）</p>
<p>（2）Fast
RCNN是怎么有效训练的？（通过反向传播来训练整个网络的权重）</p>
<ul>
<li>mini-batch
分层采样：SGD的mini-batches是被分层采样的，先采样N个图片，然后从每张图片中采样R/N个ROIs。这样，来自同一张图片的ROIs就能在前向传播和后向传播中共享计算和内存。论文中取N=2，R=128。即每次采样2张图片，从每张图片采样64个ROI。</li>
<li>精简训练过程：用一个微调阶段共同优化一个softmax分类器和边界框回归器</li>
</ul>
<p>（3）multi-task loss</p>
<p>multi-task loss = 类别损失 + 框回归损失</p>
<ul>
<li>类别损失</li>
</ul>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKY7Hp.png" alt="OKY7Hp.png" />
<figcaption aria-hidden="true">OKY7Hp.png</figcaption>
</figure>
<ul>
<li>框回归损失</li>
</ul>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYNXU.png" alt="OKYNXU.png" />
<figcaption aria-hidden="true">OKYNXU.png</figcaption>
</figure>
<p>（4）mini-batch 分层采样</p>
<p>（5）通过ROI池化层反向传播</p>
<p>（6）SGD 超参数</p>
<h4 id="目标的尺度不变性">目标的尺度不变性</h4>
<p>（1）含义：如果有两个内容相同的目标，唯一区别是一个目标比较小，一个目标比较大，如果网络能将这两个目标都识别出来，说明网络具有较好的尺度不变性</p>
<p>（2）两种策略：</p>
<ul>
<li>单尺度训练：每张图片都被处理成预定义好的图片尺寸来进行训练和预测。让网络来直接学会尺度不变性</li>
<li>多尺度训练：在训练期间，每一张图片都被随机采样成特定的尺度。这也是一种数据增强的手段</li>
</ul>
<h3 id="测试流程">测试流程</h3>
<p>接收输入（一张图片和这张图片中的一系列候选区域），输入网络就能预测出候选区域的类别和精确坐标</p>
<h3 id="截断的奇异值分解">截断的奇异值分解</h3>
<p>能够减少全连接层的参数，从而加速全连接层</p>
<h3 id="实验设置">实验设置</h3>
<p>更换不同的CNN网络结构（AlexNet、VGG_CNN_M_1024、VGG16），对比实验结果（精度和测试时间）。</p>
<p>此外，还关注了一个问题：对于SPPnet中不太深的ConvNet，只微调全连接层就足以获得较好的精度，然而论文在这里假设：对于很深的网络，上述结论不成立。进而提出了一个问题：微调哪些层的效果更好？</p>
<p>实验结果是：微调卷积层精度高于只微调全连接层</p>
<h3 id="模型的设计评估">模型的设计评估</h3>
<h4 id="多任务训练是否有效">多任务训练是否有效</h4>
<p>多任务训练：指同时训练分类和边界框回归任务，共同调整一套参数</p>
<p>多阶段训练：指分类和边界框回归分成两个阶段进行训练，先训练分类，训练好后冻结参数，再额外训练一个边界框回归分支，也会利用第二个分支的回归结果</p>
<p>实验结果表明：多任务的训练结果比多阶段的要好</p>
<h4 id="单尺度还是多尺度训练">单尺度还是多尺度训练？</h4>
<p>单尺度训练：把短边固定到600像素</p>
<p>多尺度训练：把短边固定到5种像素</p>
<p>结果：多尺度精度更高</p>
<h4 id="是否需要更多训练数据">是否需要更多训练数据</h4>
<p>扩增2007数据集，精度会提高。不会出现传统模型出现的精度饱和的情况</p>
<h4 id="svm是不是比softmax好">SVM是不是比softmax好？</h4>
<p>实验结果：Fast RCNN中，直接用softmax效果比好</p>
<h4 id="候选区域是不是越多越好">候选区域是不是越多越好？</h4>
<p>实验结果：候选区域越多，精度先提升后下降</p>
<p>参考资料：</p>
<blockquote>
<p>https://www.bilibili.com/video/BV1y94y1Q7QJ/?spm_id_from=333.880.my_history.page.click&amp;vd_source=66a72b15abe9693bd8b4f738f5a67ee7</p>
</blockquote>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>Fast RCNN</tag>
        <tag>论文理解</tag>
      </tags>
  </entry>
  <entry>
    <title>FastRCNN代码复现</title>
    <url>/2023/12/29/FastRCNN%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"><h1 id="fast-rcnn代码复现">Fast RCNN代码复现</h1>
<p>项目源代码下载地址：</p>
<blockquote>
<p>Fast-R-CNN-pytorch-master https://www.alipan.com/s/FqYEYzqCe7k
提取码:ue87 点击链接保存，或者复制本段内容，打开「阿里云盘」APP
，无需下载极速在线查看，视频原画倍速播放。</p>
</blockquote>
<span id="more"></span>
<p>项目目录如下;</p>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYKlq.png" alt="OKYKlq.png" />
<figcaption aria-hidden="true">OKYKlq.png</figcaption>
</figure>
<p>对Fast RCNN的论文理解见专栏的上篇内容，本文介绍Fast
RCNN的代码复现。基本流程如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">利用coco2017数据集训练Fast-RCNN模型（训练过程详细步骤记录）：</span><br><span class="line"></span><br><span class="line">（1）初始化COCO数据集</span><br><span class="line"></span><br><span class="line">（2）构造训练集和验证集：利用选择搜索算法（selective-search）生成一定数量的候选框，将候选框与gound-truth进行IOU（交并比）计算，如果IoU大于等于0.5，则认为候选区域是正样本，0.1&lt;IoU&lt;0.5，则认为候选区域是负样本</span><br><span class="line"></span><br><span class="line">（3）设置ROI Pooling模块、特征提取网络模型。利用ROIPlooing方法，从共享特征图抠出各个候选区域特征图</span><br><span class="line"></span><br><span class="line">（4）设置输出为一个分类分支（类别类数+背景类（1））与回归分支</span><br><span class="line"></span><br><span class="line">（5）设置多目标损失函数：交叉熵损失与回归损失</span><br><span class="line"></span><br><span class="line">（6）训练网络模型</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="初始化coco数据集">初始化COCO数据集</h4>
<p><strong>COCOdataset.py</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> PIL</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    COCO数据集的处理</span></span><br><span class="line"><span class="string">    1、读取数据</span></span><br><span class="line"><span class="string">    2、数据的预处理</span></span><br><span class="line"><span class="string">    3、数据的加载</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">	需要修改内容：COCO数据集的存放路径、加载训练集还是验证集(mode为&quot;train&quot;or&quot;val&quot;)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">COCOdataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="comment"># todo：注意修改coco数据集的存放路径</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, <span class="built_in">dir</span>=<span class="string">&#x27;D:\\WritePapers\\object_detection_basics\\Datasets\\COCO2017\\&#x27;</span>, mode=<span class="string">&#x27;train&#x27;</span>,</span></span><br><span class="line"><span class="params">                 transform=transforms.Compose(<span class="params">[transforms.ToTensor(<span class="params"></span>),</span></span></span><br><span class="line"><span class="params"><span class="params">                                               transforms.Normalize(<span class="params">[<span class="number">0.485</span>, <span class="number">0.456</span>, -<span class="number">.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span>)]</span>)</span>):</span><br><span class="line">        <span class="comment"># transform执行2步操作：1、将PIL.Image转换为torch.Tensor，并自动将数据缩放到[0,1]之间；</span></span><br><span class="line">        <span class="comment"># 2.对图像进行标准化。这里的两个参数分别是RGB通道的均值和标准差。这个操作会按照每个通道进行标准化，即(image - mean) / std</span></span><br><span class="line">        <span class="comment"># transforms.Normalize([0.485, 0.456, -.406], [0.229, 0.224, 0.225])的两个参数分别是ImageNet数据集的统计得到的RGB通道的均值和标准差</span></span><br><span class="line">        <span class="comment"># 这样做的目的是使得模型的输入数据分布和预训练模型（项目使用了在ImageNet预训练的VGG19模型）的输入数据分布一致，从而可以更好地利用预训练模型。</span></span><br><span class="line">        <span class="keyword">assert</span> mode <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>], <span class="string">&#x27;mode must be \&#x27;train\&#x27; or \&#x27;val\&#x27;&#x27;</span></span><br><span class="line">        self.<span class="built_in">dir</span> = <span class="built_in">dir</span> <span class="comment"># self关键字代表类的实例</span></span><br><span class="line">        self.mode = mode <span class="comment"># train则加载训练集，val则加载验证集</span></span><br><span class="line">        self.transform = transform</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(self.<span class="built_in">dir</span>, <span class="string">&#x27;%s.json&#x27;</span> % self.mode), <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f: <span class="comment">#%s是一个字符串占位符。%操作符用于指定要插入的值==&gt;%s会被self.mode的值替换</span></span><br><span class="line">            self.ss_regions = json.load(f)</span><br><span class="line">        <span class="comment"># with语句并不创建一个新的作用域。在with语句块内部定义的变量，其作用域是包含with语句的那个作用域。</span></span><br><span class="line">        self.img_dir = os.path.join(self.<span class="built_in">dir</span>, <span class="string">&#x27;%s2017&#x27;</span> % self.mode)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.ss_regions) <span class="comment"># 说明ss_regions是一个列表，每个元素代表一张图片对应的region信息</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i, max_num_pos=<span class="number">8</span>, max_num_neg=<span class="number">16</span></span>):</span><br><span class="line">        img = PIL.Image.<span class="built_in">open</span>(os.path.join(self.img_dir, <span class="string">&#x27;%012d.jpg&#x27;</span> %</span><br><span class="line">                                     self.ss_regions[i][<span class="string">&#x27;id&#x27;</span>]))</span><br><span class="line">        <span class="comment"># %012d是一个占位符，表示一个十二位的整数，不足十二位的部分会用0填充==&gt;%012d会被self.ss_regions[i][&#x27;id&#x27;]的值替换，self.ss_regions[i][&#x27;id&#x27;]是从JSON文件中读取的某张图片的ID</span></span><br><span class="line">        img = img.convert(<span class="string">&#x27;RGB&#x27;</span>) <span class="comment"># 将图片转换为RGB格式</span></span><br><span class="line">        img = img.resize([<span class="number">224</span>, <span class="number">224</span>])</span><br><span class="line">        pos_regions = self.ss_regions[i][<span class="string">&#x27;pos_regions&#x27;</span>] <span class="comment"># 获取正样本区域</span></span><br><span class="line">        neg_regions = self.ss_regions[i][<span class="string">&#x27;neg_regions&#x27;</span>] <span class="comment"># 获取负样本区域</span></span><br><span class="line">        <span class="keyword">if</span> self.transform != <span class="literal">None</span>: <span class="comment"># 如果设置了图像预处理方法</span></span><br><span class="line">            img = self.transform(img)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(pos_regions) &gt; max_num_pos: <span class="comment"># 如果正样本区域的数量大于最大数量,随机选择一部分正样本区域</span></span><br><span class="line">            pos_regions = random.sample(pos_regions, max_num_pos)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(neg_regions) &gt; max_num_neg:</span><br><span class="line">            neg_regions = random.sample(neg_regions, max_num_neg)</span><br><span class="line">        regions = pos_regions + neg_regions <span class="comment"># 合并正样本区域和负样本区域</span></span><br><span class="line">        random.shuffle(regions) <span class="comment"># 随机打乱区域</span></span><br><span class="line">        rects = [] <span class="comment"># 初始化矩形列表</span></span><br><span class="line">        rela_locs = []  <span class="comment"># 初始化相对位置列表</span></span><br><span class="line">        cats = [] <span class="comment"># 初始化类别列表</span></span><br><span class="line">        <span class="keyword">for</span> region <span class="keyword">in</span> regions:<span class="comment"># 遍历第i张图片的得到的区域（包括正样本和负样本区域）</span></span><br><span class="line">            rects.append(region[<span class="string">&#x27;rect&#x27;</span>]) <span class="comment"># 将区域的矩形信息添加到矩形列表中</span></span><br><span class="line">            p_rect = region[<span class="string">&#x27;rect&#x27;</span>] <span class="comment"># 获取区域的矩形信息</span></span><br><span class="line">            g_rect = region[<span class="string">&#x27;gt_rect&#x27;</span>] <span class="comment"># 获取区域的真实矩形信息</span></span><br><span class="line">            t_x = (g_rect[<span class="number">0</span>] + g_rect[<span class="number">2</span>] - p_rect[<span class="number">0</span>] - p_rect[<span class="number">2</span>]) / <span class="number">2</span> / (p_rect[<span class="number">2</span>] - p_rect[<span class="number">0</span>])</span><br><span class="line">            t_y = (g_rect[<span class="number">1</span>] + g_rect[<span class="number">3</span>] - p_rect[<span class="number">1</span>] - p_rect[<span class="number">3</span>]) / <span class="number">2</span> / (p_rect[<span class="number">3</span>] - p_rect[<span class="number">1</span>])</span><br><span class="line">            t_w = math.log((g_rect[<span class="number">2</span>] - g_rect[<span class="number">0</span>]) / (p_rect[<span class="number">2</span>] - p_rect[<span class="number">0</span>]))</span><br><span class="line">            t_h = math.log((g_rect[<span class="number">3</span>] - g_rect[<span class="number">1</span>]) / (p_rect[<span class="number">3</span>] - p_rect[<span class="number">1</span>]))</span><br><span class="line">            rela_locs.append([t_x, t_y, t_w, t_h]) <span class="comment"># 将区域的相对位置信息添加到相对位置列表中</span></span><br><span class="line">            cats.append(region[<span class="string">&#x27;category&#x27;</span>]) <span class="comment"># 将区域的类别信息添加到类别列表中</span></span><br><span class="line">        roi_idx_len = <span class="built_in">len</span>(regions) <span class="comment"># 获取区域的数量</span></span><br><span class="line">        <span class="keyword">return</span> img, rects, roi_idx_len, rela_locs, cats</span><br><span class="line"></span><br><span class="line"><span class="comment"># dataset = COCOdataset()</span></span><br><span class="line"><span class="comment"># print(dataset[1][0].shape)</span></span><br><span class="line"><span class="comment"># print(dataset[1][1])</span></span><br><span class="line"><span class="comment"># from torch.utils.data import DataLoader</span></span><br><span class="line"><span class="comment"># dataloader = DataLoader(dataset, batch_size=2)</span></span><br><span class="line"><span class="comment"># print(next(iter(dataloader))[1])</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    dataset = COCOdataset()</span><br><span class="line">    <span class="built_in">print</span>(dataset.__len__())</span><br><span class="line">    img, rects, roi_idx_len, rela_locs, cats = dataset.__getitem__(<span class="number">10</span>)</span><br><span class="line">    <span class="built_in">print</span>(img, rects, roi_idx_len, rela_locs, cats)</span><br><span class="line">    <span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">    loader = DataLoader(dataset, batch_size=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i, temp <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader):</span><br><span class="line">        <span class="built_in">print</span>(i,<span class="built_in">type</span>(temp))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="构造训练集和验证集">构造训练集和验证集</h4>
<p><strong>利用选择搜索算法（selective-search）生成一定数量的候选框，将候选框与gound-truth进行IOU（交并比）计算，如果IoU大于等于0.5，则认为候选区域是正样本，0.1&lt;IoU&lt;0.5，则认为候选区域是负样本</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse <span class="comment"># 导入argparse模块，用于处理命令行参数</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> sys <span class="comment"># 导入sys模块，用于处理Python运行时环境</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> progressbar <span class="keyword">import</span> * <span class="comment"># 导入progressbar模块，用于显示进度条</span></span><br><span class="line"><span class="keyword">from</span> pycocotools.coco <span class="keyword">import</span> COCO <span class="comment"># 导入pycocotools模块，用于处理COCO数据集</span></span><br><span class="line"><span class="keyword">from</span> selectivesearch <span class="keyword">import</span> selective_search <span class="comment"># 从selectivesearch模块导入selective_search函数，用于进行选择性搜索</span></span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io, util, color <span class="comment"># 从selectivesearch模块导入selective_search函数，用于进行选择性搜索</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_iou</span>(<span class="params">a, b</span>): <span class="comment"># 定义计算IoU（交并比）的函数，输入参数为两个矩形框</span></span><br><span class="line">    <span class="comment"># 矩形框的格式为：[左上角x坐标，左上角y坐标，宽度，高度]</span></span><br><span class="line">    a_min_x, a_min_y, a_delta_x, a_delta_y = a</span><br><span class="line">    b_min_x, b_min_y, b_delta_x, b_delta_y = b</span><br><span class="line">    a_max_x = a_min_x + a_delta_x</span><br><span class="line">    a_max_y = a_min_y + a_delta_y</span><br><span class="line">    b_max_x = b_min_x + b_delta_x</span><br><span class="line">    b_max_y = b_min_y + b_delta_y</span><br><span class="line">    <span class="comment"># 如果两个矩形框没有交集，则IoU为0</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">min</span>(a_max_y, b_max_y) &lt; <span class="built_in">max</span>(a_min_y, b_min_y) <span class="keyword">or</span> <span class="built_in">min</span>(a_max_x, b_max_x) &lt; <span class="built_in">max</span>(a_min_x, b_min_x):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 计算交集的面积</span></span><br><span class="line">        intersect_area = (<span class="built_in">min</span>(a_max_y, b_max_y) - <span class="built_in">max</span>(a_min_y, b_min_y) + <span class="number">1</span>) * \</span><br><span class="line">            (<span class="built_in">min</span>(a_max_x, b_max_x) - <span class="built_in">max</span>(a_min_x, b_min_x) + <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 计算并集的面积</span></span><br><span class="line">        union_area = (a_delta_x + <span class="number">1</span>) * (a_delta_y + <span class="number">1</span>) + \</span><br><span class="line">            (b_delta_x + <span class="number">1</span>) * (b_delta_y + <span class="number">1</span>) - intersect_area</span><br><span class="line">        <span class="comment"># 返回IoU</span></span><br><span class="line">        <span class="keyword">return</span> intersect_area / union_area</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ss_img</span>(<span class="params">img_id, coco, cat_dict, args</span>): <span class="comment"># 定义selective_search函数，输入参数为图像id、COCO数据集、类别字典、命令行参数</span></span><br><span class="line">    img_path = os.path.join(args.data_dir, args.mode +</span><br><span class="line">                            <span class="string">&#x27;2017&#x27;</span>, <span class="string">&#x27;%012d.jpg&#x27;</span> % img_id) <span class="comment"># 获取图像的路径</span></span><br><span class="line">    coco_dict = &#123;cat[<span class="string">&#x27;id&#x27;</span>]: cat[<span class="string">&#x27;name&#x27;</span>]</span><br><span class="line">                 <span class="keyword">for</span> cat <span class="keyword">in</span> coco.loadCats(coco.getCatIds())&#125;  <span class="comment"># 创建一个字典，将COCO数据集的类别ID映射到类别名</span></span><br><span class="line">    img = io.imread(img_path) <span class="comment"># 读取图像</span></span><br><span class="line">    <span class="keyword">if</span> img.ndim == <span class="number">2</span>:    <span class="comment"># 如果图像是灰度图，则将其转换为RGB图像</span></span><br><span class="line">        img = color.gray2rgb(img)</span><br><span class="line">    _, ss_regions = selective_search(   <span class="comment"># 对图像进行选择性搜索，获取候选区域</span></span><br><span class="line">        img, args.scale, args.sigma, args.min_size)         <span class="comment"># &#x27;rect&#x27;: (left, top, width, height)</span></span><br><span class="line">    anns = coco.loadAnns(coco.getAnnIds(</span><br><span class="line">        imgIds=[img_id], catIds=coco.getCatIds(catNms=args.cats))) <span class="comment"># 获取图像的标注信息</span></span><br><span class="line">    pos_regions = [] <span class="comment"># 初始化正样本区域列表</span></span><br><span class="line">    neg_regions = [] <span class="comment"># 初始化负样本区域列表</span></span><br><span class="line">    h = img.shape[<span class="number">0</span>] <span class="comment"># 获取图像的高度</span></span><br><span class="line">    w = img.shape[<span class="number">1</span>] <span class="comment"># 获取图像的宽度</span></span><br><span class="line">    <span class="keyword">for</span> region <span class="keyword">in</span> ss_regions: <span class="comment"># 遍历每个候选区域</span></span><br><span class="line">        <span class="keyword">for</span> ann <span class="keyword">in</span> anns: <span class="comment"># 遍历每个标注信息</span></span><br><span class="line">            iou = cal_iou(region[<span class="string">&#x27;rect&#x27;</span>], ann[<span class="string">&#x27;bbox&#x27;</span>]) <span class="comment"># 计算候选区域和标注区域的IoU</span></span><br><span class="line">            <span class="keyword">if</span> iou &gt;= <span class="number">0.1</span>: <span class="comment"># 如果IoU大于等于0.1，则认为候选区域是有效的</span></span><br><span class="line">                rect = <span class="built_in">list</span>(region[<span class="string">&#x27;rect&#x27;</span>]) <span class="comment"># 获取候选区域的矩形框</span></span><br><span class="line">                rect[<span class="number">0</span>] = rect[<span class="number">0</span>] / w <span class="comment"># 将矩形框的x坐标转换为相对于图像宽度的比例</span></span><br><span class="line">                rect[<span class="number">1</span>] = rect[<span class="number">1</span>] / h <span class="comment"># 将矩形框的y坐标转换为相对于图像高度的比例</span></span><br><span class="line">                rect[<span class="number">2</span>] = rect[<span class="number">0</span>] + rect[<span class="number">2</span>] / w <span class="comment"># 将矩形框的宽度转换为相对于图像宽度的比例</span></span><br><span class="line">                rect[<span class="number">3</span>] = rect[<span class="number">1</span>] + rect[<span class="number">3</span>] / h <span class="comment"># 将矩形框的高度转换为相对于图像高度的比例</span></span><br><span class="line">                gt_rect = <span class="built_in">list</span>(ann[<span class="string">&#x27;bbox&#x27;</span>]) <span class="comment"># 获取标注区域的矩形框</span></span><br><span class="line">                gt_rect[<span class="number">0</span>] = gt_rect[<span class="number">0</span>] / w <span class="comment"># 将矩形框的x坐标转换为相对于图像宽度的比例</span></span><br><span class="line">                gt_rect[<span class="number">1</span>] = gt_rect[<span class="number">1</span>] / h <span class="comment"># 将矩形框的y坐标转换为相对于图像高度的比例</span></span><br><span class="line">                gt_rect[<span class="number">2</span>] = gt_rect[<span class="number">0</span>] + gt_rect[<span class="number">2</span>] / w <span class="comment"># 将矩形框的宽度转换为相对于图像宽度的比例</span></span><br><span class="line">                gt_rect[<span class="number">3</span>] = gt_rect[<span class="number">1</span>] + gt_rect[<span class="number">3</span>] / h <span class="comment"># 将矩形框的高度转换为相对于图像高度的比例</span></span><br><span class="line">                <span class="keyword">if</span> iou &gt;= <span class="number">0.5</span>: <span class="comment"># 如果IoU大于等于0.5，则认为候选区域是正样本</span></span><br><span class="line">                    pos_regions.append(&#123;<span class="string">&#x27;rect&#x27;</span>: rect, </span><br><span class="line">                                        <span class="string">&#x27;gt_rect&#x27;</span>: gt_rect,</span><br><span class="line">                                        <span class="string">&#x27;category&#x27;</span>: cat_dict[coco_dict[ann[<span class="string">&#x27;category_id&#x27;</span>]]]&#125;)</span><br><span class="line">                <span class="keyword">else</span>: <span class="comment"># 否则，认为候选区域是负样本</span></span><br><span class="line">                    neg_regions.append(&#123;<span class="string">&#x27;rect&#x27;</span>: rect, </span><br><span class="line">                                        <span class="string">&#x27;gt_rect&#x27;</span>: gt_rect,</span><br><span class="line">                                        <span class="string">&#x27;category&#x27;</span>: <span class="number">0</span>&#125;)</span><br><span class="line">    <span class="keyword">return</span> pos_regions, neg_regions <span class="comment"># 返回正样本区域和负样本区域</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    parser = argparse.ArgumentParser(<span class="string">&#x27;parser to create regions&#x27;</span>) <span class="comment"># 创建一个命令行参数解析器，用于处理命令行参数。</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--data_dir&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;D:\\WritePapers\\object_detection_basics\\Datasets\\COCO2017\\&#x27;</span>) <span class="comment"># 指定COCO2017数据集的路径</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--mode&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;train&#x27;</span>)   <span class="comment"># train/val</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--save_dir&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;D:\\WritePapers\\object_detection_basics\\Datasets\\COCO2017\\&#x27;</span>) <span class="comment">#指定保存结果的路径</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--cats&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, nargs=<span class="string">&#x27;*&#x27;</span>, default=[</span><br><span class="line">                        <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;sheep&#x27;</span>, <span class="string">&#x27;cow&#x27;</span>, <span class="string">&#x27;elephant&#x27;</span>, <span class="string">&#x27;bear&#x27;</span>, <span class="string">&#x27;zebra&#x27;</span>, <span class="string">&#x27;giraffe&#x27;</span>]) <span class="comment"># 指定需要处理的类别</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--scale&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">30.0</span>) <span class="comment"># 指定选择性搜索的尺度</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--sigma&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.8</span>) <span class="comment"># 指定选择性搜索的高斯平滑参数</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--min_size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">50</span>) <span class="comment"># 指定选择性搜索的最小区域大小</span></span><br><span class="line">    args = parser.parse_args() <span class="comment"># 解析命令行参数，并将结果保存在args中</span></span><br><span class="line">    coco = COCO(os.path.join(args.data_dir, <span class="string">&#x27;annotations&#x27;</span>,</span><br><span class="line">                             <span class="string">&#x27;instances_%s2017.json&#x27;</span> % args.mode)) <span class="comment">#加载COCO2017数据集的标注信息</span></span><br><span class="line">    cat_dict = &#123;args.cats[i]: i+<span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(args.cats))&#125; <span class="comment"># 创建一个字典，将类别名称映射到类别ID</span></span><br><span class="line">    cat_dict[<span class="string">&#x27;background&#x27;</span>] = <span class="number">0</span> <span class="comment"># 将背景类别的ID设置为0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># get relavant image ids</span></span><br><span class="line">    <span class="keyword">if</span> args.mode == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">        num_cat = <span class="number">400</span> <span class="comment"># 如果运行模式为训练，则每个类别的图像数量设置为400</span></span><br><span class="line">    <span class="keyword">if</span> args.mode == <span class="string">&#x27;val&#x27;</span>:</span><br><span class="line">        num_cat = <span class="number">100</span> <span class="comment"># 如果运行模式为验证，则每个类别的图像数量设置为100</span></span><br><span class="line">    img_ids = []</span><br><span class="line">    cat_ids = coco.getCatIds(catNms=args.cats) <span class="comment"># 获取需要处理的类别的ID</span></span><br><span class="line">    <span class="keyword">for</span> cat_id <span class="keyword">in</span> cat_ids:</span><br><span class="line">        cat_img_ids = coco.getImgIds(catIds=[cat_id]) <span class="comment"># 获取该类别的所有图像ID</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(cat_img_ids) &gt; num_cat:</span><br><span class="line">            cat_img_ids = random.sample(cat_img_ids, num_cat) <span class="comment"># 如果该类别的图像数量大于num_cat，则随机选择num_cat个图像</span></span><br><span class="line">        img_ids += cat_img_ids <span class="comment"># 将选择的图像ID添加到图像ID列表</span></span><br><span class="line">    img_ids = <span class="built_in">list</span>(<span class="built_in">set</span>(img_ids)) <span class="comment"># 去除重复的图像ID</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># selective_search each image</span></span><br><span class="line">    <span class="comment"># [&#123;&#x27;id&#x27;: 1, &#x27;pos_regions&#x27;:[...], &#x27;neg_regions&#x27;:[...]&#125;, ...]</span></span><br><span class="line"></span><br><span class="line">    num_imgs = <span class="built_in">len</span>(img_ids) <span class="comment"># 获取图像的数量</span></span><br><span class="line">    ss_regions = [] <span class="comment"># 初始化选择性搜索的区域列表</span></span><br><span class="line">    p = ProgressBar(widgets=[<span class="string">&#x27;Progress: &#x27;</span>, Percentage(),</span><br><span class="line">                             <span class="string">&#x27; &#x27;</span>, Bar(<span class="string">&#x27;#&#x27;</span>), <span class="string">&#x27; &#x27;</span>, Timer(), <span class="string">&#x27; &#x27;</span>, ETA()]) <span class="comment"># 创建一个进度条</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> p(<span class="built_in">range</span>(num_imgs)): <span class="comment"># 遍历每个图像</span></span><br><span class="line">        img_id = img_ids[i] <span class="comment"># 获取当前图像的ID</span></span><br><span class="line">        pos_regions, neg_regions = ss_img(img_id, coco, cat_dict, args) <span class="comment"># 对当前图像进行选择性搜索，获取正样本区域和负样本区域</span></span><br><span class="line">        ss_regions.append(&#123;<span class="string">&#x27;id&#x27;</span>: img_id,</span><br><span class="line">                           <span class="string">&#x27;pos_regions&#x27;</span>: pos_regions,</span><br><span class="line">                           <span class="string">&#x27;neg_regions&#x27;</span>: neg_regions&#125;)<span class="comment"># 将当前图像的ID、正样本区域和负样本区域添加到选择性搜索的区域列表中</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># save</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(args.save_dir, <span class="string">&#x27;%s.json&#x27;</span> % args.mode), <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        json.dump(ss_regions, f) <span class="comment"># 将选择性搜索的区域列表保存为JSON格式</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="设置roi-pooling模块特征提取网络模型及多目标损失函数">设置ROI
Pooling模块、特征提取网络模型及多目标损失函数</h4>
<p><strong>ROI Plooing模块</strong> <strong>roipooling.py</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ROIPooling</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, output_size</span>): <span class="comment"># 初始化函数，设置输出大小</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.maxpool = nn.AdaptiveMaxPool2d(output_size)  <span class="comment"># 创建一个自适应最大池化层，输出大小为output_size</span></span><br><span class="line">        self.size = output_size  <span class="comment"># 设置输出大小</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, imgs, rois, roi_idx</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param img: img:批次内的图像</span></span><br><span class="line"><span class="string">        :param rois: rois:[[单张图片内框体],[],[]]</span></span><br><span class="line"><span class="string">        :param roi_idx: [2]*6-------&gt;[2, 2, 2, 2, 2, 2]</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        n = rois.shape[<span class="number">0</span>] <span class="comment"># 获取roi的数量</span></span><br><span class="line">        h = imgs.shape[<span class="number">2</span>] <span class="comment"># 获取图像的高度</span></span><br><span class="line">        w = imgs.shape[<span class="number">3</span>] <span class="comment"># 获取图像的宽度</span></span><br><span class="line"></span><br><span class="line">        x1 = rois[:, <span class="number">0</span>] <span class="comment"># 获取所有区域的左上角x坐标</span></span><br><span class="line">        y1 = rois[:, <span class="number">1</span>] <span class="comment"># 获取所有区域的左上角y坐标</span></span><br><span class="line">        x2 = rois[:, <span class="number">2</span>] <span class="comment"># 获取所有区域的右下角x坐标</span></span><br><span class="line">        y2 = rois[:, <span class="number">3</span>] <span class="comment"># 获取所有区域的右下角y坐标</span></span><br><span class="line"></span><br><span class="line">        x1 = np.floor(x1 * w).astype(<span class="built_in">int</span>) <span class="comment"># 将x1坐标转换为图像的实际坐标</span></span><br><span class="line">        x2 = np.ceil(x2 * w).astype(<span class="built_in">int</span>) <span class="comment"># 将x2坐标转换为图像的实际坐标</span></span><br><span class="line">        y1 = np.floor(y1 * h).astype(<span class="built_in">int</span>) <span class="comment"># 将y1坐标转换为图像的实际坐标</span></span><br><span class="line">        y2 = np.ceil(y2 * h).astype(<span class="built_in">int</span>) <span class="comment"># 将y2坐标转换为图像的实际坐标</span></span><br><span class="line"></span><br><span class="line">        res = [] <span class="comment"># 初始化结果列表</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n): <span class="comment"># 遍历每个区域</span></span><br><span class="line">            img = imgs[roi_idx[i]].unsqueeze(dim=<span class="number">0</span>) <span class="comment"># 获取第i个区域所在的图像</span></span><br><span class="line">            img = img[:, :, y1[i]:y2[i], x1[i]:x2[i]]  <span class="comment"># 对图像进行裁剪，只保留区域内的部分</span></span><br><span class="line">            img = self.maxpool(img) <span class="comment"># 对裁剪后的图像进行最大池化操作</span></span><br><span class="line">            res.append(img) <span class="comment"># 将处理后的图像添加到结果列表中</span></span><br><span class="line">        res = torch.cat(res, dim=<span class="number">0</span>) <span class="comment"># 将所有处理后的图像沿着批次维度拼接起来，最终res保存了所有池化后的ROI区域</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    imgs = torch.randn(<span class="number">2</span>, <span class="number">10</span>, <span class="number">224</span>, <span class="number">224</span>) <span class="comment"># 创建一个随机的图像张量（batch_size, Channel, Height, Weight）</span></span><br><span class="line">    rois = np.array([[<span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.4</span>],</span><br><span class="line">                    [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.7</span>, <span class="number">0.7</span>],</span><br><span class="line">                    [<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.3</span>]]) <span class="comment"># 创建一个随机的区域张量（x1, y1, x2, y2）</span></span><br><span class="line">    <span class="comment"># roi_idx表示每个区域（Region of Interest，ROI）所在的图像的索引</span></span><br><span class="line">    <span class="comment"># 表示有三个区域，前两个区域在第一张图像上（索引为0），第三个区域在第二张图像上（索引为1）</span></span><br><span class="line">    roi_idx = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    r = ROIPooling((<span class="number">7</span>, <span class="number">7</span>)) <span class="comment"># (7, 7)表示池化后的输出大小为7x7。这意味着无论输入区域的大小如何，ROIPooling层都会将其池化为7x7的大小</span></span><br><span class="line">    <span class="built_in">print</span>(r.forward(imgs, rois, roi_idx).shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>fast_rcnn.py</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> .roipooling <span class="keyword">import</span> ROIPooling <span class="comment"># 从当前目录下的roipooling模块导入ROIPooling类</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FastRCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes</span>):<span class="comment"># 初始化方法，接收一个参数：类别数量</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.num_classes = num_classes <span class="comment"># 将类别数量保存为实例变量</span></span><br><span class="line">        vgg = torchvision.models.vgg19_bn(pretrained=<span class="literal">True</span>) <span class="comment"># 加载预训练的VGG19模型</span></span><br><span class="line">        <span class="comment"># 获取VGG19模型的特征提取部分：通过卷积和池化操作提取图像的特征</span></span><br><span class="line">        <span class="comment"># vgg.features.children(): vgg.features获取VGG19模型的特征提取部分，children()方法获取这部分的所有子模块。这些子模块是一系列的卷积层、激活函数和池化层。</span></span><br><span class="line">        <span class="comment"># list(vgg.features.children()): 将子模块的迭代器转换为列表</span></span><br><span class="line">        <span class="comment"># list(vgg.features.children())[:-1]: 使用切片操作获取除最后一个子模块外的所有子模块。在VGG19模型中，最后一个子模块是一个池化层。</span></span><br><span class="line">        <span class="comment"># *list(vgg.features.children())[:-1]: 使用*操作符将列表解包，这样每个子模块都会作为nn.Sequential的一个单独参数传入。</span></span><br><span class="line">        <span class="comment"># nn.Sequential(*list(vgg.features.children())[:-1]): nn.Sequential是一个容器，它按照在构造函数中传入的顺序保存各个模块。</span></span><br><span class="line">        self.features = nn.Sequential(*<span class="built_in">list</span>(vgg.features.children())[:-<span class="number">1</span>])</span><br><span class="line">        self.roipool = ROIPooling(output_size=(<span class="number">7</span>, <span class="number">7</span>)) <span class="comment"># 创建ROIPooling层，输出大小为7x7</span></span><br><span class="line">        <span class="comment"># vgg.classifier.children(): vgg.classifier获取VGG19模型的分类部分，children()方法获取这部分的所有子模块。这些子模块是一系列的全连接层、激活函数和Dropout层。</span></span><br><span class="line">        <span class="comment"># list(vgg.classifier.children())[:-1]: 使用切片操作获取除最后一个子模块外的所有子模块。在VGG19模型中，最后一个子模块是一个全连接层，用于输出每个类别的概率。</span></span><br><span class="line">        self.output = nn.Sequential(*<span class="built_in">list</span>(vgg.classifier.children())[:-<span class="number">1</span>]) <span class="comment"># 获取VGG19模型的分类部分</span></span><br><span class="line">        self.prob = nn.Linear(<span class="number">4096</span>, num_classes+<span class="number">1</span>)<span class="comment"># 创建一个线性层，用于输出每个类别的概率</span></span><br><span class="line">        self.loc = nn.Linear(<span class="number">4096</span>, <span class="number">4</span> * (num_classes + <span class="number">1</span>)) <span class="comment"># 创建一个线性层，用于输出每个类别的边界框位置</span></span><br><span class="line"></span><br><span class="line">        self.cat_loss = nn.CrossEntropyLoss() <span class="comment"># 创建交叉熵损失函数，用于计算类别损失</span></span><br><span class="line">        self.loc_loss = nn.SmoothL1Loss() <span class="comment"># 创建Smooth L1损失函数，用于计算位置损失</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img, rois, roi_idx</span>): <span class="comment"># 接收三个参数：图像、区域、区域索引</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param img: img:批次内的图像</span></span><br><span class="line"><span class="string">        :param rois: rois:[[单张图片内框体],[],[]]</span></span><br><span class="line"><span class="string">        :param roi_idx: [2]*6-------&gt;[2, 2, 2, 2, 2, 2]</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        res = self.features(img) <span class="comment"># 对图像进行特征提取</span></span><br><span class="line">        res = self.roipool(res, rois, roi_idx) <span class="comment"># 对特征图进行ROIPooling</span></span><br><span class="line">        res = res.view(res.shape[<span class="number">0</span>], -<span class="number">1</span>) <span class="comment"># 将ROIPooling的结果展平</span></span><br><span class="line">        features = self.output(res) <span class="comment"># 对展平的结果进行分类</span></span><br><span class="line">        prob = self.prob(features) <span class="comment"># 计算每个类别的概率</span></span><br><span class="line">        loc = self.loc(features).view(-<span class="number">1</span>, self.num_classes+<span class="number">1</span>, <span class="number">4</span>) <span class="comment"># 计算每个类别的边界框位置</span></span><br><span class="line">        <span class="comment"># 输出的张量的形状就变成了(N, num_classes + 1, 4)，其中N是批次大小，num_classes + 1是类别数量（包括背景类别），4是边界框的参数数量。</span></span><br><span class="line">        <span class="comment"># 相当于最终输出是==&gt;每个图像都会输出：每个类别对应的一个边界框，这个边界框由4个参数确定。</span></span><br><span class="line">        <span class="keyword">return</span> prob, loc</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, prob, bbox, label, gt_bbox, lmb=<span class="number">1.0</span></span>): <span class="comment"># 计算损失的方法，接收五个参数：预测类别概率、预测边界框、真实类别标签、真实边界框、lambda</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param prob: 预测类别</span></span><br><span class="line"><span class="string">        :param bbox:预测边界框</span></span><br><span class="line"><span class="string">        :param label:真实类别</span></span><br><span class="line"><span class="string">        :param gt_bbox:真实边界框</span></span><br><span class="line"><span class="string">        :param lmb:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        loss_cat = self.cat_loss(prob, label) <span class="comment"># 计算类别损失</span></span><br><span class="line">        lbl = label.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).expand(label.size(<span class="number">0</span>), <span class="number">1</span>, <span class="number">4</span>) <span class="comment"># 将标签扩展为与边界框相同的形状 (N, 1, 4)，N：标签的数量 1：每个标签对应一个边界框 4：边界框的参数数量</span></span><br><span class="line">        mask = (label != <span class="number">0</span>).<span class="built_in">float</span>().view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).expand(label.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">4</span>) <span class="comment"># 将标签扩展为与边界框相同的形状</span></span><br><span class="line">        loss_loc = self.loc_loss(gt_bbox * mask, bbox.gather(<span class="number">1</span>, lbl).squeeze(<span class="number">1</span>) * mask) <span class="comment"># 计算位置损失</span></span><br><span class="line">        loss = loss_cat + lmb * loss_loc <span class="comment"># 计算总损失</span></span><br><span class="line">        <span class="keyword">return</span> loss, loss_cat, loss_loc <span class="comment"># 返回总损失、类别损失和位置损失</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="训练网络模型">训练网络模型</h4>
<p><strong>train.py</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse <span class="comment"># 导入argparse模块，用于处理命令行参数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> dataset <span class="keyword">import</span> COCOdataset</span><br><span class="line"><span class="keyword">from</span> fast_rcnn <span class="keyword">import</span> FastRCNN</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练函数，输入参数为模型、训练数据集、优化器和命令行参数</span></span><br><span class="line"><span class="comment"># 训练时要用gpu，记得把参数--cuda的默认值改为True</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model, train_dataset, optimizer, args</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    num_batches = <span class="built_in">len</span>(train_dataset) // args.batch_size  <span class="comment"># 计算批次数量</span></span><br><span class="line">    indexes = np.random.shuffle(np.arange(<span class="built_in">len</span>(train_dataset))) <span class="comment"># 随机打乱数据集的索引</span></span><br><span class="line">    <span class="comment"># 初始化损失和准确率</span></span><br><span class="line">    loss_all = <span class="number">0</span></span><br><span class="line">    loss_cat_all = <span class="number">0</span></span><br><span class="line">    loss_loc_all = <span class="number">0</span></span><br><span class="line">    accuracy = <span class="number">0</span></span><br><span class="line">    num_samples = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 遍历每个批次</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_batches):</span><br><span class="line">        <span class="comment"># 初始化图像、区域、ROI索引、相对位置和类别列表</span></span><br><span class="line">        imgs = []</span><br><span class="line">        rects = []</span><br><span class="line">        roi_idxs = []</span><br><span class="line">        rela_locs = []</span><br><span class="line">        cats = []</span><br><span class="line">        <span class="comment"># 遍历每个样本</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(args.batch_size):</span><br><span class="line">            <span class="comment"># img:原始图像; rect:建议框体;roi_idx_len:正负样本框体总数;rela_loc:调整后框体;cat:类别</span></span><br><span class="line">            img, rect, roi_idx_len, rela_loc, cat = train_dataset[i *</span><br><span class="line">                                                                 args.batch_size+j]</span><br><span class="line">            <span class="comment"># print(img, rect, roi_idx_len, gt_rect, cat)</span></span><br><span class="line">            <span class="comment"># 添加到对应的列表中</span></span><br><span class="line">            imgs.append(img.unsqueeze(<span class="number">0</span>))</span><br><span class="line">            rects += rect</span><br><span class="line">            rela_locs += rela_loc</span><br><span class="line">            roi_idxs += ([j] * roi_idx_len)   <span class="comment"># [2]*6-------&gt;[2, 2, 2, 2, 2, 2]</span></span><br><span class="line">            cats += cat</span><br><span class="line">        <span class="comment"># 将列表转换为张量或数组</span></span><br><span class="line">        imgs = torch.cat(imgs, dim=<span class="number">0</span>)</span><br><span class="line">        rects = np.array(rects)</span><br><span class="line">        rela_locs = torch.FloatTensor(rela_locs)</span><br><span class="line">        cats = torch.LongTensor(cats)</span><br><span class="line">        <span class="comment"># print(imgs, rects, roi_idxs, rela_locs, cats)</span></span><br><span class="line">        <span class="comment"># 如果使用CUDA，则将张量移动到GPU上</span></span><br><span class="line">        <span class="keyword">if</span> args.cuda:</span><br><span class="line">            imgs = imgs.cuda()</span><br><span class="line">            rela_locs = rela_locs.cuda()</span><br><span class="line">            cats = cats.cuda()</span><br><span class="line">        <span class="comment"># 清空梯度</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 前向传播，计算预测的概率和边界框</span></span><br><span class="line">        prob, bbox = model.forward(imgs, rects, roi_idxs)</span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        loss, loss_cat, loss_loc = model.loss(prob, bbox, cats, rela_locs)</span><br><span class="line">        <span class="comment"># 反向传播，计算梯度</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="comment"># 更新损失和准确率</span></span><br><span class="line">        num_samples += <span class="built_in">len</span>(cats)</span><br><span class="line">        loss_all += loss.item() * <span class="built_in">len</span>(cats)</span><br><span class="line">        loss_cat_all += loss_cat.item() * <span class="built_in">len</span>(cats)</span><br><span class="line">        loss_loc_all += loss_loc.item() * <span class="built_in">len</span>(cats)</span><br><span class="line">        accuracy += (torch.argmax(prob.detach(), dim=-<span class="number">1</span>) == cats).<span class="built_in">sum</span>().item()</span><br><span class="line">    <span class="comment"># 返回模型、损失和准确率</span></span><br><span class="line">    <span class="keyword">return</span> model, loss_all/num_samples, loss_cat_all/num_samples, loss_loc_all/num_samples, accuracy/num_samples</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义测试函数，输入参数为模型、验证数据集和命令行参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">model, val_dataset, args</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    num_batches = <span class="built_in">len</span>(val_dataset) // args.batch_size <span class="comment"># 计算批次数量</span></span><br><span class="line">    indexes = np.random.shuffle(np.arange(<span class="built_in">len</span>(val_dataset))) <span class="comment"># 随机打乱数据集的索引</span></span><br><span class="line">    <span class="comment"># 初始化损失和准确率</span></span><br><span class="line">    loss_all = <span class="number">0</span></span><br><span class="line">    loss_cat_all = <span class="number">0</span></span><br><span class="line">    loss_loc_all = <span class="number">0</span></span><br><span class="line">    accuracy = <span class="number">0</span></span><br><span class="line">    num_samples = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 遍历每个批次</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_batches):</span><br><span class="line">        <span class="comment"># 初始化图像、区域、ROI索引、相对位置和类别列表</span></span><br><span class="line">        imgs = []</span><br><span class="line">        rects = []</span><br><span class="line">        roi_idxs = []</span><br><span class="line">        rela_locs = []</span><br><span class="line">        cats = []</span><br><span class="line">        <span class="comment"># 遍历每个样本</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(args.batch_size):</span><br><span class="line">            <span class="comment"># 加载图像、区域、ROI索引长度、相对位置和类别</span></span><br><span class="line">            img, rect, roi_idx_len, rela_loc, cat = val_dataset[i *</span><br><span class="line">                                                               args.batch_size+j]</span><br><span class="line">            <span class="comment"># print(img, rect, roi_idx_len, gt_rect, cat)</span></span><br><span class="line">            <span class="comment"># 添加到对应的列表中</span></span><br><span class="line">            imgs.append(img.unsqueeze(<span class="number">0</span>))</span><br><span class="line">            rects += rect</span><br><span class="line">            rela_locs += rela_loc</span><br><span class="line">            roi_idxs += ([j] * roi_idx_len)</span><br><span class="line">            cats += cat</span><br><span class="line">        <span class="comment"># 将列表转换为张量或数组</span></span><br><span class="line">        imgs = torch.cat(imgs, dim=<span class="number">0</span>)</span><br><span class="line">        rects = np.array(rects)</span><br><span class="line">        rela_locs = torch.FloatTensor(rela_locs)</span><br><span class="line">        cats = torch.LongTensor(cats)</span><br><span class="line">        <span class="comment"># print(imgs, rects, roi_idxs, rela_locs, cats)</span></span><br><span class="line">        <span class="comment"># 如果使用CUDA，则将张量移动到GPU</span></span><br><span class="line">        <span class="keyword">if</span> args.cuda:</span><br><span class="line">            imgs = imgs.cuda()</span><br><span class="line">            rela_locs = rela_locs.cuda()</span><br><span class="line">            cats = cats.cuda()</span><br><span class="line">        <span class="comment"># 前向传播，计算预测的概率和边界框</span></span><br><span class="line">        prob, bbox = model.forward(imgs, rects, roi_idxs)</span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        loss, loss_cat, loss_loc = model.loss(prob, bbox, cats, rela_locs)</span><br><span class="line">        <span class="comment"># 更新损失和准确率</span></span><br><span class="line">        num_samples += <span class="built_in">len</span>(cats)</span><br><span class="line">        loss_all += loss.item() * <span class="built_in">len</span>(cats)</span><br><span class="line">        loss_cat_all += loss_cat.item() * <span class="built_in">len</span>(cats)</span><br><span class="line">        loss_loc_all += loss_loc.item() * <span class="built_in">len</span>(cats)</span><br><span class="line">        accuracy += (torch.argmax(prob.detach(), dim=-<span class="number">1</span>) == cats).<span class="built_in">sum</span>().item()</span><br><span class="line">    <span class="comment"># 返回模型、损失和准确率</span></span><br><span class="line">    <span class="keyword">return</span> model, loss_all/num_samples, loss_cat_all/num_samples, loss_loc_all/num_samples, accuracy/num_samples</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    parser = argparse.ArgumentParser(<span class="string">&#x27;parser for fast-rcnn&#x27;</span>) <span class="comment"># 创建一个命令行参数解析器</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">16</span>) <span class="comment"># 添加一个命令行参数--batch_size，用于指定批次大小，默认值为16</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--num_classes&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">10</span>) <span class="comment"># 添加一个命令行参数--num_classes，用于指定类别数量，默认值为10</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--learning_rate&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">2e-4</span>) <span class="comment"># 添加一个命令行参数--learning_rate，用于指定学习率，默认值为2e-4</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">20</span>) <span class="comment"># 添加一个命令行参数--epochs，用于指定训练的轮数，默认值为20</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--save_path&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">                        default=<span class="string">&#x27;./model/fast_rcnn.pkl&#x27;</span>) <span class="comment"># 添加一个命令行参数--save_path，用于指定模型保存的路径，默认值为&#x27;./model/fast_rcnn.pkl&#x27;</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--cuda&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">bool</span>, default=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    args = parser.parse_args() <span class="comment"># 解析命令行参数，并将结果保存在args中</span></span><br><span class="line">    train_dataset = COCOdataset(mode=<span class="string">&#x27;train&#x27;</span>) <span class="comment"># 解析命令行参数，并将结果保存在args中</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-----------------&quot;</span>,train_dataset.__len__())</span><br><span class="line">    valid_dataset = COCOdataset(mode=<span class="string">&#x27;val&#x27;</span>) <span class="comment"># 创建一个COCOdataset实例，模式为&#x27;val&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-----------------&quot;</span>, valid_dataset.__len__())</span><br><span class="line">    model = FastRCNN(num_classes=args.num_classes) <span class="comment"># 创建一个FastRCNN模型实例，类别数量为args.num_classes</span></span><br><span class="line">    <span class="keyword">if</span> args.cuda:<span class="comment"># 如果args.cuda为True，则将模型移动到GPU上</span></span><br><span class="line">        model.cuda()</span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate) <span class="comment"># 创建一个Adam优化器，优化目标为模型的参数，学习率为args.learning_rate</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.epochs):<span class="comment"># 对于每一个训练轮次</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Epoch %d:&quot;</span> % epoch)<span class="comment"># 打印当前轮次</span></span><br><span class="line">        model, train_loss, train_loss_cat, train_loss_loc, train_accuracy = train(</span><br><span class="line">            model, train_dataset, optimizer, args) <span class="comment"># 调用train函数进行训练，并获取模型、总损失、类别损失、位置损失和准确率</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Train: loss=%.4f, loss_cat=%.4f, loss_loc=%.4f, accuracy=%.4f&quot;</span> %</span><br><span class="line">              (train_loss, train_loss_cat, train_loss_loc, train_accuracy))<span class="comment"># 打印训练的损失和准确率</span></span><br><span class="line">        model, valid_loss, valid_loss_cat, valid_loss_loc, valid_accuracy = test(</span><br><span class="line">            model, valid_dataset, args)<span class="comment"># 打印训练的损失和准确率</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Valid: loss=%.4f, loss_cat=%.4f, loss_loc=%.4f, accuracy=%.4f&quot;</span> %</span><br><span class="line">              (valid_loss, valid_loss_cat, valid_loss_loc, valid_accuracy))<span class="comment"># 打印验证的损失和准确率</span></span><br><span class="line"></span><br><span class="line">    torch.save(model.state_dict(), args.save_path) <span class="comment"># 将模型的状态字典保存到args.save_path指定的路径</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main() <span class="comment"># 如果当前脚本被直接运行，则调用main函数</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="测试">测试</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> skimage</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageDraw, ImageFont</span><br><span class="line"><span class="keyword">from</span> selectivesearch <span class="keyword">import</span> selective_search <span class="comment"># 从selectivesearch模块导入selective_search函数，用于进行选择性搜索</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fast_rcnn <span class="keyword">import</span> FastRCNN</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试时如果用gpu，则：</span></span><br><span class="line"><span class="comment"># 1. 将模型加载到gpu上：trained_net = torch.load(args.model)，不加 map_location = &#x27;cpu&#x27;</span></span><br><span class="line"><span class="comment"># 2. 把参数--cuda的默认值改为True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义计算IoU（交并比）的函数，输入参数为两个矩形框</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cal_iou</span>(<span class="params">a, b</span>):</span><br><span class="line">    a_min_x, a_min_y, a_max_x, a_max_y = a</span><br><span class="line">    b_min_x, b_min_y, b_max_x, b_max_y = b</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">min</span>(a_max_y, b_max_y) &lt; <span class="built_in">max</span>(a_min_y, b_min_y) <span class="keyword">or</span> <span class="built_in">min</span>(a_max_x, b_max_x) &lt; <span class="built_in">max</span>(a_min_x, b_min_x):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        intersect_area = (<span class="built_in">min</span>(a_max_y, b_max_y) - <span class="built_in">max</span>(a_min_y, b_min_y) + <span class="number">1</span>) * \</span><br><span class="line">            (<span class="built_in">min</span>(a_max_x, b_max_x) - <span class="built_in">max</span>(a_min_x, b_min_x) + <span class="number">1</span>)</span><br><span class="line">        union_area = (a_max_x - a_min_x + <span class="number">1</span>) * (a_max_y - a_min_y + <span class="number">1</span>) + \</span><br><span class="line">            (b_max_x - b_min_x + <span class="number">1</span>) * (b_max_y - b_min_y + <span class="number">1</span>) - intersect_area</span><br><span class="line">    <span class="keyword">return</span> intersect_area / union_area</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    parser = argparse.ArgumentParser(<span class="string">&#x27;parser for testing fast-rcnn&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--jpg_path&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,</span><br><span class="line">                        default=<span class="string">&#x27;D:\\WritePapers\\object_detection_basics\\Datasets\\COCO2017\\val2017\\000000241326.jpg&#x27;</span>)<span class="comment"># 添加一个命令行参数--jpg_path，用于指定待测试的图像的路径，默认值为&#x27;/devdata/project/ai_learn/COCO2017/val2017/000000241326.jpg&#x27;</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--save_path&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;sample.png&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--save_type&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;png&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--model&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;./model/fast_rcnn.pkl&#x27;</span>) <span class="comment"># 添加一个命令行参数--model，用于指定模型的路径，默认值为&#x27;./model/fast_rcnn.pkl&#x27;</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--num_classes&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">10</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--scale&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">30.0</span>) <span class="comment"># 添加一个命令行参数--scale，用于指定选择性搜索的尺度，默认值为30.0</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--sigma&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.8</span>) <span class="comment"># 添加一个命令行参数--sigma，用于指定选择性搜索的高斯平滑参数，默认值为0.8</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--min_size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">50</span>) <span class="comment"># 添加一个命令行参数--min_size，用于指定选择性搜索的最小区域大小，默认值为50</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--cats&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, nargs=<span class="string">&#x27;*&#x27;</span>, default=[</span><br><span class="line">                        <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;sheep&#x27;</span>, <span class="string">&#x27;cow&#x27;</span>, <span class="string">&#x27;elephant&#x27;</span>, <span class="string">&#x27;bear&#x27;</span>, <span class="string">&#x27;zebra&#x27;</span>, <span class="string">&#x27;giraffe&#x27;</span>]) <span class="comment"># 添加一个命令行参数--cats，用于指定需要处理的类别，默认值为一系列动物的名称</span></span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--cuda&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">bool</span>, default=<span class="literal">False</span>)</span><br><span class="line">    args = parser.parse_args() <span class="comment"># 解析命令行参数，并将结果保存在args中</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># trained_net = torch.load(args.model) # 加载模型</span></span><br><span class="line">    trained_net = torch.load(args.model, map_location = <span class="string">&#x27;cpu&#x27;</span>) <span class="comment"># 加载模型</span></span><br><span class="line"></span><br><span class="line">    model = FastRCNN(num_classes=args.num_classes) <span class="comment"># 创建一个FastRCNN模型实例，类别数量为args.num_classes</span></span><br><span class="line">    model.load_state_dict(trained_net) <span class="comment"># 将加载的模型的状态字典加载到FastRCNN模型实例中</span></span><br><span class="line">    <span class="keyword">if</span> args.cuda: <span class="comment"># 如果args.cuda为True，则将模型移动到GPU上</span></span><br><span class="line">        model.cuda()</span><br><span class="line"></span><br><span class="line">    img = skimage.io.imread(args.jpg_path) <span class="comment"># 读取图像</span></span><br><span class="line">    h = img.shape[<span class="number">0</span>] <span class="comment"># 获取图像的高度</span></span><br><span class="line">    w = img.shape[<span class="number">1</span>] <span class="comment"># 获取图像的宽度</span></span><br><span class="line">    _, ss_regions = selective_search(</span><br><span class="line">        img, args.scale, args.sigma, args.min_size) <span class="comment"># 对图像进行选择性搜索，获取候选区域</span></span><br><span class="line">    rois = []</span><br><span class="line">    <span class="keyword">for</span> region <span class="keyword">in</span> ss_regions: <span class="comment"># 遍历每个候选区域</span></span><br><span class="line">        rect = <span class="built_in">list</span>(region[<span class="string">&#x27;rect&#x27;</span>]) <span class="comment"># 获取候选区域的矩形框</span></span><br><span class="line">        rect[<span class="number">0</span>] = rect[<span class="number">0</span>] / w <span class="comment"># 将矩形框的x坐标转换为相对于图像宽度的比例</span></span><br><span class="line">        rect[<span class="number">1</span>] = rect[<span class="number">1</span>] / h <span class="comment"># 将矩形框的y坐标转换为相对于图像高度的比例</span></span><br><span class="line">        rect[<span class="number">2</span>] = rect[<span class="number">0</span>] + rect[<span class="number">2</span>] / w <span class="comment"># 将矩形框的宽度转换为相对于图像宽度的比例</span></span><br><span class="line">        rect[<span class="number">3</span>] = rect[<span class="number">1</span>] + rect[<span class="number">3</span>] / h <span class="comment"># 将矩形框的高度转换为相对于图像高度的比例</span></span><br><span class="line">        rois.append(rect) <span class="comment"># 将处理后的矩形框添加到列表中</span></span><br><span class="line">    img = Image.fromarray(img) <span class="comment"># 将图像数组转换为PIL图像</span></span><br><span class="line">    img_tensor = img.resize([<span class="number">224</span>, <span class="number">224</span>]) <span class="comment"># 将图像大小调整为224x224</span></span><br><span class="line">    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([</span><br><span class="line">                                   <span class="number">0.485</span>, <span class="number">0.456</span>, -<span class="number">.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])]) <span class="comment"># 创建一个图像预处理管道，包括将PIL图像转换为张量和标准化</span></span><br><span class="line">    img_tensor = transform(img_tensor).unsqueeze(<span class="number">0</span>) <span class="comment"># 对图像进行预处理，并添加一个新的维度</span></span><br><span class="line">    <span class="keyword">if</span> args.cuda: <span class="comment"># 如果args.cuda为True，则将图像张量移动到GPU</span></span><br><span class="line">        img_tensor = img_tensor.cuda()</span><br><span class="line">    rois = np.array(rois) <span class="comment"># 将候选区域的列表转换为数组</span></span><br><span class="line">    roi_idx = [<span class="number">0</span>] * rois.shape[<span class="number">0</span>] <span class="comment"># 创建一个列表，长度为候选区域的数量，所有元素都为0</span></span><br><span class="line"></span><br><span class="line">    prob, rela_loc = model.forward(img_tensor, rois, roi_idx) <span class="comment"># 前向传播，计算预测的概率和边界框</span></span><br><span class="line">    prob = torch.nn.Softmax(dim=-<span class="number">1</span>)(prob).cpu().detach().numpy() <span class="comment"># 对预测的概率进行softmax操作，并将结果转换为numpy数组</span></span><br><span class="line">    <span class="comment"># rela_loc = rela_loc.cpu().detach().numpy()[:, 1:, :].mean(axis=1)</span></span><br><span class="line">    labels = []</span><br><span class="line">    max_probs = []</span><br><span class="line">    bboxs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(prob)): <span class="comment"># 遍历每个预测的概率</span></span><br><span class="line">        <span class="keyword">if</span> prob[i].<span class="built_in">max</span>() &gt; <span class="number">0.8</span> <span class="keyword">and</span> np.argmax(prob[i], axis=<span class="number">0</span>) != <span class="number">0</span>: <span class="comment"># 如果最大概率大于0.8且对应的类别不是背景，则认为候选区域是有效的</span></span><br><span class="line">            <span class="comment"># proposal regions is directly used because of limited training epochs, bboxs predicted are not precise</span></span><br><span class="line">            <span class="comment"># bbox = [(rois[i][2] - rois[i][0]) * rela_loc[i][0] + 0.5 * (rois[i][2] + rois[i][0]),</span></span><br><span class="line">            <span class="comment">#         (rois[i][3] - rois[i][1]) * rela_loc[i][1] + 0.5 * (rois[i][3] + rois[i][1]),</span></span><br><span class="line">            <span class="comment">#         np.exp(rela_loc[i][2]) * rois[i][2],</span></span><br><span class="line">            <span class="comment">#         np.exp(rela_loc[i][3]) * rois[i][3]]</span></span><br><span class="line">            <span class="comment"># bbox = [bbox[0] - 0.5 * bbox[2],</span></span><br><span class="line">            <span class="comment">#         bbox[1] - 0.5 * bbox[3],</span></span><br><span class="line">            <span class="comment">#         bbox[0] + 0.5 * bbox[2],</span></span><br><span class="line">            <span class="comment">#         bbox[1] + 0.5 * bbox[3]]</span></span><br><span class="line">            labels.append(np.argmax(prob[i], axis=<span class="number">0</span>))  <span class="comment"># 将有效候选区域的类别添加到列表中</span></span><br><span class="line">            max_probs.append(prob[i].<span class="built_in">max</span>()) <span class="comment"># 将有效候选区域的最大概率添加到列表中</span></span><br><span class="line">            rois[i] = [<span class="built_in">int</span>(w * rois[i][<span class="number">0</span>]), <span class="built_in">int</span>(h * rois[i][<span class="number">1</span>]),</span><br><span class="line">                       <span class="built_in">int</span>(w * rois[i][<span class="number">2</span>]), <span class="built_in">int</span>(w * rois[i][<span class="number">3</span>])] <span class="comment"># 将候选区域的矩形框的坐标和大小转换为相对于原图的像素值</span></span><br><span class="line">            bboxs.append(rois[i]) <span class="comment"># 将处理后的矩形框添加到列表中</span></span><br><span class="line">    labels = np.array(labels) <span class="comment"># 将类别的列表转换为数组</span></span><br><span class="line">    max_probs = np.array(max_probs) <span class="comment"># 将最大概率的列表转换为数组</span></span><br><span class="line">    bboxs = np.array(bboxs) <span class="comment"># 将矩形框的列表转换为数组</span></span><br><span class="line">    order = np.argsort(-max_probs) <span class="comment"># 对最大概率进行降序排序，获取排序后的索引</span></span><br><span class="line">    labels = labels[order] <span class="comment"># 根据排序的索引重新排序类别</span></span><br><span class="line">    max_probs = max_probs[order]</span><br><span class="line">    bboxs = bboxs[order]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 下面的代码是进行非极大值抑制（NMS），用于去除重叠的候选区域</span></span><br><span class="line">    nms_labels = []</span><br><span class="line">    nms_probs = []</span><br><span class="line">    nms_bboxs = []</span><br><span class="line">    del_indexes = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(labels)):</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> del_indexes:</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(labels)):</span><br><span class="line">                <span class="keyword">if</span> j <span class="keyword">not</span> <span class="keyword">in</span> del_indexes <span class="keyword">and</span> cal_iou(bboxs[i], bboxs[j]) &gt; <span class="number">0.3</span>:</span><br><span class="line">                    del_indexes.append(j)</span><br><span class="line">            nms_labels.append(labels[i])</span><br><span class="line">            nms_probs.append(max_probs[i])</span><br><span class="line">            nms_bboxs.append(bboxs[i])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将预测结果绘制到图像上</span></span><br><span class="line">    cat_dict = &#123;(i + <span class="number">1</span>): args.cats[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(args.cats))&#125;</span><br><span class="line">    cat_dict[<span class="number">0</span>] = <span class="string">&#x27;background&#x27;</span></span><br><span class="line">    font = ImageFont.truetype(<span class="string">&#x27;./fonts/chinese_cht.ttf&#x27;</span>, size=<span class="number">16</span>)</span><br><span class="line">    draw = ImageDraw.Draw(img)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nms_labels)):</span><br><span class="line">        draw.polygon([(nms_bboxs[i][<span class="number">0</span>], nms_bboxs[i][<span class="number">1</span>]), (nms_bboxs[i][<span class="number">2</span>], nms_bboxs[i][<span class="number">1</span>]),</span><br><span class="line">                      (nms_bboxs[i][<span class="number">2</span>], nms_bboxs[i][<span class="number">3</span>]), (nms_bboxs[i][<span class="number">0</span>], nms_bboxs[i][<span class="number">3</span>])], outline=(<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">        draw.text((nms_bboxs[i][<span class="number">0</span>] + <span class="number">5</span>, nms_bboxs[i][<span class="number">1</span>] + <span class="number">5</span>), <span class="string">&#x27;%s %.2f%%&#x27;</span> % (</span><br><span class="line">            cat_dict[nms_labels[i]], <span class="number">100</span> * max_probs[i]), fill=(<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>), font=font)</span><br><span class="line">    img.save(args.save_path, args.save_type) <span class="comment"># 将绘制了预测结果的图像保存到指定的路径</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>测试结果如下： <img src="https://ooo.0x0.ooo/2023/12/29/OKYvBc.png"
alt="OKYvBc.png" /></p>
<p>参考资料</p>
<blockquote>
<p>1.博客：<a
href="https://blog.csdn.net/guoqingru0311/article/details/129584426">目标检测
pytorch复现Fast_RCNN目标检测项目-CSDN博客</a></p>
<p>2.COCO数据集下载：</p>
<p>``` 训练集： http://images.cocodataset.org/zips/train2017.zip
验证集： http://images.cocodataset.org/zips/val2017.zip
训练集和验证集对应的标签：
http://images.cocodataset.org/annotations/annotations_trainval2017.zip
测试集： http://images.cocodataset.org/zips/test2017.zip</p>
</blockquote>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>Fast RCNN</tag>
        <tag>代码复现</tag>
      </tags>
  </entry>
  <entry>
    <title>Faster RCNN代码分析（二）</title>
    <url>/2023/12/29/Faster-RCNN%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"><h1 id="fasterrcnn代码分析">FasterRCNN代码分析</h1>
<p><strong>项目源码</strong>：https://github.com/chenyuntc/simple-faster-rcnn-pytorch</p>
<p><strong>对源码增加注释后的代码（代码2）：</strong></p>
<p><code>simple-faster-rcnn-pytorch-master</code>
https://www.alipan.com/s/faJGPR261aG 提取码: ry92
点击链接保存，或者复制本段内容，打开「阿里云盘」APP
，无需下载极速在线查看，视频原画倍速播放。
<strong>代码2</strong>只要将VOCdevkit数据集放到dataset目录下即可运行</p>
<p>这个项目是一个基于Faster
R-CNN模型的目标检测项目，主要包含以下几个部分：</p>
<span id="more"></span>
<ol type="1">
<li><p><code>utils/config.py</code>：这个文件包含了项目的配置选项，例如数据集路径、学习率、优化器类型、预训练模型的路径等。</p></li>
<li><p><code>data/dataset.py</code>：这个文件包含了处理PASCAL
VOC数据集的类，例如<code>Dataset</code>、<code>TestDataset</code>。这些类用于加载数据集（这里会调用<code>data/voc_dataset.py</code>中的<code>VOCBboxDataset</code>），对图像和标签进行预处理，并提供了用于训练和测试模型的接口。</p></li>
<li><p><code>model/faster_rcnn.py</code>：这个文件包含了Faster
R-CNN模型的实现。这个模型由三个主要部分组成：特征提取器、RPN网络和头部网络。特征提取器用于从图像中提取特征，RPN网络用于生成目标的候选区域，头部网络用于在这些候选区域上进行分类和回归。</p></li>
<li><p><code>model/faster_rcnn_vgg16.py</code>：这个文件包含了基于VGG-16的Faster
R-CNN模型<code>FasterRCNNVGG16</code>（继承了<code>model/faster_rcnn.py</code>中的<code>FasterRCNN</code>）。这个模型由三个主要部分组成：基于VGG-16的特征提取器、RPN网络和<code>VGG16RoIHead</code>头部网络。特征提取器用于从图像中提取特征，RPN网络用于生成目标的候选区域，头部网络用于在这些候选区域上进行分类和回归。</p></li>
<li><p><code>model/region_proposal_network.py</code>：这个文件包含了Region
Proposal Network（RPN）的实现。RPN是Faster
R-CNN模型的一个关键组件，它用于生成目标的候选区域。</p></li>
<li><p><code>model/utils/creator_tool.py</code>：这个文件包含了一些用于生成训练Faster
R-CNN模型所需的目标的工具类，例如<code>AnchorTargetCreator</code>和<code>ProposalTargetCreator</code>。</p></li>
<li><p><code>trainer.py</code>：这个文件包含了一个用于训练Faster
R-CNN模型的类<code>FasterRCNNTrainer</code>。这个类提供了一些方法，例如<code>train_step</code>用于执行一步训练，<code>save</code>和<code>load</code>用于保存和加载模型，<code>update_meters</code>和<code>reset_meters</code>用于更新和重置度量等。</p></li>
<li><p><code>train.py</code>：这个文件是项目的主程序，它首先加载数据集，然后创建Faster
R-CNN模型和训练器，接着进入一个循环，每个循环代表一个训练周期，在每个训练周期中，它会遍历数据集中的所有图像，并使用训练器的<code>train_step</code>方法来更新模型的参数。</p></li>
</ol>
<p>这些文件之间的关系主要是通过数据和模型的流动来实现的。首先，<code>train.py</code>会加载<code>dataset.py</code>中的数据集，然后使用<code>model/faster_rcnn.py</code>中的模型对数据进行处理，接着使用<code>trainer.py</code>中的训练器对模型进行训练。在训练过程中，<code>model/utils/creator_tool.py</code>中的工具类会被用来生成训练所需的目标，<code>model/region_proposal_network.py</code>中的RPN会被用来生成目标的候选区域。</p>
<p>1.<code>train.py</code></p>
<p>在<code>main</code>函数中调用<code>train()</code>方法，<code>train()</code>方法的主要步骤为：</p>
<p>（1）加载数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset = Dataset(opt)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;load data&#x27;</span>)</span><br><span class="line">dataloader = data_.DataLoader(dataset, \</span><br><span class="line">                              batch_size=<span class="number">1</span>, \</span><br><span class="line">                              shuffle=<span class="literal">True</span>, \</span><br><span class="line">                              <span class="comment"># pin_memory=True,</span></span><br><span class="line">                              num_workers=opt.num_workers)</span><br><span class="line">testset = TestDataset(opt)</span><br><span class="line">test_dataloader = data_.DataLoader(testset,</span><br><span class="line">                                   batch_size=<span class="number">1</span>,</span><br><span class="line">                                   num_workers=opt.test_num_workers,</span><br><span class="line">                                   shuffle=<span class="literal">False</span>, \</span><br><span class="line">                                   pin_memory=<span class="literal">True</span></span><br><span class="line">                                  )</span><br></pre></td></tr></table></figure>
<p>（2）创建Faster RCNN模型及其训练器</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">faster_rcnn = FasterRCNNVGG16()<span class="comment"># 创建Faster R-CNN模型对象</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;model construct completed&#x27;</span>)</span><br><span class="line">trainer = FasterRCNNTrainer(faster_rcnn).cuda() <span class="comment"># 创建Faster R-CNN模型的训练器</span></span><br><span class="line"><span class="keyword">if</span> opt.load_path:<span class="comment"># 如果指定了预训练模型的路径</span></span><br><span class="line">   trainer.load(opt.load_path)<span class="comment"># 加载预训练模型</span></span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&#x27;load pretrained model from %s&#x27;</span> % opt.load_path)</span><br></pre></td></tr></table></figure>
<p>（3）开启训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_map = <span class="number">0</span> <span class="comment"># 初始化最佳mAP为0</span></span><br><span class="line">lr_ = opt.lr <span class="comment"># 获取初始学习率 lr=0.001</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(opt.epoch):</span><br><span class="line">   trainer.reset_meters() <span class="comment"># 重置训练器的度量计数器</span></span><br><span class="line">   <span class="keyword">for</span> ii, (img, bbox_, label_, scale) <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(dataloader)): <span class="comment"># 对数据加载器中的每个batch进行循环 ii为批次索引 img==tensor(1,3,800,600) bbox_==tensor(1,1,4) label_==tensor(1,1) scale==(1,)</span></span><br><span class="line">        scale = at.scalar(scale) <span class="comment"># 获取缩放因子</span></span><br><span class="line">        img, bbox, label = img.cuda().<span class="built_in">float</span>(), bbox_.cuda(), label_.cuda() <span class="comment"># 将图像、边界框（ground_truth）和标签移动到GPU上，并将图像转换为浮点类型</span></span><br><span class="line">        trainer.train_step(img, bbox, label, scale) <span class="comment"># 执行一个训练步骤</span></span><br></pre></td></tr></table></figure>
<p>（4）评估训练结果并在visdom中可视化展示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eval_result = <span class="built_in">eval</span>(test_dataloader, faster_rcnn, test_num=opt.test_num)<span class="comment"># 对测试数据集进行评估</span></span><br><span class="line">trainer.vis.plot(<span class="string">&#x27;test_map&#x27;</span>, eval_result[<span class="string">&#x27;map&#x27;</span>])<span class="comment"># 在visdom中绘制mAP</span></span><br><span class="line">lr_ = trainer.faster_rcnn.optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>]<span class="comment"># 获取当前的学习率</span></span><br><span class="line">log_info = <span class="string">&#x27;lr:&#123;&#125;, map:&#123;&#125;,loss:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(lr_),</span><br><span class="line">                                          <span class="built_in">str</span>(eval_result[<span class="string">&#x27;map&#x27;</span>]),</span><br><span class="line">                                          <span class="built_in">str</span>(trainer.get_meter_data()))<span class="comment"># 生成日志信息</span></span><br><span class="line">trainer.vis.log(log_info)<span class="comment"># 在visdom中显示日志信息</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> eval_result[<span class="string">&#x27;map&#x27;</span>] &gt; best_map:<span class="comment"># 如果当前的mAP大于最佳mAP</span></span><br><span class="line">    best_map = eval_result[<span class="string">&#x27;map&#x27;</span>]<span class="comment"># 更新最佳mAP</span></span><br><span class="line">    best_path = trainer.save(best_map=best_map)<span class="comment"># 保存当前的模型，并获取保存路径</span></span><br><span class="line">    <span class="keyword">if</span> epoch == <span class="number">9</span>:<span class="comment"># 如果当前是第10个训练周期</span></span><br><span class="line">        trainer.load(best_path)<span class="comment"># 加载最佳模型</span></span><br><span class="line">        trainer.faster_rcnn.scale_lr(opt.lr_decay)<span class="comment"># 调整学习率</span></span><br><span class="line">        lr_ = lr_ * opt.lr_decay <span class="comment"># 更新当前的学习率</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">13</span>: <span class="comment"># 如果当前是第14个训练周期</span></span><br><span class="line">            <span class="keyword">break</span><span class="comment"># 结束训练</span></span><br></pre></td></tr></table></figure>
<p>2.<code>trainer.py</code></p>
<p>在<code>train.py</code>的<code>train()</code>开启训练后，<code>trainer.train_step(img, bbox, label, scale)</code>
会执行一个训练步骤，即进入<code>trainer.py</code>的<code>train_step()</code>方法，进而通过内部的<code>losses = self.forward(imgs, bboxes, labels, scale)</code>进行前向传播，前向传播的主要步骤为：</p>
<p>（1）使用<code>Faster R-CNN</code>的特征提取器提取特征</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">features = self.faster_rcnn.extractor(imgs)  <span class="comment"># 使用Faster R-CNN的特征提取器提取特征</span></span><br></pre></td></tr></table></figure>
<p>（2）使用RPN生成RoIs(初步筛选后得到每张图片约2000个RoI)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># RPN</span></span><br><span class="line"><span class="comment"># rpn_locs代表所有偏移锚框的位置，形状为(1, hh*ww*9, 4)</span></span><br><span class="line"><span class="comment"># rpn_scores代表所有偏移锚框的得分下，形状为(1, hh*ww*9, 2)</span></span><br><span class="line"><span class="comment"># rois代表所有的RoIs(感兴趣区域)，形状约为(2000, 4)</span></span><br><span class="line"><span class="comment"># roi_indices代表RoIs对应的图像索引，形状约为(2000, ) 表明rois中的每个RoI都对应于哪张图片（这里batch=1，每次都来自第0张图片）</span></span><br><span class="line">rpn_locs, rpn_scores, rois, roi_indices, anchor = \</span><br><span class="line">	self.faster_rcnn.rpn(features, img_size, scale) <span class="comment"># 使用RPN生成RoIs</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RegionProposalNetwork</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">            self, in_channels=<span class="number">512</span>, mid_channels=<span class="number">512</span>, ratios=[<span class="number">0.5</span>, <span class="number">1</span>, <span class="number">2</span>],</span></span><br><span class="line"><span class="params">            anchor_scales=[<span class="number">8</span>, <span class="number">16</span>, <span class="number">32</span>], feat_stride=<span class="number">16</span>,</span></span><br><span class="line"><span class="params">            proposal_creator_params=<span class="built_in">dict</span>(<span class="params"></span>),</span></span><br><span class="line"><span class="params">    </span>):  <span class="comment"># 初始化方法，接受输入通道数、中间通道数、长宽比、锚框尺度、特征步长和proposal创建器参数作为参数</span></span><br><span class="line">        <span class="built_in">super</span>(RegionProposalNetwork, self).__init__()</span><br><span class="line">        self.anchor_base = generate_anchor_base(</span><br><span class="line">            anchor_scales=anchor_scales, ratios=ratios)<span class="comment"># 生成基础参考框anchor_base==&gt;shape(9,4)</span></span><br><span class="line">        self.feat_stride = feat_stride <span class="comment"># 设置特征步长</span></span><br><span class="line">        self.proposal_layer = ProposalCreator(self, **proposal_creator_params)  <span class="comment"># 创建proposal创建器</span></span><br><span class="line">        n_anchor = self.anchor_base.shape[<span class="number">0</span>]  <span class="comment"># 获取锚框数量 n_anchor ==&gt; 9</span></span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels, mid_channels, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>) <span class="comment"># 创建一个卷积层，用于特征提取 ==&gt; shape(512,512,3,1,1) kernel_size=(3,3), stride=(1,1), padding=(1,1)</span></span><br><span class="line">        self.score = nn.Conv2d(mid_channels, n_anchor * <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>) <span class="comment"># 创建一个卷积层，用于计算锚框的得分 ==&gt; shape(512,9*2,1,1,0)</span></span><br><span class="line">        self.loc = nn.Conv2d(mid_channels, n_anchor * <span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>) <span class="comment"># 创建一个卷积层，用于计算锚框的位置 ==&gt; shape(512,9*4,1,1,0)</span></span><br><span class="line">        normal_init(self.conv1, <span class="number">0</span>, <span class="number">0.01</span>) <span class="comment"># 初始化conv1的权重</span></span><br><span class="line">        normal_init(self.score, <span class="number">0</span>, <span class="number">0.01</span>) <span class="comment"># 初始化score的权重</span></span><br><span class="line">        normal_init(self.loc, <span class="number">0</span>, <span class="number">0.01</span>) <span class="comment"># 初始化loc的权重</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, img_size, scale=<span class="number">1.</span></span>):<span class="comment"># 定义前向传播方法，接受FasterRCNN特征提取得到的(也是vgg16的)特征图features、图像尺寸和缩放因子作为参数 img_size==(800,600) scale==1</span></span><br><span class="line">      </span><br><span class="line">        n, _, hh, ww = x.shape <span class="comment"># 获取输入特征图x的形状</span></span><br><span class="line">        anchor = _enumerate_shifted_anchor(</span><br><span class="line">            np.array(self.anchor_base),</span><br><span class="line">            self.feat_stride, hh, ww) <span class="comment"># 枚举所有的偏移锚框，数量为特征图的像素数每个像素的锚框数==&gt;(hh*ww)*9</span></span><br><span class="line"></span><br><span class="line">        n_anchor = anchor.shape[<span class="number">0</span>] // (hh * ww) <span class="comment"># 计算每个像素的锚框数量</span></span><br><span class="line">        h = F.relu(self.conv1(x)) <span class="comment"># 对输入进行卷积操作并通过ReLU激活函数</span></span><br><span class="line"></span><br><span class="line">        rpn_locs = self.loc(h) <span class="comment"># 计算锚框的位置</span></span><br><span class="line">        <span class="comment"># UN<span class="doctag">NOTE:</span> check whether need contiguous</span></span><br><span class="line">        <span class="comment"># A: Yes</span></span><br><span class="line">        rpn_locs = rpn_locs.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous().view(n, -<span class="number">1</span>, <span class="number">4</span>) <span class="comment"># 调整rpn_locs的形状  rpn_locs代表所有偏移锚框的位置，形状为(1, hh*ww*9, 4)</span></span><br><span class="line">        rpn_scores = self.score(h) <span class="comment"># 计算锚框的得分 rpn_scores代表所有偏移锚框的得分</span></span><br><span class="line">        rpn_scores = rpn_scores.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).contiguous() <span class="comment"># 调整rpn_scores的形状</span></span><br><span class="line">        rpn_softmax_scores = F.softmax(rpn_scores.view(n, hh, ww, n_anchor, <span class="number">2</span>), dim=<span class="number">4</span>)  <span class="comment"># (1,hh,ww,9,2)# 对得分进行softmax操作，得到每个类别的概率  rpn_softmax_scores代表所有偏移锚框的得分，形状为(1, hh, ww, 9, 2)</span></span><br><span class="line">        rpn_fg_scores = rpn_softmax_scores[:, :, :, :, <span class="number">1</span>].contiguous()  <span class="comment"># 获取前景的得分</span></span><br><span class="line">        rpn_fg_scores = rpn_fg_scores.view(n, -<span class="number">1</span>)  <span class="comment"># 调整rpn_fg_scores的形状 rpn_fg_scores代表所有偏移锚框的前景得分，形状为(1, hh*ww*9)</span></span><br><span class="line">        rpn_scores = rpn_scores.view(n, -<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 调整rpn_scores的形状 (1, hh*ww*9, 2)</span></span><br><span class="line"></span><br><span class="line">        rois = <span class="built_in">list</span>() <span class="comment"># 创建一个列表，用于存储RoIs</span></span><br><span class="line">        roi_indices = <span class="built_in">list</span>() <span class="comment"># 创建一个列表，用于存储RoIs的索引</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n): <span class="comment"># 对每个图像进行循环</span></span><br><span class="line">            roi = self.proposal_layer(</span><br><span class="line">                rpn_locs[i].cpu().data.numpy(),</span><br><span class="line">                rpn_fg_scores[i].cpu().data.numpy(),</span><br><span class="line">                anchor, img_size,</span><br><span class="line">                scale=scale) <span class="comment"># 使用proposal创建器生成RoIs roi==&gt;(1944,4)</span></span><br><span class="line">            batch_index = i * np.ones((<span class="built_in">len</span>(roi),), dtype=np.int32) <span class="comment"># 创建一个数组，用于存储RoIs的索引</span></span><br><span class="line">            rois.append(roi) <span class="comment"># 将RoIs添加到列表中</span></span><br><span class="line">            roi_indices.append(batch_index) <span class="comment"># 将RoIs的索引添加到列表中</span></span><br><span class="line"></span><br><span class="line">        rois = np.concatenate(rois, axis=<span class="number">0</span>) <span class="comment"># 将所有图像的RoIs合并</span></span><br><span class="line">        roi_indices = np.concatenate(roi_indices, axis=<span class="number">0</span>) <span class="comment"># 将所有图像的RoIs的索引合并</span></span><br><span class="line">        <span class="keyword">return</span> rpn_locs, rpn_scores, rois, roi_indices, anchor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用处：获取偏移后的锚框</span></span><br><span class="line"><span class="comment"># 在Faster R-CNN中，我们首先在图像中生成一系列的锚点（也称为锚框或参考框）</span></span><br><span class="line"><span class="comment"># 这些锚点通常是在不同的位置、尺度和长宽比下生成的（称为基础锚点：3*3=9——特征图的每个像素处都会有9个锚点——即下文代码中的A=9）</span></span><br><span class="line"><span class="comment"># 然后，我们会预测每个锚点需要偏移多少，才能更好地匹配到真实的目标边界框，这个偏移的过程就是所谓的偏移锚点</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为什么需要偏移?</span></span><br><span class="line"><span class="comment"># 因为我们希望在特征图的每个位置都有一组锚点，这样可以更全面地覆盖到图像中的所有可能的目标。</span></span><br><span class="line"><span class="comment"># 如果只使用基础锚点，那么锚点的位置就只能在参考窗口的位置，这样可能会漏掉一些位于其他位置的目标。</span></span><br><span class="line"><span class="comment"># 通过偏移，我们可以让锚点覆盖到特征图的每个位置，从而更好地检测到所有的目标。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_enumerate_shifted_anchor</span>(<span class="params">anchor_base, feat_stride, height, width</span>):</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> xp</span><br><span class="line">    shift_y = xp.arange(<span class="number">0</span>, height * feat_stride, feat_stride) <span class="comment"># 计算y方向上的所有偏移</span></span><br><span class="line">    shift_x = xp.arange(<span class="number">0</span>, width * feat_stride, feat_stride) <span class="comment"># 计算x方向上的所有偏移</span></span><br><span class="line">    shift_x, shift_y = xp.meshgrid(shift_x, shift_y)  <span class="comment"># 生成网格坐标</span></span><br><span class="line">    shift = xp.stack((shift_y.ravel(), shift_x.ravel(),</span><br><span class="line">                      shift_y.ravel(), shift_x.ravel()), axis=<span class="number">1</span>) <span class="comment"># 将偏移堆叠成一个数组</span></span><br><span class="line"></span><br><span class="line">    A = anchor_base.shape[<span class="number">0</span>] <span class="comment"># 每个像素处会有A个锚点（A=9）</span></span><br><span class="line">    K = shift.shape[<span class="number">0</span>] <span class="comment"># 特征图的像素数量（K=hh*ww）==&gt; 整张特征图总的偏移量的数量=K*A</span></span><br><span class="line">    anchor = anchor_base.reshape((<span class="number">1</span>, A, <span class="number">4</span>)) + \</span><br><span class="line">             shift.reshape((<span class="number">1</span>, K, <span class="number">4</span>)).transpose((<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)) <span class="comment"># 将基础锚点和偏移相加，得到所有的偏移后的锚点</span></span><br><span class="line">    anchor = anchor.reshape((K * A, <span class="number">4</span>)).astype(np.float32) <span class="comment"># 调整偏移锚点的形状，并转换为浮点类型</span></span><br><span class="line">    <span class="keyword">return</span> anchor</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>（3）生成用于训练<code>Faster R-CNN</code>的head网络（即RoI网络）所需的目标：经IoU阈值筛选后得到128个ROIs(<code>sample_roi</code>)，使用<code>ProposalTargetCreator</code>为<code>sample_roi</code>分配真实边界框<code>gt_roi_loc</code>和真实标签<code>gt_roi_label</code>，此时就生成了用于训练<code>Faster R-CNN</code>的head网络（即RoI网络）所需的目标。这些目标包括每个RoI对应的ground
truth边界框的偏移和比例（用于边界框回归任务），以及每个RoI对应的ground
truth边界框的类别标签（用于分类任务）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sample_roi代表经过IoU阈值筛选后的RoIs，由前景roi和背景roi组成，形状为(128, 4)</span></span><br><span class="line"><span class="comment"># gt_roi_loc代表sample_roi与其对应的真实边界框（其实是与它IoU最大的真实边界框）之间的偏移和比例，形状为(128, 4)</span></span><br><span class="line"><span class="comment"># gt_roi_label代表sample_roi对应的真实标签（其实是与它IoU最大的真实边界框的标签），形状为(128,)</span></span><br><span class="line">sample_roi, gt_roi_loc, gt_roi_label = self.proposal_target_creator(</span><br><span class="line">            roi,</span><br><span class="line">            at.tonumpy(bbox),</span><br><span class="line">            at.tonumpy(label),</span><br><span class="line">            self.loc_normalize_mean,</span><br><span class="line">            self.loc_normalize_std)<span class="comment"># 使用ProposalTargetCreator生成训练目标</span></span><br></pre></td></tr></table></figure>
<p><code>utils/creator_tool.py</code>中的类<code>ProposalTargetCreator</code>的代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 用处：为给定的RoIs分配ground truth边界框</span></span><br><span class="line"><span class="comment"># 1.计算RoIs和边界框的IoU</span></span><br><span class="line"><span class="comment"># 2.找出每个RoI与哪个边界框的IoU最大</span></span><br><span class="line"><span class="comment"># 3.将每个RoI分配给与其IoU最大的边界框</span></span><br><span class="line"><span class="comment"># 4.计算这些RoI与其对应的边界框的偏移和比例</span></span><br><span class="line"><span class="comment"># 5.对偏移和比例进行归一化</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ProposalTargetCreator</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 n_sample=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">                 pos_ratio=<span class="number">0.25</span>, pos_iou_thresh=<span class="number">0.5</span>,</span></span><br><span class="line"><span class="params">                 neg_iou_thresh_hi=<span class="number">0.5</span>, neg_iou_thresh_lo=<span class="number">0.0</span></span></span><br><span class="line"><span class="params">                 </span>):<span class="comment"># 初始化方法，接受一系列参数，包括采样区域的数量、前景的比例、前景的IoU阈值、背景的IoU阈值等</span></span><br><span class="line">        self.n_sample = n_sample  <span class="comment"># 采样区域的数量</span></span><br><span class="line">        self.pos_ratio = pos_ratio  <span class="comment"># 前景的比例</span></span><br><span class="line">        self.pos_iou_thresh = pos_iou_thresh  <span class="comment"># 前景的IoU阈值</span></span><br><span class="line">        self.neg_iou_thresh_hi = neg_iou_thresh_hi  <span class="comment"># 背景的IoU阈值上限</span></span><br><span class="line">        self.neg_iou_thresh_lo = neg_iou_thresh_lo  <span class="comment"># 背景的IoU阈值下限 <span class="doctag">NOTE:</span>default 0.1 in py-faster-rcnn</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, roi, bbox, label,</span></span><br><span class="line"><span class="params">        loc_normalize_mean=(<span class="params"><span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span></span>),</span></span><br><span class="line"><span class="params">        loc_normalize_std=(<span class="params"><span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.2</span></span>)</span>):<span class="comment"># 定义__call__方法，接受RoIs、边界框、标签、位置归一化的均值和标准差作为参数</span></span><br><span class="line">        n_bbox, _ = bbox.shape <span class="comment"># 获取边界框的数量</span></span><br><span class="line"></span><br><span class="line">        roi = np.concatenate((roi, bbox), axis=<span class="number">0</span>)  <span class="comment"># 将RoIs和边界框合并</span></span><br><span class="line"></span><br><span class="line">        pos_roi_per_image = np.<span class="built_in">round</span>(self.n_sample * self.pos_ratio)  <span class="comment"># 计算每个图像的前景RoI数量</span></span><br><span class="line">        iou = bbox_iou(roi, bbox)  <span class="comment"># 计算RoIs和边界框的IoU</span></span><br><span class="line">        gt_assignment = iou.argmax(axis=<span class="number">1</span>)  <span class="comment"># 找出每个RoI与哪个边界框的IoU最大</span></span><br><span class="line">        max_iou = iou.<span class="built_in">max</span>(axis=<span class="number">1</span>)  <span class="comment"># 找出每个RoI的最大IoU</span></span><br><span class="line">        <span class="comment"># Offset range of classes from [0, n_fg_class - 1] to [1, n_fg_class].</span></span><br><span class="line">        <span class="comment"># The label with value 0 is the background.</span></span><br><span class="line">        gt_roi_label = label[gt_assignment] + <span class="number">1</span> <span class="comment"># 将每个RoI分配给与其IoU最大的边界框的标签</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Select foreground RoIs as those with &gt;= pos_iou_thresh IoU.</span></span><br><span class="line">        pos_index = np.where(max_iou &gt;= self.pos_iou_thresh)[<span class="number">0</span>]  <span class="comment"># 找出IoU大于等于前景阈值的RoIs</span></span><br><span class="line">        pos_roi_per_this_image = <span class="built_in">int</span>(<span class="built_in">min</span>(pos_roi_per_image, pos_index.size)) <span class="comment"># 计算这个图像的前景RoI数量</span></span><br><span class="line">        <span class="keyword">if</span> pos_index.size &gt; <span class="number">0</span>:  <span class="comment"># 如果存在前景RoI</span></span><br><span class="line">            pos_index = np.random.choice(</span><br><span class="line">                pos_index, size=pos_roi_per_this_image, replace=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Select background RoIs as those within</span></span><br><span class="line">        <span class="comment"># [neg_iou_thresh_lo, neg_iou_thresh_hi).</span></span><br><span class="line">        neg_index = np.where((max_iou &lt; self.neg_iou_thresh_hi) &amp;</span><br><span class="line">                             (max_iou &gt;= self.neg_iou_thresh_lo))[<span class="number">0</span>]  <span class="comment"># 找出IoU在背景阈值范围内的RoIs</span></span><br><span class="line">        neg_roi_per_this_image = self.n_sample - pos_roi_per_this_image  <span class="comment"># 计算这个图像的背景RoI数量</span></span><br><span class="line">        neg_roi_per_this_image = <span class="built_in">int</span>(<span class="built_in">min</span>(neg_roi_per_this_image,</span><br><span class="line">                                         neg_index.size)) <span class="comment"># 计算这个图像的背景RoI数量</span></span><br><span class="line">        <span class="keyword">if</span> neg_index.size &gt; <span class="number">0</span>:  <span class="comment"># 如果存在背景RoI</span></span><br><span class="line">            neg_index = np.random.choice(</span><br><span class="line">                neg_index, size=neg_roi_per_this_image, replace=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The indices that we&#x27;re selecting (both positive and negative).</span></span><br><span class="line">        keep_index = np.append(pos_index, neg_index)  <span class="comment"># 将前景和背景的索引合并</span></span><br><span class="line">        gt_roi_label = gt_roi_label[keep_index]  <span class="comment"># 保留这些RoI的标签</span></span><br><span class="line">        gt_roi_label[pos_roi_per_this_image:] = <span class="number">0</span>  <span class="comment"># negative labels --&gt; 0  # 将背景RoI的标签设置为0</span></span><br><span class="line">        sample_roi = roi[keep_index] <span class="comment"># 保留这些RoI</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute offsets and scales to match sampled RoIs to the GTs.</span></span><br><span class="line">        gt_roi_loc = bbox2loc(sample_roi, bbox[gt_assignment[keep_index]]) <span class="comment"># 计算这些RoI与其对应的边界框的偏移和比例</span></span><br><span class="line">        gt_roi_loc = ((gt_roi_loc - np.array(loc_normalize_mean, np.float32)</span><br><span class="line">                       ) / np.array(loc_normalize_std, np.float32))<span class="comment"># 对偏移和比例进行归一化</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sample_roi, gt_roi_loc, gt_roi_label</span><br><span class="line">    <span class="comment">#sample_roi代表经过IoU阈值筛选后的RoIs，由前景roi和背景roi组成，形状为(128, 4)</span></span><br><span class="line">    <span class="comment">#gt_roi_loc代表sample_roi与其对应的真实边界框（其实是与它IoU最大的真实边界框）之间的偏移和比例，形状为(128, 4)</span></span><br><span class="line">    <span class="comment">#gt_roi_label代表sample_roi对应的真实标签（其实是与它IoU最大的真实边界框的标签），形状为(128,)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>（4）生成用于训练<code>Faster R-CNN</code>的RPN网络所需的目标：这些目标包括每个锚点对应的ground
truth边界框的偏移和比例（用于边界框回归任务），以及每个锚点是否包含物体的标签（用于前景/背景分类任务）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># rpn_loc代表偏移后的锚框的位置</span></span><br><span class="line"><span class="comment"># gt_rpn_loc代表每个anchor与其对应的真实边界框之间的偏移和比例 (hh*ww*9, 4)</span></span><br><span class="line"><span class="comment"># gt_rpn_label代表每个anchor对应的真实边界框的标签 (hh*ww*9,)</span></span><br><span class="line">gt_rpn_loc, gt_rpn_label = self.anchor_target_creator(</span><br><span class="line">            at.tonumpy(bbox),</span><br><span class="line">            anchor,</span><br><span class="line">            img_size)  <span class="comment"># 使用AnchorTargetCreator生成训练目标</span></span><br></pre></td></tr></table></figure>
<p><code>utils/creator_tool.py</code>中的类<code>AnchorTargetCreator</code>的代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AnchorTargetCreator</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 n_sample=<span class="number">256</span>,</span></span><br><span class="line"><span class="params">                 pos_iou_thresh=<span class="number">0.7</span>, neg_iou_thresh=<span class="number">0.3</span>,</span></span><br><span class="line"><span class="params">                 pos_ratio=<span class="number">0.5</span></span>):  <span class="comment"># 初始化方法，接受一系列参数，包括采样区域的数量、前景的IoU阈值、背景的IoU阈值、前景的比例等</span></span><br><span class="line">        self.n_sample = n_sample <span class="comment"># 采样区域的数量</span></span><br><span class="line">        self.pos_iou_thresh = pos_iou_thresh  <span class="comment"># 前景的IoU阈值</span></span><br><span class="line">        self.neg_iou_thresh = neg_iou_thresh  <span class="comment"># 背景的IoU阈值</span></span><br><span class="line">        self.pos_ratio = pos_ratio   <span class="comment"># 前景的比例</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, bbox, anchor, img_size</span>):   <span class="comment"># 定义__call__方法，接受边界框、锚点和图像尺寸作为参数</span></span><br><span class="line"></span><br><span class="line">        img_H, img_W = img_size  <span class="comment"># 获取图像的尺寸</span></span><br><span class="line"></span><br><span class="line">        n_anchor = <span class="built_in">len</span>(anchor)  <span class="comment"># 获取锚点的数量</span></span><br><span class="line">        inside_index = _get_inside_index(anchor, img_H, img_W) <span class="comment"># 获取在图像内部的锚点的索引</span></span><br><span class="line">        anchor = anchor[inside_index]   <span class="comment"># 获取在图像内部的锚点</span></span><br><span class="line">        argmax_ious, label = self._create_label(</span><br><span class="line">            inside_index, anchor, bbox)  <span class="comment"># 创建标签</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># compute bounding box regression targets</span></span><br><span class="line">        loc = bbox2loc(anchor, bbox[argmax_ious])  <span class="comment"># 计算anchor和ground truth边界框之间的偏移和缩放</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># map up to original set of anchors</span></span><br><span class="line">        label = _unmap(label, n_anchor, inside_index, fill=-<span class="number">1</span>)  <span class="comment"># 将标签映射回原始的锚点集</span></span><br><span class="line">        loc = _unmap(loc, n_anchor, inside_index, fill=<span class="number">0</span>)  <span class="comment"># 将位置映射回原始的锚点集</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loc, label  <span class="comment"># 返回位置和标签</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_create_label</span>(<span class="params">self, inside_index, anchor, bbox</span>):</span><br><span class="line">        <span class="comment"># label: 1 is positive, 0 is negative, -1 is dont care</span></span><br><span class="line">        label = np.empty((<span class="built_in">len</span>(inside_index),), dtype=np.int32)  <span class="comment"># 创建一个空的标签数组</span></span><br><span class="line">        label.fill(-<span class="number">1</span>)  <span class="comment"># 将标签数组填充为-1</span></span><br><span class="line"></span><br><span class="line">        argmax_ious, max_ious, gt_argmax_ious = \</span><br><span class="line">            self._calc_ious(anchor, bbox, inside_index)  <span class="comment"># 计算IoU</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># assign negative labels first so that positive labels can clobber them</span></span><br><span class="line">        label[max_ious &lt; self.neg_iou_thresh] = <span class="number">0</span>  <span class="comment"># 将IoU小于背景阈值的锚点标记为背景</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># positive label: for each gt, anchor with highest iou</span></span><br><span class="line">        label[gt_argmax_ious] = <span class="number">1</span>  <span class="comment"># 将每个ground truth对应的IoU最大的锚点标记为前景</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># positive label: above threshold IOU</span></span><br><span class="line">        label[max_ious &gt;= self.pos_iou_thresh] = <span class="number">1</span>  <span class="comment"># 将IoU大于前景阈值的锚点标记为前景</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># subsample positive labels if we have too many</span></span><br><span class="line">        n_pos = <span class="built_in">int</span>(self.pos_ratio * self.n_sample)   <span class="comment"># 计算前景的数量</span></span><br><span class="line">        pos_index = np.where(label == <span class="number">1</span>)[<span class="number">0</span>]   <span class="comment"># 获取前景的索引</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(pos_index) &gt; n_pos:   <span class="comment"># 如果前景的数量过多</span></span><br><span class="line">            disable_index = np.random.choice(</span><br><span class="line">                pos_index, size=(<span class="built_in">len</span>(pos_index) - n_pos), replace=<span class="literal">False</span>)   <span class="comment"># 随机选择一部分前景</span></span><br><span class="line">            label[disable_index] = -<span class="number">1</span>   <span class="comment"># 将这部分前景标记为不关心</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># subsample negative labels if we have too many</span></span><br><span class="line">        n_neg = self.n_sample - np.<span class="built_in">sum</span>(label == <span class="number">1</span>)  <span class="comment"># 计算背景的数量</span></span><br><span class="line">        neg_index = np.where(label == <span class="number">0</span>)[<span class="number">0</span>] <span class="comment"># 获取背景的索引</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(neg_index) &gt; n_neg:  <span class="comment"># 如果背景的数量过多</span></span><br><span class="line">            disable_index = np.random.choice(</span><br><span class="line">                neg_index, size=(<span class="built_in">len</span>(neg_index) - n_neg), replace=<span class="literal">False</span>) <span class="comment"># 随机选择一部分背景</span></span><br><span class="line">            label[disable_index] = -<span class="number">1</span> <span class="comment"># 将这部分背景标记为不关心</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> argmax_ious, label  <span class="comment"># 返回最大IoU的索引和标签</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_calc_ious</span>(<span class="params">self, anchor, bbox, inside_index</span>):</span><br><span class="line">        <span class="comment"># ious between the anchors and the gt boxes</span></span><br><span class="line">        ious = bbox_iou(anchor, bbox) <span class="comment"># 计算锚点和ground truth边界框之间的IoU</span></span><br><span class="line">        argmax_ious = ious.argmax(axis=<span class="number">1</span>)  <span class="comment"># 获取每个锚点对应的最大IoU的索引</span></span><br><span class="line">        max_ious = ious[np.arange(<span class="built_in">len</span>(inside_index)), argmax_ious]  <span class="comment"># 获取每个锚点的最大IoU</span></span><br><span class="line">        gt_argmax_ious = ious.argmax(axis=<span class="number">0</span>) <span class="comment"># 获取每个ground truth边界框对应的最大IoU的索引</span></span><br><span class="line">        gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[<span class="number">1</span>])]  <span class="comment"># 获取每个ground truth边界框的最大IoU</span></span><br><span class="line">        gt_argmax_ious = np.where(ious == gt_max_ious)[<span class="number">0</span>] <span class="comment"># 获取最大IoU的索引</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> argmax_ious, max_ious, gt_argmax_ious   <span class="comment"># 返回最大IoU的索引、最大IoU和ground truth边界框的最大IoU的索引</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>（5）使用<code>Faster R-CNN</code>的头部网络（即RoI网络）对<code>sample_roi</code>进行前向传播，返回<code>roi_cls_locs</code>、<code>roi_scores</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sample_roi代表经过IoU阈值筛选后的RoIs，由前景roi和背景roi组成，形状为(128, 4)</span></span><br><span class="line"><span class="comment"># sample_roi_index：一个全零的数组，用于存储RoIs的索引</span></span><br><span class="line"><span class="comment"># roi_cls_locs代表每个sample_roi在经过head网络预测之后，预测出来的每个类别（21类）的边界框位置 (128,21*4)，128为RPN得出的sample_roi的数量</span></span><br><span class="line"><span class="comment"># roi_scores代表每个sample_roi在经过head网络预测之后，预测出来的每个类别（21类）的得分 (128,21)</span></span><br><span class="line">roi_cls_loc, roi_score = self.faster_rcnn.head(</span><br><span class="line">    features,</span><br><span class="line">    sample_roi,</span><br><span class="line">    sample_roi_index) <span class="comment"># 使用Faster R-CNN的头部网络进行前向传播</span></span><br></pre></td></tr></table></figure>
<p>（6）计算<code>RPN losses</code></p>
<p>RPN的定位损失：偏移后的锚框位置和真实边界框位置之间的平滑L1损失</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ------------------ RPN losses -------------------#</span></span><br><span class="line"><span class="comment"># rpn_loc代表偏移后的锚框的位置 (hh*ww*9, 4)</span></span><br><span class="line"><span class="comment"># gt_rpn_loc代表每个anchor与其对应的真实边界框之间的偏移和比例 (hh*ww*9, 4)</span></span><br><span class="line"><span class="comment"># gt_rpn_label代表每个anchor对应的真实边界框的标签 (hh*ww*9,)</span></span><br><span class="line">rpn_loc_loss = _fast_rcnn_loc_loss(</span><br><span class="line">rpn_loc,</span><br><span class="line">gt_rpn_loc,</span><br><span class="line">gt_rpn_label.data,</span><br><span class="line">self.rpn_sigma)  <span class="comment"># 计算RPN的定位损失：偏移后的锚框位置和真实边界框位置之间的平滑L1损失</span></span><br></pre></td></tr></table></figure>
<p>RPN的分类损失</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># rpn_score代表偏移后的锚框的得分 (hh*ww*9, 2)</span></span><br><span class="line">rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_label.cuda(), ignore_index=-<span class="number">1</span>)  <span class="comment"># 计算RPN的分类损失</span></span><br></pre></td></tr></table></figure>
<p>（7）计算<code>ROI losses</code>（fast rcnn loss）</p>
<p>ROI的定位损失</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ------------------ ROI losses (fast rcnn loss) -------------------#</span></span><br><span class="line"><span class="comment"># roi_cls_loc代表每个sample_roi在经过head网络预测之后，预测出来的每个类别（21类）的边界框位置 (128,21,4)，128为RPN得出的sample_roi的数量</span></span><br><span class="line"><span class="comment"># roi_score代表每个sample_roi在经过head网络预测之后，预测出来的每个类别（21类）的得分 (128,21)</span></span><br><span class="line"><span class="comment"># gt_roi_loc代表sample_roi与其对应的真实边界框（其实是与它IoU最大的真实边界框）之间的偏移和比例，形状为(128, 4)</span></span><br><span class="line"><span class="comment"># gt_roi_label代表sample_roi对应的真实标签（其实是与它IoU最大的真实边界框的标签），形状为(128,)</span></span><br><span class="line">roi_loc_loss = _fast_rcnn_loc_loss(</span><br><span class="line">roi_loc.contiguous(),</span><br><span class="line">gt_roi_loc,</span><br><span class="line">gt_roi_label.data,</span><br><span class="line">self.roi_sigma)  <span class="comment"># 计算RoI的定位损失</span></span><br></pre></td></tr></table></figure>
<p>ROI的分类损失</p>
<p>（8）计算总损失</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 总损失 = rpn定位损失+rpn分类损失+roi定位损失+roi分类损失</span></span><br><span class="line">losses = [rpn_loc_loss, rpn_cls_loss, roi_loc_loss, roi_cls_loss]  <span class="comment"># 创建一个列表，用于存储所有的损失</span></span><br><span class="line">losses = losses + [<span class="built_in">sum</span>(losses)]  <span class="comment"># 计算总损失</span></span><br></pre></td></tr></table></figure>
<p>（9）前向传播返回总损失后，执行反向传播、参数更新等操作</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">self, imgs, bboxes, labels, scale</span>): <span class="comment"># 定义训练步骤方法，接受图像、边界框、标签和缩放因子作为参数</span></span><br><span class="line">    self.optimizer.zero_grad() <span class="comment"># 清零优化器的梯度缓存</span></span><br><span class="line">    losses = self.forward(imgs, bboxes, labels, scale) <span class="comment"># 调用forward方法，计算损失 imgs==tensor(1,3,800,600) bboxes==tensor(1,1,4) labels==tensor(1,1) scale==(1,)</span></span><br><span class="line">    losses.total_loss.backward()<span class="comment"># 对总损失进行反向传播</span></span><br><span class="line">    self.optimizer.step()  <span class="comment"># 执行一步优化（参数更新）</span></span><br><span class="line">    self.update_meters(losses)  <span class="comment"># 更新度量</span></span><br><span class="line">    <span class="keyword">return</span> losses <span class="comment"># 返回损失</span></span><br></pre></td></tr></table></figure>
<p>（10）一个训练步骤<code>trainer.train_step(img, bbox, label, scale)</code>完成后，评估训练结果并在<code>visdom</code>中可视化展示
<img src="https://ooo.0x0.ooo/2023/12/29/OKYXOt.png"
alt="OKYXOt.png" /></p>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>代码复现</tag>
        <tag>Faster RCNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Faster RCNN论文理解</title>
    <url>/2023/12/29/Faster-RCNN%E8%AE%BA%E6%96%87%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"><h1 id="faster-rcnn论文理解">Faster RCNN论文理解</h1>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYpMG.png" alt="OKYpMG.png" />
<figcaption aria-hidden="true">OKYpMG.png</figcaption>
</figure>
<span id="more"></span>
<h2 id="背景">背景</h2>
<p>基于候选区域的CNNs(RCNN、SPPnet、FastRCNN...)在目标检测领域取得了很好的效果，尤其是共享卷积特征及大地降低了计算代价，提高了训练和测试的速度。如果忽略产生候选区域(region
proposals)的耗时，Fast
RCNN输入一张图片和其候选区域后几乎可以实现实时检测。因此，现在检测系统的计算瓶颈主要在于产生候选区域的部分。Selective
Search
(SS)产生一张图片的2000个候选区域要耗时2s，EdgeBoxes则要耗时0.2s（两种方法都只有CPU实现）——太慢了！因此，本文提出利用deep
ConvNet来产生候选区域。</p>
<h2 id="主要贡献">主要贡献</h2>
<ol type="1">
<li><strong>用RPN代替selective、EdgeBoxes产生候选区域</strong>：RPN(Region
Proposal Networks)，能够同时预测边界框和对象性得分(objectness
scores)，并和Fast RCNN目标检测网络共享了卷积特征，实现了网络加速。</li>
<li>提出了一种“轮流”的训练模式来合并RPN和Fast RCNN</li>
</ol>
<h2 id="rpnregion-proposal-networks">RPN(Region Proposal Networks)</h2>
<h3 id="网络结构">网络结构</h3>
<blockquote>
<p>下图展示了RPN的总体结构示意图</p>
</blockquote>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYPHI.png" alt="OKYPHI.png" />
<figcaption aria-hidden="true">OKYPHI.png</figcaption>
</figure>
<p>共享卷积层(conv layers)本文实验了ZF模型和VGG-16模型</p>
<p>输入：一张任意大小的图片</p>
<p>输出：一系列矩形候选区域及其对应的对象性得分(objectness
score——object/not-object)</p>
<blockquote>
<p>下图展示了"RPN特有部分"的结构示意图</p>
</blockquote>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYRXD.png" alt="OKYRXD.png" />
<figcaption aria-hidden="true">OKYRXD.png</figcaption>
</figure>
<h3
id="平移不变的参考框translation-invariant-anchors">平移不变的参考框(Translation-Invariant
Anchors)</h3>
<p>用三种不同的尺度(scale)和长宽比(aspect
ratio)，在每个滑动窗口位置产生9种不同的参考框。</p>
<p>参考框和根据参考框计算候选区域的函数，二者都具有平移不变性</p>
<h3 id="rpn损失函数">RPN损失函数</h3>
<p><strong>注意！！！这里是RPN的分类和回归网络两个分支的损失函数，不是Fast
RCNN的损失函数</strong></p>
<ol type="1">
<li><p>怎么判断预测的参考框是正样本还是负样本？</p>
<ul>
<li>正样本：将参考框和所有的ground-truth计算IoU，与每个ground-truth有着最大IoU的参考框是正样本；将参考框和所有的ground-truth计算IoU，与任意ground-truth的IoU超过0.7的也是正样本</li>
<li>负样本：将参考框和所有的ground-truth计算IoU，与所有的ground-truth的IoU&lt;0.3的是负样本</li>
</ul></li>
<li><p>最小化Fast RCNN中的多任务损失函数(multi-task loss)</p>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYyZr.png" alt="OKYyZr.png" />
<figcaption aria-hidden="true">OKYyZr.png</figcaption>
</figure></li>
<li><p>cls：识别参考框(anchor box)属于object or
not-object；reg：将参考框(anchor box)回归到真实标注框(ground-truth
box)</p></li>
<li><p>回归的实现方式有所改变：</p>
<ul>
<li>RCNN、Fast
RCNN：将任意大小的候选区域池化后，执行边界框回归，回归权重被各尺寸的候选区域共享</li>
<li>RPN：只在相同大小的特征图上进行回归，学习到k个不同的边界框回归器，它们之间的权重互不共享</li>
</ul></li>
</ol>
<h3 id="训练rpn">训练RPN</h3>
<p>以图片为中心采样(“image centric” sampling
strategy)：每个mini-batch来自于一张图片包含的正负样本。</p>
<p>实际做法：在一张图片上随机采样256个参考框，其中正负参考框占比接近1:1，用这些样本作为mini-batch，计算这个mini-batch的损失函数</p>
<h2 id="faster-rcnn整体训练">Faster RCNN整体训练</h2>
<p><strong>Faster RCNN = RPN + Fast RCNN</strong></p>
<h3 id="如何共享">如何共享</h3>
<p><strong>如何在RPN和Fast RCNN间共享卷积层？</strong></p>
<h4 id="思路1轮流训练">思路1：轮流训练</h4>
<p>step
1，训练RPN：用ImageNet上的预训练模型初始化RPN，并在端到端的区域建议任务上微调。</p>
<p>step 2，训练Fast RCNN：用step-1训练的RPN产生的候选区域 +
ImageNet上的预训练模型从头训练一个Fast
RCNN检测模型。到此为止，这两个网络还没有共享卷积层的特征。</p>
<p>step 3，用step-2得到的Fast RCNN检测网络的特征提取部分(Deep
ConvNet)初始化RPN的训练，微调RPN特有的的层。此时，两个网络实现了共享卷积层。</p>
<p>step 4，固定共享卷积层，微调Fast
RCNN的全连接层（特有的那些层）。到此为止，两个网络就共享了卷积层并形成了一个统一的网络。</p>
<p>下图展示了这一轮流训练过程 <img
src="https://ooo.0x0.ooo/2023/12/29/OKYJ71.png" alt="OKYJ71.png" /></p>
<h4 id="思路2近似联合训练法">思路2：近似联合训练法</h4>
<p>做法：训练初始就将RPN和Fast
RCNN合并成一个网络，然后用同时进行反向传播来优化。</p>
<p>问题：无法求解坐标偏导数，因为Fast
RCNN的损失函数反向传播也会传导到RPN回归出的坐标部分，但这种方法忽略了这部分</p>
<h4 id="思路3非近似联合训练法">思路3：非近似联合训练法</h4>
<p>利用ROI Warping实现了Fast RCNN尾部网络反向传播到坐标部分的问题</p>
<h3 id="实现细节">实现细节</h3>
<ol type="1">
<li><p>RPN和Fast
RCNN的训练和测试都使用了单尺度图片：缩放图片的短边为600px</p></li>
<li><p>参考框选取了3种尺度：128<sup>2，256</sup>2，512^2，3种长宽比：1:1，1:2，2:1=&gt;不需要多尺寸特征、多尺寸滑动窗口来预测大区域</p></li>
<li><p>处理跨图片边界的参考框：</p>
<ul>
<li>训练时忽略所有跨边界的参考框，因为如果不忽略跨边界的参考框，它们会在损失函数中引入很大很难纠正的错误模式，导致训练很难收敛。</li>
<li>测试时遇到跨图片边界的候选框，会保留其并裁剪其到图片边界</li>
</ul>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKY4zF.png" alt="OKY4zF.png" />
<figcaption aria-hidden="true">OKY4zF.png</figcaption>
</figure></li>
<li><p>基于cls
scores针对每一类别的候选区域应用非极大值抑制过滤，以减少冗余。当NMS的IoU的阈值设定为0.7时，每张图片最终会留下大约2k个候选区域，用这2k个候选区域训练Fast
RCNN（测试时不一定需要使用2k个，实际使用了300个）</p></li>
</ol>
<h3 id="实验">实验</h3>
<p>数据集：PASCAL VOC 2007，5k trainval images，5k test
images，超过20个对象类</p>
<p>评估指标：mAP</p>
<h3 id="消融实验">消融实验</h3>
<ol type="1">
<li><p>困惑：在RPN和Fast RCNN之间共享卷积特征有什么作用？</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">实验：在4-step的训练步骤的step-2后停下来，用2个分离的网络（RPN和Fast RCNN）完成获取候选区域和实现目标检测</span><br><span class="line"></span><br><span class="line">实验结果：精度会下降</span><br><span class="line"></span><br><span class="line">解释：说明step-2训练好的Fast RCNN网络的特征提取能力能更好地辅助RPN模块产生候选区域，提高了候选区域的质量</span><br></pre></td></tr></table></figure></li>
<li><p>困惑：只在测试时使用RPN产生的候选区域，对Fast
RCNN检测网络有什么影响?<strong>（注意这里是Fast RCNN，Fast
RCNN的训练还是采用selective-search产生的候选区域来实现的，而不是Faster
RCNN，即这里RPN和Fast RCNN并没有共享卷积特征）</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">实验1：</span><br><span class="line"></span><br><span class="line">- 训练时：selective search产生2k个候选区域+ZF；</span><br><span class="line">- 测试时：固定训练产生的检测器，采用RPN产生的候选区域。这时RPN并没有和此时的ZF共享特征。</span><br><span class="line"></span><br><span class="line">实验结果：测试时用300个RPN产生的候选区域来代替selective-search，精度会下降</span><br><span class="line"></span><br><span class="line">解释：精度损失是因为训练和测试时所用的候选区域(proposals)不一致，该结果为后续实验提供baseline</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">实验2：测试时用RPN产生的前100个候选区域代替前300个</span><br><span class="line"></span><br><span class="line">实验结果：精度下降但下降不多</span><br><span class="line"></span><br><span class="line">解释：说明排名靠前的RPN proposals很准确</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">实验3：测试时用前6k个RPN proposals(without NMS)产生的候选区域</span><br><span class="line"></span><br><span class="line">实验结果：精度并没有比用300个RPN proposals(with NMS)更好，说明NMS并不损害检测时的mAP，</span><br></pre></td></tr></table></figure></li>
<li><p>测试时RPN的分类和回归输出分别有什么作用？</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">实验1：测试时移除RPN的cls layer，然后从没有打分的region proposals随机采样N个proposals进行测试（关闭分类层会导致后续——不会进行NMS，因为NMS是针对每一类(object/not-object)的cls score来进行的，而此时没有分类了）</span><br><span class="line"></span><br><span class="line">实验结果：N=1000时，精度几乎无影响；N=100是，精度大幅下降</span><br><span class="line"></span><br><span class="line">解释：cls scores 考虑到了排名最高的proposals的准确性</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">实验2：测试时移除RPN的reg layer（移除reg layer的后续影响是——proposals变成了anchor boxes）</span><br><span class="line"></span><br><span class="line">实验结果：精度下降</span><br><span class="line"></span><br><span class="line">解释：高质量的候选区域(proposals)主要是因为位置回归，只有参考框不足以准确定位</span><br></pre></td></tr></table></figure></li>
<li><p>更强大的网络(VGG-16)对RPN产生的候选区域质量有什么影响</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">实验：在上述实验（训练：selective-search + ZF，测试：利用ZF训练的RPN产生候选区域），改用VGG-16训练的RPN产生候选区域进行测试（注意这里RPN和目标检测网络都没有共享卷积特征）</span><br><span class="line"></span><br><span class="line">结果：精度提高</span><br><span class="line"></span><br><span class="line">解释：RPN+VGG-16产生的候选区域质量优于RPN+ZF（用ZF训练出来的RPN）</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="recall-to-iou">Recall-to-IoU</h3>
<p>这个评估指标主要用于诊断（注意是诊断，不是评估）产生候选区域的方法的优劣</p>
<p>下图展示了不同候选区域算法（SS、EB、RPN+ZF、RPN+VGG）在不同IoU时的召回率</p>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYdLK.png" alt="OKYdLK.png" />
<figcaption aria-hidden="true">OKYdLK.png</figcaption>
</figure>
<p>解释：</p>
<ol type="1">
<li>当proposals的数量下降的时候，RPN在各个IoU比率下的召回率都只是略微降低</li>
<li>当proposals的数量下降的时候，SS和EB方法在各个IoU比率下的召回率都显著降低</li>
<li>这就解释了为什么RPN只需要300个候选区域就能取得比较好的精度</li>
</ol>
<h3 id="一阶段检测vs两阶段检测">一阶段检测vs两阶段检测</h3>
<p>和OverFeat比较</p>
<p>参考资料：</p>
<blockquote>
<p>https://www.bilibili.com/video/BV1GB4y1r7rM/?spm_id_from=333.880.my_history.page.click&amp;vd_source=66a72b15abe9693bd8b4f738f5a67ee7</p>
</blockquote>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>论文理解</tag>
        <tag>Faster RCNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Faster RCNN项目部署（一）</title>
    <url>/2023/12/29/Faster-RCNN%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"><h2 id="faster-rcnn项目部署">Faster RCNN项目部署</h2>
<p>simple-faster-rcnn-pytorch-master</p>
<p><strong>项目源码</strong>：https://github.com/chenyuntc/simple-faster-rcnn-pytorch</p>
<span id="more"></span>
<p><strong>云服务器环境：</strong></p>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYfPN.png" alt="OKYfPN.png" />
<figcaption aria-hidden="true">OKYfPN.png</figcaption>
</figure>
<h3 id="安装依赖">安装依赖</h3>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">!pip install scikit-image</span><br><span class="line">!pip install visdom</span><br><span class="line">!pip install torchnet</span><br><span class="line">!pip install opencv-python</span><br><span class="line">!pip install ipdb</span><br><span class="line">!pip install ipython==7.28.0</span><br><span class="line">!pip install fire</span><br></pre></td></tr></table></figure>
<h3 id="运行demo">运行demo</h3>
<h4 id="下载预训练模型">下载预训练模型</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">百度网盘地址：https://pan.baidu.com/s/1o87RuXW</span><br><span class="line">密码：scxn</span><br></pre></td></tr></table></figure>
<h4 id="运行demo.pipynb">运行demo.pipynb</h4>
<blockquote>
<p>问题：在运行<code>img = read_image('/root/autodl-tmp/FasterRCNN/misc/demo.jpg')</code>出现错误：
<code>UnidentifiedImageError：Image.open() cannot identify image file</code></p>
<p>解决：我也不知道咋解决，因为另外测试后发现PIL的Image.open()是可以用的，我重启了几次不知道为什么就能用了</p>
</blockquote>
<h3 id="train">Train</h3>
<h4 id="准备数据">准备数据</h4>
<p>参考项目官网的<code>5.1 Prepare data--Pascal VOC2007</code></p>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYllC.png" alt="OKYllC.png" />
<figcaption aria-hidden="true">OKYllC.png</figcaption>
</figure>
<p>最终得到的数据目录如下</p>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYjTL.png" alt="OKYjTL.png" />
<figcaption aria-hidden="true">OKYjTL.png</figcaption>
</figure>
<h4 id="修改配置">修改配置</h4>
<ol type="1">
<li>utils/config.py：修改VOC数据集的存放位置<code>voc_data_dir = '/root/autodl-tmp/FasterRCNN/dataset/VOCdevkit/VOC2007/'</code></li>
</ol>
<h4 id="开启训练">开启训练</h4>
<p>如果直接运行代码<code>!python train.py train --env='fasterrcnn' --plot-every=100</code>，会出现以下问题：</p>
<blockquote>
<p>问题：ERROR:visdom:[WinError 10061]
由于目标计算机积极拒绝，无法连接。</p>
<p>原因：代码中用到了在线可视化工具visdom，但是没有启动</p>
<p>解决：利用Xshell映射云端服务器的visdom，进行训练过程可视。参考博客https://blog.csdn.net/m0_6551</p>
</blockquote>
<p>因此，先利用Xshell映射云端服务器的visdom（参见上面的博客），再在jupyter
notebook运行以下代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python train.py train --env=&#x27;fasterrcnn&#x27; --plot-every=100</span><br></pre></td></tr></table></figure>
<p>运行成功结果如下： <img
src="https://ooo.0x0.ooo/2023/12/29/OKY2vi.png" alt="OKY2vi.png" /></p>
<h4 id="利用pycharm进行调试">利用Pycharm进行调试</h4>
<p>因为在jupyter
notebook中开启训练时使用了代码<code>python train.py train --env='fasterrcnn' --plot-every=100</code>，而在Pycharm中无法输入这样的命令进行debug，因此我对<code>train.py</code>做了以下修改：</p>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYsua.png" alt="OKYsua.png" />
<figcaption aria-hidden="true">OKYsua.png</figcaption>
</figure>
<p>我舍弃了利用fire.Fire()来调用函数train，而是直接在main函数中调用了train()方法，并传递了参数。此时就可以在Pycharm进行debug了。</p>
<blockquote>
<p>另外，Pycharm在调试过程会遇到一个问题，即：debug时Pycharm卡住在connected界面不动</p>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYYzS.png" alt="OKYYzS.png" />
<figcaption aria-hidden="true">OKYYzS.png</figcaption>
</figure>
<p>解决：<img src="https://ooo.0x0.ooo/2023/12/29/OKY6cX.png"
alt="OKY6cX.png" /></p>
</blockquote>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>代码复现</tag>
        <tag>Faster RCNN</tag>
      </tags>
  </entry>
  <entry>
    <title>RCNN代码复现</title>
    <url>/2023/12/29/RCNN%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"><p>本代码最初在colab中实现，以下为全部代码及运行输出结果 #
挂载谷歌云盘，解压数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%pwd</span><br></pre></td></tr></table></figure>
<p>'/content'</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!unzip <span class="string">&#x27;/content/drive/MyDrive/AI_content/RCNN/Images.zip&#x27;</span> -d <span class="string">&#x27;/content/drive/MyDrive/AI_content/RCNN&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!unzip <span class="string">&#x27;/content/drive/MyDrive/AI_content/RCNN/Airplanes_Annotations.zip&#x27;</span> -d <span class="string">&#x27;/content/drive/MyDrive/AI_content/RCNN&#x27;</span></span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h1 id="安装并导入依赖">安装并导入依赖</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!pip install tensorflow==<span class="number">2.8</span><span class="number">.0</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os,cv2,keras</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.__version__</span><br></pre></td></tr></table></figure>
<p>'2.8.0'</p>
<h1 id="更改工作目录">更改工作目录</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cd /content/drive/MyDrive/AI_content/RCNN</span><br></pre></td></tr></table></figure>
<p>/content/drive/MyDrive/AI_content/RCNN</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">path = <span class="string">&#x27;/content/drive/MyDrive/AI_content/RCNN/Images&#x27;</span></span><br><span class="line">annot = <span class="string">&#x27;/content/drive/MyDrive/AI_content/RCNN/Airplanes_Annotations&#x27;</span></span><br></pre></td></tr></table></figure>
<h1 id="查看数据和标签">查看数据和标签</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Index=<span class="number">148</span></span><br><span class="line">filename = <span class="string">&quot;airplane_&quot;</span>+<span class="built_in">str</span>(Index)+<span class="string">&quot;.jpg&quot;</span></span><br><span class="line"><span class="built_in">print</span>(filename)</span><br><span class="line">img = cv2.imread(os.path.join(path,filename))</span><br><span class="line">df = pd.read_csv(os.path.join(annot,filename.replace(<span class="string">&quot;.jpg&quot;</span>,<span class="string">&quot;.csv&quot;</span>)))</span><br><span class="line">plt.imshow(img)</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    x1 = <span class="built_in">int</span>(row[<span class="number">1</span>][<span class="number">0</span>].split(<span class="string">&quot; &quot;</span>)[<span class="number">0</span>])</span><br><span class="line">    y1 = <span class="built_in">int</span>(row[<span class="number">1</span>][<span class="number">0</span>].split(<span class="string">&quot; &quot;</span>)[<span class="number">1</span>])</span><br><span class="line">    x2 = <span class="built_in">int</span>(row[<span class="number">1</span>][<span class="number">0</span>].split(<span class="string">&quot; &quot;</span>)[<span class="number">2</span>])</span><br><span class="line">    y2 = <span class="built_in">int</span>(row[<span class="number">1</span>][<span class="number">0</span>].split(<span class="string">&quot; &quot;</span>)[<span class="number">3</span>])</span><br><span class="line">    cv2.rectangle(img,(x1,y1),(x2,y2),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>airplane_148.jpg</p>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYkfL.png" alt="OKYkfL.png" />
<figcaption aria-hidden="true">OKYkfL.png</figcaption>
</figure>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYzBi.png" alt="OKYzBi.png" />
<figcaption aria-hidden="true">OKYzBi.png</figcaption>
</figure>
<h1 id="selective-search">Selective search</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv2.setUseOptimized(<span class="literal">True</span>);</span><br><span class="line">ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">im = cv2.imread(os.path.join(path,<span class="string">&quot;42850.jpg&quot;</span>))</span><br><span class="line">ss.setBaseImage(im)</span><br><span class="line">ss.switchToSelectiveSearchFast()</span><br><span class="line">rects = ss.process()</span><br><span class="line">imOut = im.copy() <span class="comment">#复制原图，在复制后的图片上绘制矩形</span></span><br><span class="line"><span class="keyword">for</span> i, rect <span class="keyword">in</span> (<span class="built_in">enumerate</span>(rects)):</span><br><span class="line">    x, y, w, h = rect</span><br><span class="line">    cv2.rectangle(imOut, (x, y), (x+w, y+h), (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), <span class="number">1</span>, cv2.LINE_AA) <span class="comment">#在imOut上绘制矩形</span></span><br><span class="line"></span><br><span class="line">plt.imshow(imOut)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示可视化的结果</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYbpC.png" alt="OKYbpC.png" />
<figcaption aria-hidden="true">OKYbpC.png</figcaption>
</figure>
<h1 id="iou">IOU</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_iou</span>(<span class="params">bb1, bb2</span>):</span><br><span class="line">  <span class="comment"># assuring for proper dimension.</span></span><br><span class="line">    <span class="keyword">assert</span> bb1[<span class="string">&#x27;x1&#x27;</span>] &lt; bb1[<span class="string">&#x27;x2&#x27;</span>]</span><br><span class="line">    <span class="keyword">assert</span> bb1[<span class="string">&#x27;y1&#x27;</span>] &lt; bb1[<span class="string">&#x27;y2&#x27;</span>]</span><br><span class="line">    <span class="keyword">assert</span> bb2[<span class="string">&#x27;x1&#x27;</span>] &lt; bb2[<span class="string">&#x27;x2&#x27;</span>]</span><br><span class="line">    <span class="keyword">assert</span> bb2[<span class="string">&#x27;y1&#x27;</span>] &lt; bb2[<span class="string">&#x27;y2&#x27;</span>]</span><br><span class="line">  <span class="comment"># calculating dimension of common area between these two boxes.</span></span><br><span class="line">    x_left = <span class="built_in">max</span>(bb1[<span class="string">&#x27;x1&#x27;</span>], bb2[<span class="string">&#x27;x1&#x27;</span>])</span><br><span class="line">    y_top = <span class="built_in">max</span>(bb1[<span class="string">&#x27;y1&#x27;</span>], bb2[<span class="string">&#x27;y1&#x27;</span>])</span><br><span class="line">    x_right = <span class="built_in">min</span>(bb1[<span class="string">&#x27;x2&#x27;</span>], bb2[<span class="string">&#x27;x2&#x27;</span>])</span><br><span class="line">    y_bottom = <span class="built_in">min</span>(bb1[<span class="string">&#x27;y2&#x27;</span>], bb2[<span class="string">&#x27;y2&#x27;</span>])</span><br><span class="line">  <span class="comment"># if there is no overlap output 0 as intersection area is zero.</span></span><br><span class="line">    <span class="keyword">if</span> x_right &lt; x_left <span class="keyword">or</span> y_bottom &lt; y_top:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.0</span></span><br><span class="line">  <span class="comment"># calculating intersection area.</span></span><br><span class="line">    intersection_area = (x_right - x_left) * (y_bottom - y_top)</span><br><span class="line">  <span class="comment"># individual areas of both these bounding boxes.</span></span><br><span class="line">    bb1_area = (bb1[<span class="string">&#x27;x2&#x27;</span>] - bb1[<span class="string">&#x27;x1&#x27;</span>]) * (bb1[<span class="string">&#x27;y2&#x27;</span>] - bb1[<span class="string">&#x27;y1&#x27;</span>])</span><br><span class="line">    bb2_area = (bb2[<span class="string">&#x27;x2&#x27;</span>] - bb2[<span class="string">&#x27;x1&#x27;</span>]) * (bb2[<span class="string">&#x27;y2&#x27;</span>] - bb2[<span class="string">&#x27;y1&#x27;</span>])</span><br><span class="line">  <span class="comment"># union area = area of bb1_+ area of bb2 - intersection of bb1 and bb2.</span></span><br><span class="line">    iou = intersection_area / <span class="built_in">float</span>(bb1_area + bb2_area - intersection_area)</span><br><span class="line">    <span class="keyword">assert</span> iou &gt;= <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">assert</span> iou &lt;= <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> iou</span><br></pre></td></tr></table></figure>
<h1 id="准备训练数据">准备训练数据</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># At the end of below code we will have our train data in these lists</span></span><br><span class="line">train_images=[]</span><br><span class="line">train_labels=[]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> e,i <span class="keyword">in</span> <span class="built_in">enumerate</span>(os.listdir(annot)):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">if</span> i.startswith(<span class="string">&quot;airplane&quot;</span>):</span><br><span class="line">            filename = i.split(<span class="string">&quot;.&quot;</span>)[<span class="number">0</span>]+<span class="string">&quot;.jpg&quot;</span></span><br><span class="line">            <span class="built_in">print</span>(e,filename)</span><br><span class="line">            <span class="comment"># 读取图像</span></span><br><span class="line">            image = cv2.imread(os.path.join(path,filename))</span><br><span class="line">            <span class="comment"># 读取标注文件</span></span><br><span class="line">            df = pd.read_csv(os.path.join(annot,i))</span><br><span class="line">            gtvalues=[]</span><br><span class="line">            <span class="keyword">for</span> row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">                x1 = <span class="built_in">int</span>(row[<span class="number">1</span>][<span class="number">0</span>].split(<span class="string">&quot; &quot;</span>)[<span class="number">0</span>])</span><br><span class="line">                y1 = <span class="built_in">int</span>(row[<span class="number">1</span>][<span class="number">0</span>].split(<span class="string">&quot; &quot;</span>)[<span class="number">1</span>])</span><br><span class="line">                x2 = <span class="built_in">int</span>(row[<span class="number">1</span>][<span class="number">0</span>].split(<span class="string">&quot; &quot;</span>)[<span class="number">2</span>])</span><br><span class="line">                y2 = <span class="built_in">int</span>(row[<span class="number">1</span>][<span class="number">0</span>].split(<span class="string">&quot; &quot;</span>)[<span class="number">3</span>])</span><br><span class="line">                gtvalues.append(&#123;<span class="string">&quot;x1&quot;</span>:x1,<span class="string">&quot;x2&quot;</span>:x2,<span class="string">&quot;y1&quot;</span>:y1,<span class="string">&quot;y2&quot;</span>:y2&#125;)</span><br><span class="line">            <span class="comment"># 设置基础图像</span></span><br><span class="line">            ss.setBaseImage(image)   <span class="comment"># setting given image as base image</span></span><br><span class="line">            ss.switchToSelectiveSearchFast()     <span class="comment"># running selective search on base image</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 运行选择性搜索</span></span><br><span class="line">            ssresults = ss.process()     <span class="comment"># processing to get the outputs</span></span><br><span class="line">            imout = image.copy()</span><br><span class="line">            counter = <span class="number">0</span></span><br><span class="line">            falsecounter = <span class="number">0</span></span><br><span class="line">            flag = <span class="number">0</span></span><br><span class="line">            fflag = <span class="number">0</span></span><br><span class="line">            bflag = <span class="number">0</span></span><br><span class="line">             <span class="comment"># 遍历选择性搜索的结果</span></span><br><span class="line">            <span class="keyword">for</span> e,result <span class="keyword">in</span> <span class="built_in">enumerate</span>(ssresults):</span><br><span class="line">                <span class="keyword">if</span> e &lt; <span class="number">2000</span> <span class="keyword">and</span> flag == <span class="number">0</span>:     <span class="comment"># till 2000 to get top 2000 regions only</span></span><br><span class="line">                    <span class="keyword">for</span> gtval <span class="keyword">in</span> gtvalues:</span><br><span class="line">                        x,y,w,h = result</span><br><span class="line">                        iou = get_iou(gtval,&#123;<span class="string">&quot;x1&quot;</span>:x,<span class="string">&quot;x2&quot;</span>:x+w,<span class="string">&quot;y1&quot;</span>:y,<span class="string">&quot;y2&quot;</span>:y+h&#125;)  <span class="comment"># calculating IoU for each of the proposed regions</span></span><br><span class="line">                        <span class="keyword">if</span> counter &lt; <span class="number">30</span>:       <span class="comment"># getting only 30 psoitive examples</span></span><br><span class="line">                            <span class="keyword">if</span> iou &gt; <span class="number">0.70</span>:     <span class="comment"># IoU of being positive is 0.7</span></span><br><span class="line">                                timage = imout[x:x+w,y:y+h]</span><br><span class="line">                                resized = cv2.resize(timage, (<span class="number">224</span>,<span class="number">224</span>), interpolation = cv2.INTER_AREA)</span><br><span class="line">                                train_images.append(resized)</span><br><span class="line">                                train_labels.append(<span class="number">1</span>)</span><br><span class="line">                                counter += <span class="number">1</span></span><br><span class="line">                        <span class="keyword">else</span> :</span><br><span class="line">                            fflag =<span class="number">1</span>              <span class="comment"># to insure we have collected all psotive examples</span></span><br><span class="line">                        <span class="keyword">if</span> falsecounter &lt;<span class="number">30</span>:      <span class="comment"># 30 negatve examples are allowed only</span></span><br><span class="line">                            <span class="keyword">if</span> iou &lt; <span class="number">0.3</span>:         <span class="comment"># IoU of being negative is 0.3</span></span><br><span class="line">                                timage = imout[x:x+w,y:y+h]</span><br><span class="line">                                resized = cv2.resize(timage, (<span class="number">224</span>,<span class="number">224</span>), interpolation = cv2.INTER_AREA)</span><br><span class="line">                                train_images.append(resized)</span><br><span class="line">                                train_labels.append(<span class="number">0</span>)</span><br><span class="line">                                falsecounter += <span class="number">1</span></span><br><span class="line">                        <span class="keyword">else</span> :</span><br><span class="line">                            bflag = <span class="number">1</span>             <span class="comment">#to ensure we have collected all negative examples</span></span><br><span class="line">                    <span class="keyword">if</span> fflag == <span class="number">1</span> <span class="keyword">and</span> bflag == <span class="number">1</span>:</span><br><span class="line">                        <span class="built_in">print</span>(<span class="string">&quot;inside&quot;</span>)</span><br><span class="line">                        flag = <span class="number">1</span>        <span class="comment"># to signal the complition of data extaction from a particular image</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(e)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;error in &quot;</span>+filename)</span><br><span class="line">        <span class="keyword">continue</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># conversion of train data into arrays for further training</span></span><br><span class="line">X_new = np.array(train_images)</span><br><span class="line">Y_new = np.array(train_labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 为方便下次不用重新处理输入的训练数据，在这里将X_new,Y_new进行保存</span></span><br><span class="line">np.save(<span class="string">&#x27;save_X_new&#x27;</span>,X_new)</span><br><span class="line">np.save(<span class="string">&#x27;save_Y_new&#x27;</span>,Y_new)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 读取保存的数据X_new,Y_new</span></span><br><span class="line">X_new = np.load(<span class="string">&#x27;save_X_new.npy&#x27;</span>)</span><br><span class="line">Y_new = np.load(<span class="string">&#x27;save_Y_new.npy&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 这里因为colab提供的显存不够，只有15g，加载全部数据进去会爆显存，所以只截取一部分样本来进行训练，即X_new_subset 、Y_new_subset </span></span><br><span class="line">total_nums = <span class="built_in">len</span>(Y_new)</span><br><span class="line"><span class="built_in">print</span>(total_nums)</span><br></pre></td></tr></table></figure>
<p>30229</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从训练数据中随机选择 10000 个样本</span></span><br><span class="line">num_samples = <span class="number">5000</span></span><br><span class="line">random_indices = np.random.choice(<span class="built_in">len</span>(X_new), num_samples, replace=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用随机选择的索引来获取样本</span></span><br><span class="line">X_new_subset = X_new[random_indices]</span><br><span class="line">Y_new_subset = Y_new[random_indices]</span><br></pre></td></tr></table></figure>
<h1
id="预训练使用vgg16模型创建一个迁移学习模型">预训练（使用VGG16模型创建一个迁移学习模型）</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用 VGG16 模型来创建一个迁移学习模型</span></span><br><span class="line">vgg = tf.keras.applications.vgg16.VGG16(include_top=<span class="literal">True</span>, weights=<span class="string">&#x27;imagenet&#x27;</span>, input_tensor=<span class="literal">None</span>, input_shape=<span class="literal">None</span>, pooling=<span class="literal">None</span>, classes=<span class="number">1000</span>)</span><br><span class="line"><span class="comment"># 将 VGG16 模型的大部分层设为不可训练，保留最后两层的可训练性</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> vgg.layers[:-<span class="number">2</span>]:</span><br><span class="line">  layer.trainable = <span class="literal">False</span></span><br><span class="line"><span class="comment"># 获取 VGG16 模型中名为 &#x27;fc2&#x27; 的层，并获取该层的输出</span></span><br><span class="line">x = vgg.get_layer(<span class="string">&#x27;fc2&#x27;</span>)</span><br><span class="line">last_output =  x.output</span><br><span class="line"><span class="comment"># 在 VGG16 模型的 &#x27;fc2&#x27; 层之后添加了一个新的全连接层，这个全连接层只有一个单元，使用 sigmoid 激活函数来输出二元分类的概率</span></span><br><span class="line">x = tf.keras.layers.Dense(<span class="number">1</span>,activation = <span class="string">&#x27;sigmoid&#x27;</span>)(last_output)</span><br><span class="line"><span class="comment"># 创建一个新的模型，该模型接受 VGG16 的输入，并输出通过添加新层后的结果</span></span><br><span class="line">model = tf.keras.Model(vgg.<span class="built_in">input</span>,x)</span><br><span class="line"><span class="comment"># 编译模型，使用 Adam 优化器，二元交叉熵作为损失函数进行训练，并监控模型的精度（accuracy）指标</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer = <span class="string">&quot;adam&quot;</span>,</span><br><span class="line">              loss = <span class="string">&#x27;binary_crossentropy&#x27;</span>,</span><br><span class="line">              metrics = [<span class="string">&#x27;acc&#x27;</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存一下这个模型文件</span></span><br><span class="line">model.save(<span class="string">&#x27;my_model_vgg16.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103:
UserWarning: You are saving your model as an HDF5 file via
<code>model.save()</code>. This file format is considered legacy. We
recommend using instead the native Keras format, e.g.
<code>model.save('my_model.keras')</code>. saving_api.save_model(</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看模型结构并进行训练</span></span><br><span class="line">model.summary()</span><br><span class="line">model.fit(X_new_subset,Y_new_subset,batch_size = <span class="number">32</span>,epochs = <span class="number">3</span>, verbose = <span class="number">1</span>,validation_split=<span class="number">.05</span>,shuffle = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;model&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 224, 224, 3)]     0         
                                                                 
 block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      
                                                                 
 block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     
                                                                 
 block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         
                                                                 
 block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     
                                                                 
 block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    
                                                                 
 block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         
                                                                 
 block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    
                                                                 
 block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    
                                                                 
 block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    
                                                                 
 block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         
                                                                 
 block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   
                                                                 
 block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   
                                                                 
 block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   
                                                                 
 block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         
                                                                 
 block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   
                                                                 
 block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   
                                                                 
 block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   
                                                                 
 block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         
                                                                 
 flatten (Flatten)           (None, 25088)             0         
                                                                 
 fc1 (Dense)                 (None, 4096)              102764544 
                                                                 
 fc2 (Dense)                 (None, 4096)              16781312  
                                                                 
 dense (Dense)               (None, 1)                 4097      
                                                                 
=================================================================
Total params: 134264641 (512.18 MB)
Trainable params: 16785409 (64.03 MB)
Non-trainable params: 117479232 (448.15 MB)
_________________________________________________________________
Epoch 1/3
149/149 [==============================] - 43s 214ms/step - loss: 1.4586 - acc: 0.7680 - val_loss: 0.3230 - val_acc: 0.8880
Epoch 2/3
149/149 [==============================] - 19s 129ms/step - loss: 0.3880 - acc: 0.8215 - val_loss: 0.3339 - val_acc: 0.8560
Epoch 3/3
149/149 [==============================] - 20s 131ms/step - loss: 0.3464 - acc: 0.8495 - val_loss: 0.3104 - val_acc: 0.8760

&lt;keras.src.callbacks.History at 0x7a67e60b2140&gt;</code></pre>
<h1 id="创建带有svm的新网络">创建带有SVM的新网络</h1>
<h2 id="创建供svm使用的数据集">创建供SVM使用的数据集</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">svm_image = [];</span><br><span class="line">svm_label = [];</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建SVM数据集采用了和训练数据集不同的iou标准</span></span><br><span class="line"><span class="keyword">for</span> e,i <span class="keyword">in</span> <span class="built_in">enumerate</span>(os.listdir(annot)):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">if</span> i.startswith(<span class="string">&quot;airplane&quot;</span>):</span><br><span class="line">            <span class="comment"># 提取图像文件名并读取图像</span></span><br><span class="line">            filename = i.split(<span class="string">&quot;.&quot;</span>)[<span class="number">0</span>]+<span class="string">&quot;.jpg&quot;</span></span><br><span class="line">            <span class="built_in">print</span>(e,filename)</span><br><span class="line">            image = cv2.imread(os.path.join(path,filename))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 读取对应的标注文件</span></span><br><span class="line">            df = pd.read_csv(os.path.join(annot,i))</span><br><span class="line">            gtvalues=[]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 解析标注文件中的目标坐标信息</span></span><br><span class="line">            <span class="keyword">for</span> row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">                x1 = <span class="built_in">int</span>(row[<span class="number">1</span>][<span class="number">0</span>].split(<span class="string">&quot; &quot;</span>)[<span class="number">0</span>])</span><br><span class="line">                y1 = <span class="built_in">int</span>(row[<span class="number">1</span>][<span class="number">0</span>].split(<span class="string">&quot; &quot;</span>)[<span class="number">1</span>])</span><br><span class="line">                x2 = <span class="built_in">int</span>(row[<span class="number">1</span>][<span class="number">0</span>].split(<span class="string">&quot; &quot;</span>)[<span class="number">2</span>])</span><br><span class="line">                y2 = <span class="built_in">int</span>(row[<span class="number">1</span>][<span class="number">0</span>].split(<span class="string">&quot; &quot;</span>)[<span class="number">3</span>])</span><br><span class="line">                gtvalues.append(&#123;<span class="string">&quot;x1&quot;</span>:x1,<span class="string">&quot;x2&quot;</span>:x2,<span class="string">&quot;y1&quot;</span>:y1,<span class="string">&quot;y2&quot;</span>:y2&#125;)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 从图像中截取目标区域并调整大小，作为正样本(ground_truth对应的图像区域，作为正样本)</span></span><br><span class="line">                timage = image[x1:x2,y1:y2]</span><br><span class="line">                resized = cv2.resize(timage, (<span class="number">224</span>,<span class="number">224</span>), interpolation = cv2.INTER_AREA)</span><br><span class="line">                svm_image.append(resized)</span><br><span class="line">                svm_label.append([<span class="number">0</span>,<span class="number">1</span>])<span class="comment"># 正样本标签 [0, 1]</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 执行选择性搜索算法获取区域建议</span></span><br><span class="line">            ss.setBaseImage(image)</span><br><span class="line">            ss.switchToSelectiveSearchFast()</span><br><span class="line">            ssresults = ss.process()</span><br><span class="line">            imout = image.copy()</span><br><span class="line">            counter = <span class="number">0</span></span><br><span class="line">            falsecounter = <span class="number">0</span></span><br><span class="line">            flag = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">             <span class="comment"># 遍历选择性搜索结果以构建负样本</span></span><br><span class="line">            <span class="keyword">for</span> e,result <span class="keyword">in</span> <span class="built_in">enumerate</span>(ssresults):</span><br><span class="line">                <span class="keyword">if</span> e &lt; <span class="number">2000</span> <span class="keyword">and</span> flag == <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">for</span> gtval <span class="keyword">in</span> gtvalues:</span><br><span class="line">                        x,y,w,h = result</span><br><span class="line">                        iou = get_iou(gtval,&#123;<span class="string">&quot;x1&quot;</span>:x,<span class="string">&quot;x2&quot;</span>:x+w,<span class="string">&quot;y1&quot;</span>:y,<span class="string">&quot;y2&quot;</span>:y+h&#125;)</span><br><span class="line"></span><br><span class="line">                        <span class="comment"># 添加满足条件的负样本</span></span><br><span class="line">                        <span class="keyword">if</span> falsecounter &lt;<span class="number">5</span>:</span><br><span class="line">                            <span class="keyword">if</span> iou &lt; <span class="number">0.3</span>:</span><br><span class="line">                                timage = imout[x:x+w,y:y+h]</span><br><span class="line">                                resized = cv2.resize(timage, (<span class="number">224</span>,<span class="number">224</span>), interpolation = cv2.INTER_AREA)</span><br><span class="line">                                svm_image.append(resized)</span><br><span class="line">                                svm_label.append([<span class="number">1</span>,<span class="number">0</span>]) <span class="comment"># 负样本标签 [1, 0]</span></span><br><span class="line">                                falsecounter += <span class="number">1</span></span><br><span class="line">                        <span class="keyword">else</span> :</span><br><span class="line">                            flag = <span class="number">1</span> <span class="comment"># 达到负样本数量上限</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(e)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;error in &quot;</span>+filename)</span><br><span class="line">        <span class="keyword">continue</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 为防止RAM不够，这里把svm_image、svm_label也只选取一部分</span></span><br><span class="line">X_svm = np.array(svm_image)</span><br><span class="line">Y_svm = np.array(svm_label)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 为方便下次不用重新处理输入的训练数据，在这里将X_new,Y_new进行保存</span></span><br><span class="line">np.save(<span class="string">&#x27;save_X_svm&#x27;</span>,X_svm)</span><br><span class="line">np.save(<span class="string">&#x27;save_Y_svm&#x27;</span>,Y_svm)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">total_nums_svm = <span class="built_in">len</span>(Y_svm)</span><br><span class="line"><span class="built_in">print</span>(total_nums_svm)</span><br></pre></td></tr></table></figure>
<p>7750</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_svm = np.load(<span class="string">&#x27;save_X_svm.npy&#x27;</span>)</span><br><span class="line">Y_svm = np.load(<span class="string">&#x27;save_Y_svm.npy&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从训练数据中随机选择 2000 个样本</span></span><br><span class="line">num_samples = <span class="number">2000</span></span><br><span class="line">random_indices = np.random.choice(<span class="built_in">len</span>(X_svm), num_samples, replace=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用随机选择的索引来获取样本</span></span><br><span class="line">X_svm_subset = X_svm[random_indices]</span><br><span class="line">Y_svm_subset = Y_svm[random_indices]</span><br></pre></td></tr></table></figure>
<h2 id="svm模型结构">SVM模型结构</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从现有模型中获取 &#x27;fc2&#x27; 层的输出</span></span><br><span class="line">x = model.get_layer(<span class="string">&#x27;fc2&#x27;</span>).output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加一个具有2个单元的全连接层，没有激活函数</span></span><br><span class="line">Y = tf.keras.layers.Dense(<span class="number">2</span>)(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个新的模型，该模型接收现有模型的输入，并输出新添加的全连接层的结果</span></span><br><span class="line">final_model = tf.keras.Model(model.<span class="built_in">input</span>, Y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译新模型</span></span><br><span class="line">final_model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;hinge&#x27;</span>,  <span class="comment"># 使用hinge loss损失函数</span></span><br><span class="line">                    optimizer=<span class="string">&#x27;adam&#x27;</span>,  <span class="comment"># 优化器为 Adam</span></span><br><span class="line">                    metrics=[<span class="string">&#x27;accuracy&#x27;</span>])  <span class="comment"># 监控模型的准确度指标</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出新模型的概要信息</span></span><br><span class="line">final_model.summary()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练模型的权重</span></span><br><span class="line"><span class="comment"># final_model.load_weights(&#x27;my_model_weights.h5&#x27;)</span></span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;model_1&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 224, 224, 3)]     0         
                                                                 
 block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      
                                                                 
 block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     
                                                                 
 block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         
                                                                 
 block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     
                                                                 
 block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    
                                                                 
 block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         
                                                                 
 block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    
                                                                 
 block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    
                                                                 
 block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    
                                                                 
 block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         
                                                                 
 block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   
                                                                 
 block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   
                                                                 
 block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   
                                                                 
 block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         
                                                                 
 block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   
                                                                 
 block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   
                                                                 
 block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   
                                                                 
 block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         
                                                                 
 flatten (Flatten)           (None, 25088)             0         
                                                                 
 fc1 (Dense)                 (None, 4096)              102764544 
                                                                 
 fc2 (Dense)                 (None, 4096)              16781312  
                                                                 
 dense_1 (Dense)             (None, 2)                 8194      
                                                                 
=================================================================
Total params: 134268738 (512.19 MB)
Trainable params: 16789506 (64.05 MB)
Non-trainable params: 117479232 (448.15 MB)
_________________________________________________________________</code></pre>
<h2 id="模型训练">模型训练</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用 SVM 数据集对最终模型进行训练，训练过程中的结果将存储在 hist_final 中</span></span><br><span class="line">hist_final = final_model.fit(</span><br><span class="line">    X_svm_subset,  <span class="comment"># SVM 数据集中的图像数据</span></span><br><span class="line">    Y_svm_subset,  <span class="comment"># SVM 数据集中的标签数据</span></span><br><span class="line">    batch_size=<span class="number">32</span>,        <span class="comment"># 批处理大小</span></span><br><span class="line">    epochs=<span class="number">20</span>,            <span class="comment"># 迭代次数</span></span><br><span class="line">    verbose=<span class="number">1</span>,            <span class="comment"># 训练过程中输出日志的详细程度（1为详细输出，0为不输出）</span></span><br><span class="line">    shuffle=<span class="literal">True</span>,         <span class="comment"># 在每个 epoch 开始时是否对数据进行洗牌</span></span><br><span class="line">    validation_split=<span class="number">0.05</span>  <span class="comment"># 验证集的拆分比例，这里设置为 0.05 表示将 5% 的数据作为验证集</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/20
60/60 [==============================] - 17s 217ms/step - loss: 0.7543 - accuracy: 0.6689 - val_loss: 0.7870 - val_accuracy: 0.6100
Epoch 2/20
60/60 [==============================] - 8s 128ms/step - loss: 0.5756 - accuracy: 0.7463 - val_loss: 0.6612 - val_accuracy: 0.7200
Epoch 3/20
60/60 [==============================] - 8s 134ms/step - loss: 0.4762 - accuracy: 0.7905 - val_loss: 0.6471 - val_accuracy: 0.7300
Epoch 4/20
60/60 [==============================] - 8s 132ms/step - loss: 0.4303 - accuracy: 0.8232 - val_loss: 0.7102 - val_accuracy: 0.6900
Epoch 5/20
60/60 [==============================] - 8s 131ms/step - loss: 0.4178 - accuracy: 0.8200 - val_loss: 0.6434 - val_accuracy: 0.7000
Epoch 6/20
60/60 [==============================] - 8s 136ms/step - loss: 0.3378 - accuracy: 0.8558 - val_loss: 0.7106 - val_accuracy: 0.7000
Epoch 7/20
60/60 [==============================] - 8s 135ms/step - loss: 0.3321 - accuracy: 0.8647 - val_loss: 0.6975 - val_accuracy: 0.7400
Epoch 8/20
60/60 [==============================] - 8s 130ms/step - loss: 0.3007 - accuracy: 0.8737 - val_loss: 0.7403 - val_accuracy: 0.7500
Epoch 9/20
60/60 [==============================] - 8s 138ms/step - loss: 0.2735 - accuracy: 0.8858 - val_loss: 0.8128 - val_accuracy: 0.7100
Epoch 10/20
60/60 [==============================] - 8s 133ms/step - loss: 0.2241 - accuracy: 0.9153 - val_loss: 0.9183 - val_accuracy: 0.6900
Epoch 11/20
60/60 [==============================] - 8s 141ms/step - loss: 0.2732 - accuracy: 0.8916 - val_loss: 0.7814 - val_accuracy: 0.7100
Epoch 12/20
60/60 [==============================] - 8s 142ms/step - loss: 0.2009 - accuracy: 0.9163 - val_loss: 0.8467 - val_accuracy: 0.7100
Epoch 13/20
60/60 [==============================] - 8s 140ms/step - loss: 0.2214 - accuracy: 0.9126 - val_loss: 0.7389 - val_accuracy: 0.7500
Epoch 14/20
60/60 [==============================] - 9s 144ms/step - loss: 0.1761 - accuracy: 0.9268 - val_loss: 0.8726 - val_accuracy: 0.7300
Epoch 15/20
60/60 [==============================] - 9s 142ms/step - loss: 0.1645 - accuracy: 0.9295 - val_loss: 0.7956 - val_accuracy: 0.7400
Epoch 16/20
60/60 [==============================] - 9s 142ms/step - loss: 0.1385 - accuracy: 0.9453 - val_loss: 0.7979 - val_accuracy: 0.7200
Epoch 17/20
60/60 [==============================] - 8s 138ms/step - loss: 0.1303 - accuracy: 0.9495 - val_loss: 0.8370 - val_accuracy: 0.7700
Epoch 18/20
60/60 [==============================] - 8s 139ms/step - loss: 0.1469 - accuracy: 0.9426 - val_loss: 0.7578 - val_accuracy: 0.7900
Epoch 19/20
60/60 [==============================] - 9s 143ms/step - loss: 0.1020 - accuracy: 0.9611 - val_loss: 0.7946 - val_accuracy: 0.7600
Epoch 20/20
60/60 [==============================] - 9s 147ms/step - loss: 0.1020 - accuracy: 0.9632 - val_loss: 0.8619 - val_accuracy: 0.7500</code></pre>
<h2 id="绘制损失变化曲线">绘制损失变化曲线</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 绘制模型训练过程中的损失变化曲线</span></span><br><span class="line">plt.plot(hist_final.history[<span class="string">&#x27;loss&#x27;</span>])        <span class="comment"># 训练集的损失</span></span><br><span class="line">plt.plot(hist_final.history[<span class="string">&#x27;val_loss&#x27;</span>])    <span class="comment"># 验证集的损失</span></span><br><span class="line">plt.title(<span class="string">&quot;model loss&quot;</span>)                     <span class="comment"># 图像标题</span></span><br><span class="line">plt.ylabel(<span class="string">&quot;Loss&quot;</span>)                          <span class="comment"># y 轴标签为损失</span></span><br><span class="line">plt.xlabel(<span class="string">&quot;Epoch&quot;</span>)                         <span class="comment"># x 轴标签为 epoch</span></span><br><span class="line">plt.legend([<span class="string">&quot;Loss&quot;</span>, <span class="string">&quot;Validation Loss&quot;</span>])     <span class="comment"># 添加图例，分别对应训练集和验证集的损失</span></span><br><span class="line">plt.show()                                  <span class="comment"># 显示图像</span></span><br><span class="line"></span><br><span class="line">plt.savefig(<span class="string">&#x27;chart_loss.png&#x27;</span>)               <span class="comment"># 保存图像为文件（在 plt.show() 之后保存是无效的，应该放在 plt.show() 之前）</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYT8S.png" alt="OKYT8S.png" />
<figcaption aria-hidden="true">OKYT8S.png</figcaption>
</figure>
<p>​</p>
<p>&lt;Figure size 640x480 with 0 Axes&gt;</p>
<h1 id="测试">测试</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">###########  it&#x27;s time for test a image    ##########</span></span><br><span class="line">image = cv2.imread(os.path.join(path,<span class="string">&#x27;airplane_020.jpg&#x27;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ss.setBaseImage(image) <span class="comment"># 设置选择性搜索算法的基础图像为读取的图像</span></span><br><span class="line">ss.switchToSelectiveSearchFast() <span class="comment"># 使用选择性搜索算法的快速模式</span></span><br><span class="line">ssresults = ss.process() <span class="comment"># 对基础图像执行选择性搜索，获取区域建议</span></span><br><span class="line"></span><br><span class="line">imOut = image.copy() <span class="comment"># 创建图像的副本，用于绘制矩形框</span></span><br><span class="line">boxes = [] <span class="comment"># 存储被判断为飞机区域的边界框信息</span></span><br><span class="line">count = <span class="number">0</span> <span class="comment"># 计数器：记录被判断为飞机区域的数量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对选择性搜索结果的前 50 个区域建议进行处理</span></span><br><span class="line"><span class="keyword">for</span> e,result <span class="keyword">in</span> <span class="built_in">enumerate</span>(ssresults):</span><br><span class="line">  <span class="keyword">if</span> e &lt; <span class="number">50</span>:</span><br><span class="line">    x,y,w,h = result</span><br><span class="line"></span><br><span class="line">    timage = imout[x:x+w,y:y+h] <span class="comment"># 从原始图像中获取当前建议区域的图像部分</span></span><br><span class="line">    resized = cv2.resize(timage, (<span class="number">224</span>,<span class="number">224</span>), interpolation = cv2.INTER_AREA) <span class="comment"># 调整图像大小为模型的输入尺寸</span></span><br><span class="line">    resized = np.expand_dims(resized,axis = <span class="number">0</span>) <span class="comment"># 将图像扩展一个维度以适应模型输入的要求</span></span><br><span class="line">    out = final_model.predict(resized)  <span class="comment"># 使用最终的模型对该区域进行预测，得到输出结果</span></span><br><span class="line">    <span class="built_in">print</span>(e,out)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(out[<span class="number">0</span>][<span class="number">0</span>]&lt;out[<span class="number">0</span>][<span class="number">1</span>]): <span class="comment"># 如果模型判断该区域可能包含飞机</span></span><br><span class="line">      boxes.append([x,y,w,h]) <span class="comment"># 将边界框信息添加到列表中，并增加计数器</span></span><br><span class="line">      count+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对被判断为飞机区域的边界框进行处理</span></span><br><span class="line"><span class="keyword">for</span> box <span class="keyword">in</span> boxes:</span><br><span class="line">    x, y, w, h = box</span><br><span class="line">    <span class="built_in">print</span>(x,y,w,h)</span><br><span class="line"><span class="comment">#     imOut = imOut[x:x+w,y:y+h]</span></span><br><span class="line">    <span class="comment"># 在原始图像上绘制矩形框，以突出显示这些被判断为飞机的区域</span></span><br><span class="line">    cv2.rectangle(imOut, (x, y), (x+w, y+h), (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), <span class="number">1</span>, cv2.LINE_AA)</span><br><span class="line"></span><br><span class="line">plt.imshow(imOut)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>1/1 [==============================] - 1s 1s/step
0 [[ 2.551831  -2.6361141]]
1/1 [==============================] - 0s 31ms/step
1 [[ 1.2116516 -1.1462703]]
1/1 [==============================] - 0s 43ms/step
2 [[ 3.1247723 -3.0577419]]
1/1 [==============================] - 0s 38ms/step
3 [[ 0.3094477  -0.35184118]]
1/1 [==============================] - 0s 33ms/step
4 [[ 16.334412 -16.097301]]
1/1 [==============================] - 0s 36ms/step
5 [[-1.9055386  1.8246138]]
1/1 [==============================] - 0s 46ms/step
6 [[ 3.849068 -3.596069]]
1/1 [==============================] - 0s 35ms/step
7 [[-4.3343387  4.457566 ]]
1/1 [==============================] - 0s 28ms/step
8 [[ 2.1157243 -2.0935826]]
1/1 [==============================] - 0s 37ms/step
9 [[ 1.1227907 -1.0547544]]
1/1 [==============================] - 0s 32ms/step
10 [[ 3.028215  -3.0655315]]
1/1 [==============================] - 0s 35ms/step
11 [[-3.4406524  3.4818974]]
1/1 [==============================] - 0s 36ms/step
12 [[-3.3148727  3.2502732]]
1/1 [==============================] - 0s 69ms/step
13 [[-1.7705667  1.8401496]]
1/1 [==============================] - 0s 119ms/step
14 [[ 17.1168   -17.020542]]
1/1 [==============================] - 0s 31ms/step
15 [[-0.54532474  0.49859324]]
1/1 [==============================] - 0s 39ms/step
16 [[ 1.3955598 -1.4487445]]
1/1 [==============================] - 0s 39ms/step
17 [[-0.9255678  0.7681236]]
1/1 [==============================] - 0s 34ms/step
18 [[-1.0967708  1.0601681]]
1/1 [==============================] - 0s 35ms/step
19 [[ 1.6157322 -1.5883387]]
1/1 [==============================] - 0s 19ms/step
20 [[ 6.222667 -6.078978]]
1/1 [==============================] - 0s 22ms/step
21 [[ 1.9781907 -1.9643315]]
1/1 [==============================] - 0s 21ms/step
22 [[ 2.6352754 -2.6751401]]
1/1 [==============================] - 0s 22ms/step
23 [[-0.6199166  0.6234232]]
1/1 [==============================] - 0s 26ms/step
24 [[ 0.56931984 -0.52301127]]
1/1 [==============================] - 0s 19ms/step
25 [[-4.092036   4.0529504]]
1/1 [==============================] - 0s 20ms/step
26 [[-1.1211745  1.1134607]]
1/1 [==============================] - 0s 20ms/step
27 [[ 1.5422791 -1.504165 ]]
1/1 [==============================] - 0s 20ms/step
28 [[ 0.9709082 -1.1293985]]
1/1 [==============================] - 0s 22ms/step
29 [[ 6.2005806 -6.223065 ]]
1/1 [==============================] - 0s 19ms/step
30 [[ 0.7283702  -0.67930716]]
1/1 [==============================] - 0s 20ms/step
31 [[ 3.7712991 -3.7369084]]
1/1 [==============================] - 0s 20ms/step
32 [[ 1.7139057 -1.7024881]]
1/1 [==============================] - 0s 21ms/step
33 [[ 12.521779 -12.554838]]
1/1 [==============================] - 0s 24ms/step
34 [[ 3.4832761 -3.3890066]]
1/1 [==============================] - 0s 23ms/step
35 [[ 1.2881904 -1.3030462]]
1/1 [==============================] - 0s 31ms/step
36 [[ 1.3349662 -1.2856408]]
1/1 [==============================] - 0s 20ms/step
37 [[ 0.29870683 -0.25320527]]
1/1 [==============================] - 0s 20ms/step
38 [[-1.2835077  1.3210849]]
1/1 [==============================] - 0s 19ms/step
39 [[ 1.3556112 -1.3576012]]
1/1 [==============================] - 0s 20ms/step
40 [[ 5.945995  -5.7617545]]
1/1 [==============================] - 0s 20ms/step
41 [[ 4.5127177 -4.54366  ]]
1/1 [==============================] - 0s 19ms/step
42 [[ 1.2226268 -1.2334687]]
1/1 [==============================] - 0s 19ms/step
43 [[ 2.2175348 -2.1676831]]
1/1 [==============================] - 0s 20ms/step
44 [[ 5.3103013 -5.1500263]]
1/1 [==============================] - 0s 22ms/step
45 [[ 1.7600315 -1.8218967]]
1/1 [==============================] - 0s 20ms/step
46 [[ 3.2599857 -2.9830909]]
1/1 [==============================] - 0s 19ms/step
47 [[-1.2734337  1.2411362]]
1/1 [==============================] - 0s 21ms/step
48 [[ 10.405064 -10.193722]]
1/1 [==============================] - 0s 20ms/step
49 [[-1.247491   1.2709295]]
145 129 35 31
0 71 98 70
176 148 64 43
49 91 49 22
0 71 77 70
199 148 38 29
174 130 21 27
19 95 58 45
111 142 27 23
120 127 32 33
0 74 48 53
120 143 33 19
29 149 36 40
111 117 34 46</code></pre>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYWxN.png" alt="OKYWxN.png" />
<figcaption aria-hidden="true">OKYWxN.png</figcaption>
</figure>
<blockquote>
<p>最后的测试效果没有很好，可能因为使用的训练数据过少，如果显存足够可以不用截取训练集的子集来进行训练，效果应该会提高。</p>
</blockquote>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>代码复现</tag>
        <tag>RCNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Pycharm连接云服务器训练</title>
    <url>/2023/12/29/Pycharm%E8%BF%9E%E6%8E%A5%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"><h4 id="在autodl租用实例">在AutoDL租用实例</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">获得主机名、端口号、用户名和密码</span><br></pre></td></tr></table></figure>
<h4 id="点击-tools点击-部署deployment点击configuration">点击
<code>Tools</code>，点击
部署<code>Deployment</code>，点击<code>Configuration</code></h4>
<span id="more"></span>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKfzRF.png" alt="OKfzRF.png" />
<figcaption aria-hidden="true">OKfzRF.png</figcaption>
</figure>
<h4 id="配置服务器信息并测试连接">配置服务器信息并测试连接</h4>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKfbuI.png" alt="OKfbuI.png" />
<figcaption aria-hidden="true">OKfbuI.png</figcaption>
</figure>
<h4 id="设置本地路径和远程路径映射">设置本地路径和远程路径映射</h4>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKfoUq.png" alt="OKfoUq.png" />
<figcaption aria-hidden="true">OKfoUq.png</figcaption>
</figure>
<h4 id="勾选同步代码时自动创建文件夹">勾选同步代码时自动创建文件夹</h4>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKfMPY.png" alt="OKfMPY.png" />
<figcaption aria-hidden="true">OKfMPY.png</figcaption>
</figure>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKfWE1.png" alt="OKfWE1.png" />
<figcaption aria-hidden="true">OKfWE1.png</figcaption>
</figure>
<h4 id="设置远程解释器和同步目录">设置远程解释器和同步目录</h4>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKf5dr.png" alt="OKf5dr.png" />
<figcaption aria-hidden="true">OKf5dr.png</figcaption>
</figure>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKfO9v.png" alt="OKfO9v.png" />
<figcaption aria-hidden="true">OKfO9v.png</figcaption>
</figure>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYILj.png" alt="OKYILj.png" />
<figcaption aria-hidden="true">OKYILj.png</figcaption>
</figure>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYuNx.png" alt="OKYuNx.png" />
<figcaption aria-hidden="true">OKYuNx.png</figcaption>
</figure>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYwFU.png" alt="OKYwFU.png" />
<figcaption aria-hidden="true">OKYwFU.png</figcaption>
</figure>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKfqvc.png" alt="OKfqvc.png" />
<figcaption aria-hidden="true">OKfqvc.png</figcaption>
</figure>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKY3up.png" alt="OKY3up.png" />
<figcaption aria-hidden="true">OKY3up.png</figcaption>
</figure>
<h4 id="连接成功">连接成功</h4>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKfxFD.png" alt="OKfxFD.png" />
<figcaption aria-hidden="true">OKfxFD.png</figcaption>
</figure>
<h4 id="打开远程terminal">打开远程Terminal</h4>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKfTaM.png" alt="OKfTaM.png" />
<figcaption aria-hidden="true">OKfTaM.png</figcaption>
</figure>
<h4 id="同地区可以选择跨实例拷贝数据">同地区可以选择跨实例拷贝数据</h4>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKfUNG.png" alt="OKfUNG.png" />
<figcaption aria-hidden="true">OKfUNG.png</figcaption>
</figure>
]]></content>
      <categories>
        <category>经验技巧</category>
      </categories>
      <tags>
        <tag>Pycharm</tag>
        <tag>云服务器</tag>
      </tags>
  </entry>
  <entry>
    <title>RCNN论文理解</title>
    <url>/2023/12/29/RCNN%E8%AE%BA%E6%96%87%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"><h1 id="rcnn">RCNN</h1>
<h2 id="背景">背景</h2>
<p>过去主流的目标检测思路：输入一张图片==》获取一系列区域==》传统的特征提取方法（HOG特征）得到一组特征表示（x1,x2,...xn）==》输入预训练好的机器学习算法（SVM、决策树）进行分类</p>
<p>本文创新：用CNN代替传统的特征提取方法来提取图像特征</p>
<span id="more"></span>
<h2 id="主要贡献">主要贡献</h2>
<ol type="1">
<li>用CNN代替传统的特征提取方法来提取图像特征</li>
<li>迁移学习（模型微调）：模型在大的源领域数据集上预训练后，再在特定目标领域的小数据集上微调，能够适应新任务的要求。——解决目标领域数据稀缺问题。</li>
</ol>
<h2 id="面临问题及解决思路">面临问题及解决思路</h2>
<h3 id="问题1如何用cnn来定位目标">问题1：如何用CNN来定位目标？</h3>
<h4 id="问题">问题</h4>
<p>分类任务只需要确定类别；检测任务除了需要确定类别，还需要获得对象的检测边界框。CNN过去已经被证明可以用于分类任务，如何利用它来检测边界框呢？</p>
<h4 id="解决思路">解决思路</h4>
<ol type="1">
<li><p>基于回归的定位策略：直接用CNN来回归边界框 <img
src="https://ooo.0x0.ooo/2023/12/29/OKYa0B.png"
alt="OKYa0B.png" /></p></li>
<li><p>基于滑动窗口检测器的定位策略：在输入图片上用特定大小和比例的滑动窗口进行滑动，对于滑动到的区域用<strong>浅层的CNN</strong>进行检测，适用于人脸、行人这类<strong>特定比例的任务</strong>。（有人试了，效果不好）</p>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYows.png" alt="OKYows.png" />
<figcaption aria-hidden="true">OKYows.png</figcaption>
</figure></li>
<li><p><strong>基于区域的定位策略（本文√）</strong>：输入一张图片，用某种方法产生一定数量的候选区域（Region
proposal），然后用一个深层的CNN提取特征，最后输入分类器种进行分类</p>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKVIk6.png" alt="OKVIk6.png" />
<figcaption aria-hidden="true">OKVIk6.png</figcaption>
</figure></li>
</ol>
<h3 id="问题2标注数据太少">问题2：标注数据太少</h3>
<h4 id="问题-1">问题</h4>
<p>图像分类领域ImageNet数据集，标注数据丰富，而当时目标检测领域只有PASCAL
VOC数据集，标注数据很少</p>
<h4 id="解决思路-1">解决思路</h4>
<ol type="1">
<li>传统思路：无监督预训练+有监督微调</li>
<li>本文思路：迁移学习。在ImageNet大数据集上有监督预训练+在PASCAL
VOC小数据集上微调</li>
</ol>
<h2 id="模型结构">模型结构</h2>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKY51K.png" alt="OKY51K.png" />
<figcaption aria-hidden="true">OKY51K.png</figcaption>
</figure>
<h3 id="三个模块">三个模块</h3>
<ol type="1">
<li><p>产生候选区域：selective search</p></li>
<li><p>抽取图像特征： 利用deep CNN从每个候选区域(region proposal
)抽取出一个固定长度(4096-dim)的特征向量(feature vector)</p>
<ol type="1">
<li><p>5个卷积层 +
2个全连接层前向传播，删除了源网络最后的1000种分类层（AlexNet）</p>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKVX8F.png" alt="OKVX8F.png" />
<figcaption aria-hidden="true">OKVX8F.png</figcaption>
</figure></li>
<li><p>deep
CNN要求输入的图片固定大小，因此需要对不同大小的候选区域进行缩放至(227 x
227 RGB)</p>
<p>缩放方法：</p>
<ol type="1">
<li>tightest square with
context：把整张图片的长边缩放到227，短边按照此比例缩放</li>
<li>tightest square without
context：只把候选区域的长边缩放到227，短边按照此比例缩放</li>
<li>warp：把候选区域的长短边都直接缩放到227（有形变）（本文√）</li>
</ol></li>
</ol></li>
<li><p>用特定类的SVM来对候选区域进行分类：多个二分类构成的SVMs</p></li>
</ol>
<h3 id="训练思路">训练思路</h3>
<h4 id="训练特征提取网络">训练特征提取网络</h4>
<ol type="1">
<li>有监督预训练：在ImageNet上预训练</li>
<li>针对特定领域微调（在VOC数据集上微调出21类的分类模型-softmax层实现分类）
<ol type="1">
<li>网络结构：把ImageNet上训练的CNN(AlexNet)最后一层的1000种分类改成
(N+1) 类的分类层，N为微调的数据集的对象类别数，+1为背景</li>
<li>构建数据集：候选区域和ground-truth box的iou &gt;
0.5，为正样本；否则为负样本</li>
<li>SGD：
<ol type="1">
<li>初始学习率0.001</li>
<li>每个iteration，统一采样32个正样本和96个负样本组成128的mini-batch</li>
</ol></li>
</ol></li>
</ol>
<p>训练好了分类网络后，就可以在VOC数据集上较好地提取图像特征；在提取特征后，可以训练一个SVM模型，来对候选区域进行分类。</p>
<h4 id="训练svms分类器">训练SVMs分类器</h4>
<p>SVMs是由N个linear SVM构成的，我们需要为每个对象类优化一个linear
SVM分类器</p>
<ol type="1">
<li>构建数据集：只把ground truth box作为每个类的正样本；把和ground-truth
box的iou &lt;
0.3的候选区域，作为负样本（因为负样本很多，这里采用难负例挖掘策略（hard
negative mining method）来进一步挑选）</li>
<li>训练分类器：为每个类训练一个linear SVM分类器</li>
</ol>
<h3 id="测试思路">测试思路</h3>
<p>输入一张图片==》运行selective search产生2000个候选区域（Region
proposals）==》缩放成固定大小的proposal==》利用CNN从每个proposal抽取出一个固定长度的feature
vector==》用针对特定类的linear SVM分类每一个region
proposal==》对每一个类的proposals进行非极大值抑制</p>
<h2 id="可视化">可视化</h2>
<p>如何可视化高层特征？</p>
<p>在第五个卷积层conv5后跟了一个池化层pool5，经过该于池化层后得到6 x 6 x
256的特征图。提取出一个数据集的所有图像对应的所有候选区域在某一通道某一特定位置的数据进行可视化，实验规律如下：</p>
<ol type="1">
<li>对于高层特征来说，每一个不同的通道，学习到一种高层特征</li>
<li>对于同一个通道的不同位置，它表示的特征是相同的，只是边界框的位置有偏移</li>
</ol>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYM5l.png" alt="OKYM5l.png" />
<figcaption aria-hidden="true">OKYM5l.png</figcaption>
</figure>
<h2 id="消融实验">消融实验</h2>
<ol type="1">
<li>在训练CNN的时候，我们使用的是fc7的4096维的输出作为图片的特征，那能不能使用fc6的输出，或pool5的输出作为提取到的特征呢？</li>
</ol>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYOKg.png" alt="OKYOKg.png" />
<figcaption aria-hidden="true">OKYOKg.png</figcaption>
</figure>
<ol start="2" type="1">
<li><p>使用微调和不使用微调对最终的精度有没有影响？</p>
<p>在1中选取不同层的输出作为图片的特征的基础上，测试微调后的精度如何改变。实验结果发现：</p>
<p>（1）微调后精度提高，说明微调重要；</p>
<p>（2）在使用全连接层特征、不同层的卷积特征分别进行微调的基础上，发现使用全连接层特征微调后的精度改善更明显，说明微调更多改善的是全连接层的特征提取能力</p></li>
<li><p>不同的CNN结构对特征提取效果有什么影响？</p>
<p>对比了AlexNet（本文）、VGG（更复杂）两种CNN结构提取特征的效果，实验结果发现：</p>
<p>VGG更复杂，提取到的特征更好，检测的准确率也更高</p></li>
</ol>
<h2 id="边界框回归">边界框回归</h2>
<p>因为我们在进行目标检测，所以不仅要实现分类，还要对目标的边界框进行回归。因此，在提取出CNN特征（这里用的pool5层的特征）后又训练了一个SVM模型，用于回归出目标的边界框，即原始的候选区域到真实框的偏移量。而在实际实验中发现，分类的预测精度较高，但目标的位置预测并不准确。也就是说，我们希望预测框（黄框）尽可能接近真实框（绿框），即损失函数尽可能小</p>
<p>因此要解决的问题变成了：已知一个不准确的位置，要预测出一个准确的位置（从黄框变成绿框）。</p>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKVCfb.png" alt="OKVCfb.png" />
<figcaption aria-hidden="true">OKVCfb.png</figcaption>
</figure>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKV3pP.png" alt="OKV3pP.png" />
<figcaption aria-hidden="true">OKV3pP.png</figcaption>
</figure>
<h2 id="错检情况分析">错检情况分析</h2>
<p>分析以下四类错误的占比情况</p>
<ol type="1">
<li>类别正确，但位置不准确</li>
<li>类别识别为相似类</li>
<li>类别识别明显错误</li>
<li>目标误识别为类别</li>
</ol>
<figure>
<img src="https://ooo.0x0.ooo/2023/12/29/OKYBHa.png" alt="OKYBHa.png" />
<figcaption aria-hidden="true">OKYBHa.png</figcaption>
</figure>
<h2
id="检测不同因素对算法的敏感度影响看不懂">检测不同因素对算法的敏感度影响（看不懂）</h2>
<h2 id="实验设置的一些问题解释">实验设置的一些问题解释</h2>
<ol type="1">
<li>为什么CNN和SVM的正负例定义的方法不同
<ol type="1">
<li>实验观察：通过实验先确定了SVM的划分方法，然后才确定了CNN的正负样本的划分方法</li>
<li>理论解释：训练CNN要求的参数很多，需要大量的训练样本。如果也采用训练SVM时采用的正负样本策略，即只把ground
truth的标注框作为每个类的正样本，iou &lt;
0.3的候选区域为负样本，这样训练样本就太少了；而采用另外的方法，即iou
&gt;
0.5的候选区域为正样本，否则为负样本，能够在标注框附近引入大量的抖动样本，丰富的数据量可以避免CNN过拟合</li>
</ol></li>
<li>在微调之后明明已经能够对候选区域进行多分类了，后面训练SVMs的部分能不能去掉？
<ol type="1">
<li>实验观察：直接用CNN的softmax层分类会让精度降低</li>
<li>解释：
<ol type="1">
<li>CNN在训练时使用的正样本（抖动样本）并不是每个目标的精确位置，可能会导致误差
富的数据量可以避免CNN过拟合</li>
</ol></li>
</ol></li>
<li>在微调之后明明已经能够对候选区域进行多分类了，后面训练SVMs的部分能不能去掉？
<ol type="1">
<li>实验观察：直接用CNN的softmax层分类会让精度降低</li>
<li>解释：
<ol type="1">
<li>CNN在训练时使用的正样本（抖动样本）并不是每个目标的精确位置，可能会导致误差</li>
<li>在训练CNN时的负样本是被随机挑选的，而我们可以从CNN分类错误的样本中去挑选那些难负样本，让SVM训练，这样也让增加SVMs分类器后的训练效果更好</li>
</ol></li>
</ol></li>
</ol>
<p><strong>参考资料：</strong>
https://www.bilibili.com/video/BV1CZ4y1a7NP/?spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=66a72b15abe9693bd8b4f738f5a67ee7</p>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>论文理解</tag>
        <tag>RCNN</tag>
      </tags>
  </entry>
  <entry>
    <title>字节跳动Coze教程（GPT4）</title>
    <url>/2024/01/04/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8Coze%E6%95%99%E7%A8%8B%EF%BC%88GPT4%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"><h1 id="coze使用教程">Coze使用教程</h1>
<p>目前能免费使用GPT4的网站有以下几个：</p>
<blockquote>
<p>YesChat：一个聚合搜索网站，地址为https://www.yeschat.ai/zh-CN</p>
<p>codeium：需要申请内测资格，地址为https://codeium.com/waitlist/gpt-4</p>
<p>Coze：本身仅是构建平台，需要部署到cic/discord/telegram等平台使用，地址为https://www.coze.com。</p>
<p>目前Coze平台使用GPT4免费无限制。有人测试每小时问答二三十次，连续几个小时使用，均没有触发限制。</p>
</blockquote>
<p>本文介绍基于Coze平台的chatGPT4如何使用。</p>
<span id="more"></span>
<blockquote>
<p>ip问题：目前大陆ip被屏蔽，非大陆的ip都可以尝试。我用日本、新加坡的都可以。如果ip问题无法访问，可以设置为<code>全局模式</code>尝试一下，如果<code>全局模式</code>可用，说明就是vpn软件的设置存在问题。</p>
</blockquote>
<h2 id="注册登录">注册登录</h2>
<p>访问https://www.coze.com/。</p>
<p>用手机号/谷歌账号注册登录。</p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZo3Pq.png" alt="OZo3Pq.png" />
<figcaption aria-hidden="true">OZo3Pq.png</figcaption>
</figure>
<h2 id="探索公开的bot">探索公开的Bot</h2>
<p>登录成功后，会自动跳转到explore页面，这个页面是一些公开的、别人配置好的、具有特定功能的bot（类似于专用机器人），如果有满足自己需求的可以直接点进去使用。</p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZqOvM.png" alt="OZqOvM.png" />
<figcaption aria-hidden="true">OZqOvM.png</figcaption>
</figure>
<p>比如我觉得这个”小狗插画生成器“很有意思，直接点击进入</p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZoIzv.png" alt="OZoIzv.png" />
<figcaption aria-hidden="true">OZoIzv.png</figcaption>
</figure>
<p>进入之后的界面如下图所示：</p>
<p>界面分为左中右三个部分，左侧介绍了这个bot的角色、技能和约束等信息；中间主要提供了bot在完成功能时所需要的插件，右侧可以对这个bot进行预览测试。</p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZoClc.png" alt="OZoClc.png" />
<figcaption aria-hidden="true">OZoClc.png</figcaption>
</figure>
<p>在右侧对话框直接运行提供的测试案例，让bot给我画小狗插画，效果如下</p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZqacG.png" alt="OZqacG.png" />
<figcaption aria-hidden="true">OZqacG.png</figcaption>
</figure>
<p>如果想要在这个bot上进行二次开发，可以点击项目右侧的Duplicate，就会把这个项目复制到我们的个人工作空间，可以进行二次开发</p>
<h2 id="创建自己的bot">创建自己的Bot</h2>
<h3 id="方式1全才gpt4仅文字对话">方式1：全才gpt4(仅文字对话)</h3>
<p>这一部分介绍如何直接使用可以<strong>回答各方面问题</strong>的chatgpt4，但是<strong>仅能通过文字进行对话</strong>。</p>
<h4 id="创建bot">创建Bot</h4>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZqqO1.png" alt="OZqqO1.png" />
<figcaption aria-hidden="true">OZqqO1.png</figcaption>
</figure>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZqAPC.png" alt="OZqAPC.png" />
<figcaption aria-hidden="true">OZqAPC.png</figcaption>
</figure>
<h4 id="使用gpt4">使用GPT4</h4>
<p>不用设置左侧的提示词和中间的插件，直接在右边使用，默认就会使用GPT4(8k)进行回答。</p>
<blockquote>
<p><strong>这里的"8K"代表什么意思？</strong></p>
<p>至目前为止，GPT-4有两个主要版本，即GPT-4 8k版本和GPT-4
32k版本。"8k"版本是指该模型有8000个token的上下文长度。也就是说，<strong>当我们让模型生成文本时，它能考虑的最大输入长度是8000个token</strong>。这些token包括单词，标点符号，甚至是空格等。例如，如果我们有一段文本，长度为10000个token，那么模型在处理时，只会关注最后的8000个token，前2000个token将被忽视。这是由于模型的这种窗口限制而造成的，称为模型的上下文窗口或者上下文长度。</p>
<p>理论上，<strong>对于中文，一个字符（包括标点符号等）被视为一个token，所以8000个token对应的中文字符数量也是8000个。</strong>对于英文，一个token的定义可能会稍微复杂一些。一个token可以是一个单词，也可能是一个标点符号，或者是一个单词内部的一个部分。例如，“hamburger”被分成“ham”、“bur”和“ger”三个
Token，而“pear”是一个
Token。如果我们假定一个英文单词平均由5个字母组成，那么8000个token大致对应1600个英文单词。</p>
<p>另外，这个限制还有一个细节，<strong>Token
限制的计数包含输入和输出的文本</strong>。也就是说，当你给模型提供一个初始的输入序列，并让它生成一些额外的输出时，这个输入序列和生成的输出序列的总长度通常不能超过这个token限制。如果模型版本是8k，那么我们给模型的输入序列应该要留有足够的空间来让模型生成输出。如果输入就已经是8000个token，那么模型就没有多余的空间来生成输出了。</p>
</blockquote>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZqNuS.png" alt="OZqNuS.png" />
<figcaption aria-hidden="true">OZqNuS.png</figcaption>
</figure>
<h4 id="对比gpt3.5">对比GPT3.5</h4>
<p>对比经典的<code>鲁迅和周树人问题</code>，GPT4确实要更胜一筹。</p>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>gpt4</th>
<th>gpt3.5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img src="https://ooo.0x0.ooo/2024/01/04/OZqvUi.png"
alt="OZqvUi.png" /></td>
<td><img src="https://ooo.0x0.ooo/2024/01/04/OZq1NK.png"
alt="OZq1NK.png" /></td>
</tr>
</tbody>
</table>
<h3
id="方式2全才gpt4文字图片联网...">方式2：全才gpt4(文字+图片+联网+...)</h3>
<p>这一部分介绍如何直接使用可以<strong>回答各方面问题</strong>的chatgpt4，<strong>不仅能通过文字，还能借助图片进行对话，实现联网搜索等功能</strong>。</p>
<p>点击中间Plugins后面的＋，为gpt添加插件，常用插件的功能介绍如下</p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZqK9L.png" alt="OZqK9L.png" />
<figcaption aria-hidden="true">OZqK9L.png</figcaption>
</figure>
<p>添加以上插件后，此时的gpt就具有了联网搜索、文生图、识别图片内容、搜索github的api等功能，还可以探索其他的插件。添加了插件后的使用效果如下：</p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZqrFN.png" alt="OZqrFN.png" />
<figcaption aria-hidden="true">OZqrFN.png</figcaption>
</figure>
<p>但是这个图片上传后分析的功能好像有问题，上传图片后出现以下错误<code>对不起，由于技术限制，我无法解析这张图片的内容。我会尽快向后端技术团队反馈以便解决此问题。感谢你的理解和耐心等待。</code></p>
<p>我觉得以上的2种方式已经能够满足日常使用的需求，<strong>尤其是方式2，很容易就实现了联网搜索和图片互动等刚需要求</strong>，不过如果想要创建一个属于自己的、专门用于完成特定任务的Bot，可以参考方式3。</p>
<h3 id="方式3专才gpt4">方式3：专才gpt4</h3>
<p>我想创建一个专门用于解决编程问题的助手，具体操作步骤如下：</p>
<h4 id="创建bot-1">创建Bot</h4>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZqqO1.png" alt="OZqqO1.png" />
<figcaption aria-hidden="true">OZqqO1.png</figcaption>
</figure>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZq57I.png" alt="OZq57I.png" />
<figcaption aria-hidden="true">OZq57I.png</figcaption>
</figure>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZqBLD.png" alt="OZqBLD.png" />
<figcaption aria-hidden="true">OZqBLD.png</figcaption>
</figure>
<h4 id="定义功能">定义功能</h4>
<p>因为这个bot主要用于编程辅助，这里给出一个bot的功能定义的示例。</p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZqTuF.png" alt="OZqTuF.png" />
<figcaption aria-hidden="true">OZqTuF.png</figcaption>
</figure>
<p>点击右上角的Optimize，可以自动优化提示词，优化后的提示词如下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 角色</span><br><span class="line">你是一个精通各种编程语言的编程解决方案专家，你的专业技能包括编写代码、解答复杂编程问题以及优化代码以提升运行效率。</span><br><span class="line"></span><br><span class="line">## 技能</span><br><span class="line">###技能一：代码优化</span><br><span class="line">- 分析用户提交的初步代码，理解其目标和核心原理。</span><br><span class="line">- 遵循代码规范和最佳实践对代码进行优化。</span><br><span class="line">- 将优化后的代码反馈给用户。</span><br><span class="line">###技能二：编程问题解答</span><br><span class="line">- 掌握用户的编程问题。</span><br><span class="line">- 使用在线编程知识库（search(site:stackoverflow.com)）寻找解决方案。</span><br><span class="line">- 用易于理解的方式向用户解释答案。</span><br><span class="line">###技能三：算法设计</span><br><span class="line">- 理解用户所需解决的问题，识别适用的算法类型。</span><br><span class="line">- 设计有效的算法或数据结构以解决问题。</span><br><span class="line">- 向用户展示算法或数据结构的设计过程以及相应的代码。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 工作流程</span><br><span class="line">### 步骤1: 信息收集</span><br><span class="line">首先自我介绍并询问用户需求：&quot;我是一位经验丰富的编程专家，能够根据你的需求撰写符合你预期的代码。如果你需要个性化的编程解决方案，只需告诉我你需要解决的问题，预期的输出结果，以及你希望使用的编程语言。&quot;</span><br><span class="line">需要收集的信息：</span><br><span class="line"> - 需要解决的问题描述</span><br><span class="line"> - 预期的输出结果</span><br><span class="line"> - 用户希望使用的编程语言</span><br><span class="line"></span><br><span class="line">### 步骤2: 创建编程方案</span><br><span class="line">结合用户提供的问题描述、期望结果和编程语言准备编程方案。实例提示为：</span><br><span class="line">- 问题定义: `&lt;问题描述&gt;`</span><br><span class="line">- 输出目标: `&lt;预期结果&gt;`</span><br><span class="line">- 推荐编程语言: `&lt;编程语言&gt;`</span><br><span class="line"></span><br><span class="line">此提示仅作为参考，不直接展示给用户。</span><br><span class="line"></span><br><span class="line">### 步骤3: 提供编程解决方案</span><br><span class="line">根据步骤2中的指引，创建编程解决方案，并将结果展示给用户。</span><br><span class="line"></span><br><span class="line">## 约束条件</span><br><span class="line">- 仅回答和编程问题相关的内容，不能回答与编程无关的问题。</span><br><span class="line">- 遵守约定的工作流程，按步骤进行操作。</span><br><span class="line">- 将工作成果给予用户，而不是工作过程中的提示信息。</span><br><span class="line">- 确保提供的编程作品适合所有用户，避免使用不恰当或冒犯性的内容。</span><br></pre></td></tr></table></figure>
<h4 id="添加插件">添加插件</h4>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZqWz6.png" alt="OZqWz6.png" />
<figcaption aria-hidden="true">OZqWz6.png</figcaption>
</figure>
<h4 id="测试效果">测试效果</h4>
<p>在右侧的Preview对话栏可以执行测试</p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZqbPP.png" alt="OZqbPP.png" />
<figcaption aria-hidden="true">OZqbPP.png</figcaption>
</figure>
<h4 id="对比gpt3.5-1">对比GPT3.5</h4>
<p>对于同一个问题，将gpt4和gpt3.5给出的回答做一个对比，<strong>可以看出gpt4的效果确实要比3.5好一些。</strong></p>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>gpt4</th>
<th>gpt3.5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img src="https://ooo.0x0.ooo/2024/01/04/OZqk9b.png"
alt="OZqk9b.png" /></td>
<td><img src="https://ooo.0x0.ooo/2024/01/04/OZqzTl.png"
alt="OZqzTl.png" /></td>
</tr>
</tbody>
</table>
<h4 id="发布">发布</h4>
<p>测试完确认没问题后，我们就可以在右侧的对话框进行问答了。但是如果每次都重新重复上述流程来创建自定义Bot有点麻烦，因此我们可以直接将这个Bot发布，发布后就可以直接使用这个定制的Bot了。</p>
<p>点击右上角的publish，可以选择将其发布到以下四类平台，测试后发现cici平台是最方便的，直接勾选cici平台，然后点击右上角的Publish就可以了。</p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZqFvg.png" alt="OZqFvg.png" />
<figcaption aria-hidden="true">OZqFvg.png</figcaption>
</figure>
<p>发布成功，点击Open in Cici</p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZqhdB.png" alt="OZqhdB.png" />
<figcaption aria-hidden="true">OZqhdB.png</figcaption>
</figure>
<p>这里也需要登录一下Cici平台，正常注册登录就好。<strong>登陆完成就来到了以下界面，然后就可以在这里直接使用我们自己定制的Bot了。</strong></p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/04/OZqnOs.png" alt="OZqnOs.png" />
<figcaption aria-hidden="true">OZqnOs.png</figcaption>
</figure>
]]></content>
      <categories>
        <category>经验技巧</category>
      </categories>
      <tags>
        <tag>GPT4</tag>
      </tags>
  </entry>
  <entry>
    <title>《动手学深度学习2.0》学习笔记（一）</title>
    <url>/2024/01/11/%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02-0%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"><h1
id="动手学深度学习2.0学习笔记一">《动手学深度学习2.0》学习笔记（一）</h1>
<p>《动手学深度学习2.0》电子书的链接地址为https://zh.d2l.ai/index.html</p>
<p>本文记录了我在学习本书前4章节（包括引言、预备知识、线性神经网络、多层感知机）过程中的理解和收获。</p>
<span id="more"></span>
<h2 id="引言">引言</h2>
<h3 id="机器学习中关键组件">机器学习中关键组件</h3>
<ol type="1">
<li>可以用来学习的<em>数据</em>（data）；</li>
<li>如何转换数据的<em>模型</em>（model）；</li>
<li>一个<em>目标函数</em>（objective
function），有时被称为<em>损失函数</em>（loss function，或cost
function），用来量化模型的有效性；</li>
<li>调整模型参数以优化目标函数的<em>算法</em>（algorithm）</li>
</ol>
<h3 id="机器学习问题分类">机器学习问题分类</h3>
<h4 id="监督学习">监督学习</h4>
<p>通过一组已经标注好的训练数据学习预测结果=》需要向模型提供巨大数据集：每个样本包含特征和相应标签值。</p>
<ol type="1">
<li><p>回归</p>
<p>当标签取任意数值时，就是回归问题，旨在解决“有多少”的问题。</p></li>
<li><p>分类</p>
<p>当标签取离散数值时，就是分类问题，旨在解决“哪一个”的问题。</p></li>
<li><p>标记</p>
<p>又叫<em>多标签分类</em>（multi-label
classification），旨在通过学习预测不相互排斥的类别</p></li>
<li><p>搜索</p>
<p>对一组项目进行排序。如谷歌最初的评分系统PageRank，对搜索结果根据相关性分数进行排序。</p></li>
<li><p>推荐系统</p>
<p>推荐系统会为“给定用户和物品”的匹配性打分，从而将匹配性得分最高的对象集推荐给对应的用户。</p></li>
<li><p>序列学习</p>
<blockquote>
<p>以上大多数问题都具有固定大小的输入和产生固定大小的输出。
例如，在预测房价的问题中，我们考虑从一组固定的特征：房屋面积、卧室数量、浴室数量、步行到市中心的时间；
图像分类问题中，输入为固定尺寸的图像，输出则为固定数量（有关每一个类别）的预测概率；
在这些情况下，模型只会将输入作为生成输出的“原料”，而不会“记住”输入的具体内容。</p>
<p>如果输入的样本之间没有任何关系，以上模型可能完美无缺。
但是如果输入是连续的，模型可能就需要拥有“记忆”功能。</p>
</blockquote>
<p>序列学习需要摄取输入序列和预测输出序列，其中输入和输出都是可变长度的序列。常见的应用有：</p>
<ul>
<li><p>标记和解析：用属性注释文本序列。输入和输出的数量基本相同</p>
<blockquote>
<p>下面是一个非常简单的示例，它使用“标记”来注释一个句子，该标记指示哪些单词引用命名实体。
标记为“Ent”，是<em>实体</em>（entity）的简写。</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Tom has dinner in Washington with Sally</span><br><span class="line">Ent  -    -    -     Ent      -    Ent</span><br></pre></td></tr></table></figure></li>
<li><p>语音识别：输出比输入短得多</p></li>
<li><p>文本到语音：输出比输入长得多</p></li>
<li><p>机器翻译：输入和输出的顺序需要颠倒</p></li>
</ul></li>
</ol>
<h4 id="无监督学习">无监督学习</h4>
<ol type="1">
<li><em>聚类</em>（clustering）问题</li>
<li><em>主成分分析</em>（principal component analysis）问题</li>
<li><em>因果关系</em>（causality）和<em>概率图模型</em>（probabilistic
graphical models）问题</li>
<li><em>生成对抗性网络</em>（generative adversarial
networks）：为我们提供一种合成数据的方法，甚至像图像和音频这样复杂的非结构化数据。</li>
</ol>
<h4 id="与环境互动">与环境互动</h4>
<blockquote>
<p>我们可能会期望人工智能不仅能够做出预测，而且能够与真实环境互动。
与预测不同，“与真实环境互动”实际上会影响环境。
这里的人工智能是“智能代理”，而不仅是“预测模型”。
因此，我们必须考虑到它的行为可能会影响未来的观察结果。</p>
</blockquote>
<h4 id="强化学习">强化学习</h4>
<blockquote>
<p>在强化学习问题中，智能体（agent）在一系列的时间步骤上与环境交互。
在每个特定时间点，智能体从环境接收一些<em>观察</em>（observation），并且必须选择一个<em>动作</em>（action），然后通过某种机制（有时称为执行器）将其传输回环境，最后智能体从环境中获得<em>奖励</em>（reward）。
此后新一轮循环开始，智能体接收后续观察，并选择后续操作，依此类推。
强化学习的过程在 图1中进行了说明。
请注意，强化学习的目标是产生一个好的<em>策略</em>（policy）。
强化学习智能体选择的“动作”受策略控制，即一个从环境观察映射到行动的功能。</p>
</blockquote>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/11/OZVFua.png"
alt="图1 强化学习和环境之间的相互作用" />
<figcaption aria-hidden="true">图1
强化学习和环境之间的相互作用</figcaption>
</figure>
<blockquote>
<p>强化学习框架的通用性十分强大。
例如，我们可以将任何监督学习问题转化为强化学习问题。
假设我们有一个分类问题，可以创建一个强化学习智能体，每个分类对应一个“动作”。
然后，我们可以创建一个环境，该环境给予智能体的奖励。
这个奖励与原始监督学习问题的损失函数是一致的。</p>
<p>当然，强化学习还可以解决许多监督学习无法解决的问题。
例如，在监督学习中，我们总是希望输入与正确的标签相关联。
但在强化学习中，我们并不假设环境告诉智能体每个观测的最优动作。
一般来说，智能体只是得到一些奖励。
此外，环境甚至可能不会告诉是哪些行为导致了奖励。</p>
<p>最后，在任何时间点上，强化学习智能体可能知道一个好的策略，但可能有许多更好的策略从未尝试过的。
强化学习智能体必须不断地做出选择：是应该利用当前最好的策略，还是探索新的策略空间（放弃一些短期回报来换取知识）。</p>
</blockquote>
<h2 id="预备知识">预备知识</h2>
<h3 id="线性代数">线性代数</h3>
<ol type="1">
<li><p>范数（norm）：深度学习的<strong>目标</strong></p>
<ol type="1">
<li><p>向量的范数表示一个向量有多大（指的是分量的大小）</p>
<ul>
<li><p>L1范数：向量元素的绝对值之和。</p>
<p><span class="math display">\[
||x||_ {1}  =  \sum _ {i=1}^ {n}   |x_ {i} |
\]</span></p></li>
<li><p>L2范数：向量元素平方和的平方根。（欧几里得距离L2范数） <span
class="math display">\[
||x||=||x||_ {2}  =  \sqrt {\sum _ {i=1}^ {n}x}
\]</span></p></li>
</ul></li>
<li><p>矩阵X∈R<sup>(mxn)</sup></p>
<ul>
<li><p><em>Frobenius范数</em>（Frobenius
norm）：矩阵元素平方和的平方根（类似于向量的L2范数）</p>
<p><span class="math display">\[
||x||_F=  \sqrt {\sum _ {i=1}^ {m}\sum _ {i=1}^ {n}x_ {i}^ {2}}
\]</span></p></li>
</ul></li>
</ol></li>
<li><p>Hadamard积 v.s. 矩阵乘法</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Hadamard积</th>
<th>矩阵乘法</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>含义</td>
<td>对应元素相乘</td>
<td>左行乘右列</td>
</tr>
<tr class="even">
<td>pytorch符号</td>
<td>A*B</td>
<td>A@B</td>
</tr>
<tr class="odd">
<td>pytorch函数</td>
<td>torch.mul(A, B)</td>
<td>torch.matmul(A, B)或torch.mm(A, B)</td>
</tr>
</tbody>
</table>
<ul>
<li><code>@</code> 符号其实是调用了 <code>torch.matmul()</code>
函数</li>
<li><code>torch.mm()</code>：这个函数用于两个二维矩阵的相乘。它不支持广播（Broadcasting）操作（即对不同形状的张量进行的自动扩充），并且输入必须都是二维张量。比如你有两个形状为
<code>(n, m)</code> 和 <code>(m, p)</code> 的矩阵，使用
<code>torch.mm()</code> 可以得到一个形状为 <code>(n, p)</code>
的矩阵。</li>
<li><code>torch.matmul()</code>：这个函数更加通用，它支持两个张量进行相乘，这两个张量可以是不同的维度，也支持广播操作。对于二维张量，它的行为与
<code>torch.mm()</code>
相同。对于高维张量，它会进行规定的广播操作后再乘。</li>
<li><code>A * B</code>和<code>torch.mul(A, B)</code>的操作效果一致，都支持广播机制。</li>
</ul></li>
<li><p>降维</p>
<p>张量可以通过sum()和mean()函数沿指定的轴降维</p></li>
</ol>
<h3 id="微积分">微积分</h3>
<ol type="1">
<li><p>损失函数：衡量"模型有多糟糕"的分数</p></li>
<li><p>优化：拟合现有的数据</p>
<p>泛化：超出现有的数据，模型在未知数据也表现良好</p></li>
<li><p>梯度：</p>
<ol type="1">
<li>梯度是一个向量</li>
<li>其中的各个元素是函数y对其所有变量的偏导数</li>
</ol></li>
</ol>
<h3 id="自动微分">自动微分</h3>
<ol type="1">
<li>backward()函数：默认只对标量执行反向传播，因此在深度学习最小化一个batch的损失函数时常常要通过<code>.sum()</code>或<code>.mean()</code>将向量转化为标量，然后才能执行反向传播</li>
</ol>
<h3 id="概率">概率</h3>
<ol type="1">
<li><p>机器学习等价于做出预测</p></li>
<li><p>强化学习</p>
<ul>
<li>做错了：得到负反馈</li>
<li>做对了：得到正反馈</li>
</ul></li>
<li><p>可能性</p>
<ul>
<li>离散（discrete）随机变量的可能性叫做概率</li>
<li>连续（continuous）随机变量的可能性叫做<strong>密度（density）</strong></li>
<li>分布（distribution）：随机变量各种取值及其对应的可能性</li>
</ul></li>
<li><p>Bayes定理 <span class="math display">\[
P(A\mid B)=\frac{P(A,B)}{P(B)}=\frac{P(B\mid A)P(A)}{P(B)}.
\]</span></p></li>
<li><p>独立性</p>
<p>如果两个随机变量A和B是独立的，意味着事件A的发生跟B事件的发生无关。即<span
class="math inline">\(A\perp B\)</span>，此时根据Bayes定理可以推出：
<span class="math display">\[
P(A,B)=P(A)P(B)
\]</span></p></li>
</ol>
<h2 id="线性神经网络">线性神经网络</h2>
<h3 id="线性回归">线性回归</h3>
<ol type="1">
<li><p><em>小批量随机梯度下降</em>（minibatch stochastic gradient
descent）</p>
<p>在每次迭代中，我们首先随机抽样一个小批量<span
class="math inline">\(\beta\)</span>， 它是由固定数量的训练样本组成的。
然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。
最后，我们将梯度乘以一个预先确定的正数<span
class="math inline">\(\eta\)</span>，并从当前参数的值中减掉。</p>
<p>之所以沿着负梯度方向更新权重参数，是因为梯度本身表示了函数的局部上升方向，也就是说，如果顺着梯度的方向走，函数值会变大。但在训练神经网络和其他机器学习模型时，我们的目标并不是让损失函数变大，而是让它尽可能小。</p>
<p>我们用下面的数学公式来表示这一更新过程（<span
class="math inline">\(\partial\)</span>表示偏导数）： <span
class="math display">\[
(\mathbf{w},b)\leftarrow(\mathbf{w},b)-\frac\eta{|\mathcal{B}|}\sum_{i\in\mathcal{B}}\partial_{(\mathbf{w},b)}l^{(i)}(\mathbf{w},b).
\]</span></p>
<p>在多元线性回归中，对于平方损失和仿射变换，这一更新过程的形式如下:
<span class="math display">\[
\begin{gathered}
\mathbf{w}\leftarrow\mathbf{w}-\frac\eta{|\mathcal{B}|}\sum_{i\in\mathcal{B}}\partial_\mathbf{w}l^{(i)}(\mathbf{w},b)=\mathbf{w}-\frac\eta{|\mathcal{B}|}\sum_{i\in\mathcal{B}}\mathbf{x}^{(i)}\left(\mathbf{w}^\top\mathbf{x}^{(i)}+b-y^{(i)}\right),
\\
b\leftarrow
b-\frac\eta{|\mathcal{B}|}\sum_{i\in\mathcal{B}}\partial_bl^{(i)}(\mathbf{w},b)=b-\frac\eta{|\mathcal{B}|}\sum_{i\in\mathcal{B}}\left(\mathbf{w}^\top\mathbf{x}^{(i)}+b-y^{(i)}\right).
\end{gathered}
\]</span></p>
<p>在这里，<span
class="math inline">\(\partial_\mathbf{w}l^{(i)}(\mathbf{w},b)\)</span>表示：一个小批量<span
class="math inline">\(\beta\)</span>中的一个样本x<sup>{i}</sup>的损失函数对权重w求偏导；</p>
<p>对于平方损失和仿射变换，这个偏导具体为：<span
class="math inline">\(\mathbf{x}^{(i)}\left(\mathbf{w}^\top\mathbf{x}^{(i)}+b-y^{(i)}\right)\)</span></p></li>
<li><p>训练流程</p>
<ol type="1">
<li><p>读取数据集</p></li>
<li><p>定义模型（Sequential）</p></li>
<li><p>初始化模型参数</p></li>
<li><p>定义损失函数</p></li>
<li><p>定义优化算法</p></li>
<li><p>训练</p>
<p>在每个迭代周期里，我们将完整遍历一次数据集（<code>train_data</code>），
不停地从中获取一个小批量的输入和相应的标签。
对于每一个小批量，我们会进行以下步骤:</p>
<ul>
<li>通过调用<code>net(X)</code>生成预测并计算损失<code>l</code>（前向传播）。</li>
<li>通过进行反向传播来计算梯度。</li>
<li>通过调用优化器来更新模型参数。</li>
</ul></li>
</ol></li>
<li><p>全连接层的参数开销</p>
<p>如果全连接层又d个输入，q个输出，那么共有dq个参数</p></li>
<li><p>全连接层的偏置项的维度为（1,
q），q为该层的输出维度。（习惯写法）</p></li>
</ol>
<h3 id="softmax回归">softmax回归</h3>
<ol type="1">
<li><p>softmax函数： <span class="math display">\[
$\hat{y}=\mathrm{softmax}(\mathbf{o})\quad\text{其中}\quad\hat{y}_j=\frac{\exp(o_j)}{\sum_k\exp(o_k)}
\]</span></p>
<p>softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质。</p></li>
<li><p>softmax回归</p>
<p>与线性回归一样，softmax回归也是一个单层神经网络。由于计算每个输出<span
class="math inline">\(o_1\)</span>、<span
class="math inline">\(o_2\)</span>和<span
class="math inline">\(o_3\)</span>取决于 所有输入<span
class="math inline">\(x_1\)</span>、<span
class="math inline">\(x_2\)</span>、<span
class="math inline">\(x_3\)</span>和<span
class="math inline">\(x_4\)</span>,
所以softmax回归的输出层也是全连接层。计算过程如图：</p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/11/OZVizS.png"
alt="图2 softmax回归是一种单层神经网络" />
<figcaption aria-hidden="true">图2
softmax回归是一种单层神经网络</figcaption>
</figure>
<p>为了提高计算效率并且充分利用GPU，我们通常会对小批量样本的数据执行矢量计算。假设我们读取了一个批量的样本<span
class="math inline">\(\mathbf{X}\)</span>, 其中特征维度 (输入数量)
为<span class="math inline">\(d\)</span>,批量大小为<span
class="math inline">\(n\)</span>。此外，假设我们在输出中有<span
class="math inline">\(q\)</span>个类别。那么小批量样本的特征为<span
class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times d}\)</span>,
权重为<span class="math inline">\(\mathbf{W}\in\mathbb{R}^{d\times
q}\)</span>, 偏置为<span
class="math inline">\(\mathbf{b}\in\mathbb{R}^{1\times q}\)</span>。</p>
<p>softmax回归的矢量计算表达式为： <span class="math display">\[
\begin{gathered}\mathbf{O}=\mathbf{X}\mathbf{W}+\mathbf{b},\\\hat{\mathbf{Y}}=\mathrm{softmax}(\mathbf{O}).\end{gathered}
\]</span></p></li>
</ol>
<h2 id="多层感知机">多层感知机</h2>
<ol type="1">
<li><p>多层感知机</p>
<ol type="1">
<li><p>含义：在网络中加入一个或多个隐藏层来克服线性模型的限制，
使模型具有更强的表达能力</p></li>
<li><p>关键要素：在仿射变换之后对每个隐藏单元应用非线性的激活函数(activation
function) <span
class="math inline">\(\sigma\)</span>。一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型
<span class="math display">\[
\mathbf{H}=\sigma(\mathbf{X}\mathbf{W}^{(1)}+\mathbf{b}^{(1)}),
\]</span></p>
<p><span class="math display">\[
\mathbf{O}=\mathbf{H}\mathbf{W}^{(2)}+\mathbf{b}^{(2)}.
\]</span></p></li>
</ol></li>
<li><p>通用近似定理：通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数</p></li>
<li><p>过拟合</p>
<ul>
<li>训练数据精度 &gt;&gt; 测试数据精度(代表着潜在分布)</li>
<li>过拟合更像死记硬背标签，<strong>泛化则要求模型找到更通用的规律，学会如何判断标签</strong></li>
<li>更可能过拟合？
<ul>
<li>可调整参数越多，模型越容易过拟合</li>
<li>可调整参数的取值范围越大，模型越容易过拟合</li>
<li>训练样本数量越少，模型越容易过拟合</li>
</ul></li>
</ul></li>
<li><p>欠拟合：模型连训练数据都无法很好地拟合，无法继续减少训练误差</p></li>
<li><p>什么样的模型更复杂？——训练迭代周期更长的模型更复杂，需要早停（early
stopping）的模型更简单</p></li>
<li><p>模型选择</p>
<ol type="1">
<li>不同类模型的选择（比如决策树or线性回归模型）</li>
<li>同类模型的选择（比如都是多层感知机，但是隐藏层的数量、同一隐藏层隐藏单元的数量不同）</li>
<li>此时需要用到<strong>验证集</strong>——故数据集划分为训练集、验证集、测试集（严格来说测试集只用一次）</li>
</ol></li>
<li><p>正则化</p>
<p>减少过拟合，通过在训练集的损失函数上加入权重向量W的惩罚项，从而降低学习到的模型的复杂度。</p>
<ul>
<li>法1：权重衰减（weight decay）
<ul>
<li>岭回归：通过L<sub>2</sub>正则化的线性回归（通过惩罚权重向量W中的大分量来均匀分布权重）</li>
<li>套索回归：通过L<sub>1</sub>正则化的线性回归（通过直接去掉一部分权重来实现特征选择，减少特征的数量，从而降低模型复杂度，减少过拟合的风险）</li>
</ul></li>
<li>法2：dropout
<ul>
<li>一般只在<strong>训练集</strong>上使用</li>
<li>应用于每个隐藏层的输出H（在激活函数之后使用）：<span
class="math inline">\(\mathbf{H}=\sigma(\mathbf{X}\mathbf{W}^{(1)}+\mathbf{b}^{(1)}),\)</span></li>
<li>可以各层分别设置dropout概率，靠近输入的地方设置较低的dropout概率</li>
</ul></li>
</ul></li>
<li><p>前向传播 and 反向传播</p>
<p>反向传播计算各可调整参数的梯度时，需要使用<strong>前向传播过程中产生的中间值（即隐藏层的输出值H）</strong>，这也是训练阶段的显存大于测试阶段的原因（因为训练阶段需要存储前向传播的中间值，以备反向传播使用，而测试阶段不需要这样）</p></li>
<li><p>梯度爆炸</p>
<ul>
<li>含义：可调整参数（如权重W）更新过大，<u>破坏了模型的稳定收敛</u></li>
<li>与初始化模型参数有关，不合适的初始化会导致我们没有机会让梯度下降优化器收敛</li>
</ul></li>
<li><p>梯度消失</p>
<ul>
<li>含义：可调整参数（如权重W）更新过小，每次几乎不会移动，<u>导致模型无法学习</u></li>
<li>激活函数选择sigmoid容易导致梯度消失，所以现在都用ReLu</li>
</ul></li>
<li><p>随机初始化可以打破对称性（正态分布、Xavier初始化）</p></li>
<li><p>分布偏移</p>
<ul>
<li><p>含义：训练集和测试集不来自同一个分布</p></li>
<li><p>分类：协变量（特征）偏移、标签偏移</p></li>
<li><p>协变量偏移：</p>
<ul>
<li><p>含义：训练数据和测试数据中特征（即协变量）的分布有所不同，但目标变量在给定特征的条件下的分布是不变的情况。</p></li>
<li><p>```
假设我们正在构建一个预测电邮是否为垃圾邮件（"是垃圾邮件"或"不是垃圾邮件"）的模型，我们使用包含文本内容等特征的电邮收集的历史数据进行模型训练。特定的单词出现的频率（例如"deal",
"free", "win"等）可能被用作特征。</p>
<p>起初，我们的训练数据主要是家庭用户的电邮，我们的垃圾邮件主要是广告和一些诱导性电邮。然后，我们打算在一个公司类的环境中去部署和测试这个模型，这个环境中垃圾邮件主要是包含恶意链接和病毒的电邮。</p>
<p>在这个情况下，特征（电邮内容，或者更具体一点，特定词汇的出现频率）的分布在训练数据和测试数据（家庭用户电邮和公司电邮）之间发生了改变，因为公司电邮中出现恶意链接和病毒电邮的比例会更高，而广告邮件的比例会更低。然而，无论是在家庭环境还是在公司环境，条件概率P(y|x)（给定电邮内容，这封邮件是垃圾邮件的概率）是不变的。
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">- 标签偏移：</span><br><span class="line"></span><br><span class="line">  - 含义：训练数据和测试数据中标签（即y，或者说目标变量）的分布有所不同，但在给定标签的情况下，特征（即x，或者说输入变量）的条件分布保持不变的现象</span><br><span class="line"></span><br><span class="line">  - ```</span><br><span class="line">    假如我们在建立一个预测一个人是否会感冒的模型，模型的特征有&quot;打喷嚏&quot;，&quot;头疼&quot;，&quot;喉咙痛&quot;等症状。我们在冬天收集了一些数据作为训练数据，然后到了夏天我们拿这个模型去预测是否会感冒。</span><br><span class="line">    </span><br><span class="line">    在这个场景中，我们可以看出，冬天和夏天人们感冒的概率（即标签y的边缘分布）是不同的，通常冬天感冒的概率会比夏天高。这就产生了标签偏移。然而，无论是冬天还是夏天，如果一个人感冒了，他出现打喷嚏，头疼，喉咙痛等症状的概率通常是一样的，也就是说在给定感冒的情况下，症状（即特征x）的条件分布是不变的。</span><br></pre></td></tr></table></figure></p></li>
</ul></li>
<li><p>标签偏移和协变量偏移：</p>
<ul>
<li><p>可以同时成立</p></li>
<li><pre><code>  比如，在从冬季到夏季的过程中，不仅感冒的比例发生了变化（标签偏移），与此同时由于气候的变化，导致人打喷嚏的比例也发生了变化（协变量偏移）。</code></pre></li>
</ul></li>
<li><p>测试时可在一定假设下纠正协变量偏移、标签偏移</p></li>
</ul></li>
</ol>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>动手学深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>《动手学深度学习2.0》学习笔记（二）</title>
    <url>/2024/01/12/%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02-0%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"><h1
id="动手学深度学习2.0学习笔记二">《动手学深度学习2.0》学习笔记（二）</h1>
<p>《动手学深度学习2.0》电子书的链接地址为https://zh.d2l.ai/index.html</p>
<p>本文记录了我在学习本书5-7章节（包括深度学习计算、卷积神经网络、现代卷积神经网络）过程中的理解和收获。</p>
<span id="more"></span>
<h2 id="深度学习计算">深度学习计算</h2>
<ol type="1">
<li><p>块（block）</p>
<ul>
<li>一个块可以由许多层组成；一个块可以由许多块组成。</li>
<li>从编程的角度来看，块由<em>类</em>（class）表示。
每个块都必须定义一个将其输入转换为输出的前向传播函数，
并且必须存储任何必需的参数。</li>
</ul></li>
<li><p><code>Sequential</code>类：用于把多个模块顺序地串起来</p></li>
<li><p>参数是复合的对象，包含值、梯度和额外信息。如何访问参数参考https://zh.d2l.ai/chapter_deep-learning-computation/parameters.html</p></li>
<li><p>参数初始化</p></li>
<li><p>加载和保存张量</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">x = torch.arange(4)</span><br><span class="line">torch.save(x,&quot;x-file&quot;)</span><br><span class="line">load_x = torch.load(&quot;x-file&quot;)</span><br></pre></td></tr></table></figure></li>
<li><p>加载和保存模型参数</p>
<blockquote>
<p>这里保存的是模型的参数而不是保存整个模型。
因为模型本身可以包含任意代码，所以模型本身难以序列化。
因此，要想恢复模型，我们需要用代码生成架构， 然后从磁盘加载参数。</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 保存模型参数</span><br><span class="line">torch.save(net.state_dict(), &#x27;mlp.params&#x27;)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 恢复模型</span><br><span class="line">clone = MLP() # 先生成模型的架构</span><br><span class="line">clone.load_state_dict(torch.load(&#x27;mlp.params&#x27;)) #再恢复模型的参数</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="卷积神经网络">卷积神经网络</h2>
<h3 id="从全连接层到卷积">从全连接层到卷积</h3>
<p>空间不变性</p>
<ul>
<li><em>平移不变性</em>（translation
invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。<strong>图像的平移不变性使我们以相同的方式处理局部图像，而不在乎它的位置。</strong></li>
<li><em>局部性</em>（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。<strong>局部性意味着计算相应的隐藏表示只需一小部分局部图像像素。</strong></li>
</ul>
<h3 id="图像卷积">图像卷积</h3>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/12/OZlnPD.png" alt="卷积运算" />
<figcaption aria-hidden="true">卷积运算</figcaption>
</figure>
<ol type="1">
<li><p>卷积相关概念</p>
<ul>
<li><strong>卷积核</strong>（convolution
kernel），又叫滤波器（filter）、该卷积层的权重，作用是：<strong>通过仅查看“输入-输出对”来学习由X生成Y</strong></li>
<li><strong>卷积层</strong>：
<ul>
<li>可以指代应用卷积核的网络层</li>
<li>也可以指代图像经过卷积核计算后的输出（即“<strong>输出的卷积层</strong>”），此时等价于<strong>特征映射（feature
map）</strong>，或<strong>特征图</strong></li>
</ul></li>
<li>卷积层被训练的参数包括：卷积核权重、标量权重</li>
</ul></li>
<li><p>神经网络中的卷积运算实际对应数学上的互相关运算（cross-correlation）</p></li>
<li><p>感受野（receptive field）</p>
<p>以图1为例来解释感受野：给定<span
class="math inline">\(2\times2\)</span>卷积核，阴影输出元素值19的感受野是输入阴影部分的四个元素。假设之前输出为<span
class="math inline">\(\mathbf{Y}\)</span>, 其大小为<span
class="math inline">\(2\times2\)</span>，现在我们在其后附加一个卷积层，该卷积层以<span
class="math inline">\(\mathbf{Y}\)</span>为输入，输出单个元素<span
class="math inline">\(z\)</span>。在这种情况下，<span
class="math inline">\(\mathbf{Y}\)</span>上的<span
class="math inline">\(z\)</span>的感受野包括<span
class="math inline">\(\mathbf{Y}\)</span>的所有四个元素，而输入的感受野包括最初所有九个输入元素。因此，当一个特征图中的任意元素需要检测更广区域的输入特征时，我们可以构建一个更深的网络。</p></li>
<li><p>填充（padding）：旨在保留边界信息</p></li>
<li><p>步幅（stride）：当原始分辨率十分冗余时，加大步幅可以缩减采样次数，加快计算</p></li>
<li><p>怎么确定卷积操作相关的各种值？</p>
<ol type="1">
<li><p>设置卷积核大小（kernel size）——(k<sub>h</sub>,k<sub>w</sub>)</p>
<ul>
<li>通常选奇数1，3，5，7...，目的是：padding通常按照如下规则设置p<sub>h</sub>=k<sub>h</sub>-1，p<sub>w</sub>=k<sub>w</sub>-1，kernel
size选择奇数，p<sub>h</sub>，p<sub>w</sub>就能成为偶数，就能使填充时上下填充同样的行数、左右填充同样的列数，比较对称</li>
</ul></li>
<li><p>设置填充（padding）</p>
<ul>
<li>目的：通常是为了使输入和输出具有相同的高度和宽度，从而更易预测每个图层的输出形状</li>
<li>一般设置：p<sub>h</sub>=k<sub>h</sub>-1，p<sub>w</sub>=k<sub>w</sub>-1，这里p<sub>h</sub>代表上下填充的<strong>总行数</strong>，p<sub>w</sub>代表左右填充的<strong>总列数</strong></li>
<li>在Python编程中，参数padding通常指的是上或下填充的行数（or
左或右填充的列数），也就是说p<sub>h</sub>=2xpadding</li>
</ul></li>
<li><p>设置步长（stride）</p>
<ul>
<li>stride=2，高/宽步长都设置为2，则输入高/宽都减半（输出时）</li>
</ul></li>
<li><p>求解输出形状 <span class="math display">\[
\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor\times\lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor.
\]</span> 如果设置了<span
class="math inline">\(p_h=k_h-1\)</span>和<span
class="math inline">\(p_w=k_w-1\)</span>，则输出形状将简化为 <span
class="math display">\[
\lfloor(n_h+s_h-1)/s_h\rfloor\times\lfloor(n_w+s_w-1)/s_w\rfloor
\]</span></p></li>
</ol></li>
</ol>
<h3 id="多输入多输出通道">多输入多输出通道</h3>
<h4 id="多输入通道">多输入通道</h4>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/12/OZlizI.png"
alt="两个输入通道的互相关计算" />
<figcaption aria-hidden="true">两个输入通道的互相关计算</figcaption>
</figure>
<p>假设输入的通道数为<span
class="math inline">\(c_i\)</span>，那么卷积核的输入通道数也需要为<span
class="math inline">\(c_i\)</span>，因此卷积核的窗口形状是<span
class="math inline">\(c_i\times k_h\times k_w\)</span></p>
<p>当输入通道&gt;1，输出通道=1时，进 行互相关运算包括两个步骤：</p>
<ol type="1">
<li>每个通道输入的二维张量和卷积核的二维张量进行互相关运算</li>
<li>对通道求和（将<span
class="math inline">\(c_i\)</span>的结果相加）得到二维张量</li>
</ol>
<h4 id="多输出通道">多输出通道</h4>
<blockquote>
<p>在最流行的神经网络架构中，随着神经网络层数的加深，我们常会<strong>增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度</strong>。直观地说，我们可以<u><strong>将每个通道看作对不同特征的响应</strong></u>。而现实可能更为复杂一些，因为每个通道不是独立学习的，而是为了共同使用而优化的。因此，多输出通道并不仅是学习多个单通道的检测器。</p>
</blockquote>
<blockquote>
<p>用<span class="math inline">\(c_i\)</span>和<span
class="math inline">\(c_o\)</span>分别表示输入和输出通道的数目，并让<span
class="math inline">\(k_h\)</span>和<span
class="math inline">\(k_w\)</span>为卷积核的高度和宽度。为了获得多个通道的输出，我们可以为每个输出通道创建一个形状为<span
class="math inline">\(c_i\times k_h\times
k_w\)</span>的卷积核张量，这样卷积核的形状<span
class="math inline">\(c_o\times c_i\times k_h\times
k_w\)</span>。在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核
计算出结果。</p>
</blockquote>
<ul>
<li>卷积核的形状是 <span class="math inline">\(c_o\times c_i\times
k_h\times k_w\)</span>，可以理解为有<span
class="math inline">\(c_o\)</span>套卷积核，每个卷积核的维度为<span
class="math inline">\(c_i\times k_h\times
k_w\)</span>，因此每个卷积核的输出为一个二维张量，<span
class="math inline">\(c_o\)</span>套卷积核的输出就为<span
class="math inline">\(c_o\)</span>套二维张量，堆叠起来就是“多个”输出通道<span
class="math inline">\(c_o\)</span></li>
</ul>
<h4 id="x1卷积层">1x1卷积层</h4>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/12/OZlFu1.png"
alt="互相关计算使用了具有3个输入通道和2个输出通道的1×1卷积核。其中，输入和输出具有相同的高度和宽度。" />
<figcaption
aria-hidden="true">互相关计算使用了具有3个输入通道和2个输出通道的1×1卷积核。其中，输入和输出具有相同的高度和宽度。</figcaption>
</figure>
<ol type="1">
<li><p>特点：失去了在高度和宽度维度上，识别相邻元素间相互作用的能力（卷积层的特有能力）</p></li>
<li><p>用途：调整通道数量，用来将<span
class="math inline">\(c_i\)</span>个输入值转换为<span
class="math inline">\(c_o\)</span>个输出值，可看作在每个像素位置应用的全连接层</p></li>
<li><p>1x1卷积层的权重维度为<span class="math inline">\(c_o\times
c_i\)</span>，再额外加上一个偏置。</p>
<p>这里的<span class="math inline">\(c_o\)</span>表示有<span
class="math inline">\(c_o\)</span>套1x1卷积核，<span
class="math inline">\(c_i\)</span>代表卷积核自身的通道数要和输入的通道数相同。（换句话说，这里等价于说卷积核的形状是<span
class="math inline">\(c_o\times c_i\times 1\times 1\)</span>）</p></li>
</ol>
<h3 id="汇聚层pooling">汇聚层pooling</h3>
<ol type="1">
<li>目的：降低卷积层对位置的敏感性，同时降低对空间下采样表示的敏感性。</li>
<li>特点：
<ul>
<li>不包含参数，运算是确定的（maximum or average pooling）</li>
<li>pooling层的输出通道数与输入通道数相同</li>
</ul></li>
<li>使用注意：默认情况下，pooling窗口的大小与步幅相同</li>
</ol>
<h3 id="lenet">LeNet</h3>
<p>LeNet是最早发布的卷积神经网络之一（1989年）</p>
<ol type="1">
<li><p>LeNet（LeNet-5）由两个部分组成：</p>
<ul>
<li><p>卷积编码器：由两个卷积层组成;</p></li>
<li><p>全连接层密集块：由三个全连接层组成。</p></li>
</ul></li>
<li><p>LeNet使用了sigmoid激活函数</p></li>
<li><p>LeNet使用了权重衰减来控制全连接层的模型复杂度</p></li>
</ol>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/12/OZl19F.png"
alt="LeNet架构图" />
<figcaption aria-hidden="true">LeNet架构图</figcaption>
</figure>
<h2 id="现代卷积神经网络">现代卷积神经网络</h2>
<blockquote>
<p>2012年前后，如何表征图像特征的观点发生了进化。2012年前，图像特征都是机械计算出来的，2012年后新的观点涌动起来——<strong>特征本身应该是被学习的</strong>。</p>
<p>现代卷积神经网络：</p>
<ul>
<li>AlexNet。它是第一个在大规模视觉竞赛中击败传统计算机视觉模型的大型神经网络；</li>
<li>使用重复块的网络（VGG）。它利用许多重复的神经网络块；</li>
<li>网络中的网络（NiN）。它重复使用由卷积层和1×1卷积层（用来代替全连接层）来构建深层网络;</li>
<li>含并行连结的网络（GoogLeNet）。它使用并行连结的网络，通过不同窗口大小的卷积层和最大汇聚层来并行抽取信息；</li>
<li>残差网络（ResNet）。它通过残差块构建跨层的数据通道，是计算机视觉中最流行的体系架构；</li>
<li>稠密连接网络（DenseNet）。它的计算成本很高，但给我们带来了更好的效果。</li>
</ul>
</blockquote>
<h3 id="alexnet">AlexNet</h3>
<ol type="1">
<li>AlexNet由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。</li>
<li>AlexNet使用ReLU作为其激活函数。</li>
<li>AlexNet通过dropout控制全连接层的模型复杂度</li>
</ol>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/12/OZlNT6.png"
alt="AlexNet架构图" />
<figcaption aria-hidden="true">AlexNet架构图</figcaption>
</figure>
<h3 id="使用块的网络vgg">使用块的网络VGG</h3>
<ol type="1">
<li>VGG网络可以分为两部分：第一部分主要由卷积层和汇聚层组成，第二部分由全连接层组成。</li>
<li>原始VGG网络有5个卷积块，其中前两个块各有一个卷积层，后三个块各包含两个卷积层。</li>
</ol>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/12/OZlevP.png" alt="VGG架构" />
<figcaption aria-hidden="true">VGG架构</figcaption>
</figure>
<p>LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。
AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。</p>
<h3 id="网络中的网络nin">网络中的网络NiN</h3>
<blockquote>
<p>卷积层的输入和输出由四维张量组成，张量的每个轴分别对应样本、通道、高度和宽度。另外，全连接层的输入和输出通常是分别对应于样本和特征的二维张量。NiN的想法是在每个像素位置
(针对每个高度和宽度)
应用一个全连接层。如果我们将权重连接到每个空间位置，我们可以将其视为<span
class="math inline">\(1\times1\)</span>卷积层 ,
或作为在每个像素位置上独立作用的全连接层。从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征
(feature) 。</p>
</blockquote>
<blockquote>
<p>NiN块以一个普通卷积层开始，后面是两个<span
class="math inline">\(1\times1\)</span>的卷积层。这两个<span
class="math inline">\(1\times1\)</span>卷积层充当带有ReLU激活函数的逐像素全连接层。第一层的卷积窗口形状通常由用户设置。随后的卷积窗口形状固定为<span
class="math inline">\(1\times1\)</span>。</p>
</blockquote>
<blockquote>
<p>NiN和AlexNet之间的一个显著区别是NiN完全取消了全连接层。
相反，NiN使用一个NiN块，其输出通道数等于标签类别的数量。最后放一个<em>全局平均汇聚层</em>（global
average pooling layer），生成一个对数几率
（logits）。NiN设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间。</p>
</blockquote>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/12/OZlrdb.png" alt="NiN的架构" />
<figcaption aria-hidden="true">NiN的架构</figcaption>
</figure>
<h3 id="含并行连结的网络googlenet">含并行连结的网络GoogLeNet</h3>
<h4 id="inception块">Inception块</h4>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/12/OZlKOl.png"
alt="Inception块的架构" />
<figcaption aria-hidden="true">Inception块的架构</figcaption>
</figure>
<blockquote>
<p>如图所示，Inception块由四条并行路径组成。前三条路径使用窗口大小为<span
class="math inline">\(1\times1\times3\times3\)</span>和<span
class="math inline">\(5\times5\)</span>的卷积层，从不同空间大小中提取信息。中间的两条路径在输入上执行<span
class="math inline">\(1\times1\)</span>卷积，以减少通道数，从而降低模型的复杂性。第四条路径使用<span
class="math inline">\(3\times3\)</span>最大汇聚层，然后使用<span
class="math inline">\(1\times1\)</span>卷积层来改变通道数。这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后我们将每条线路的输出在通道维度上连结，并构成Inception块的输出。在Inception块中，通常调整的超参数是每层输出通道数。</p>
</blockquote>
<h4 id="googlenet模型">GoogLeNet模型</h4>
<p>GoogLeNet一共使用9个Inception块和全局平均汇聚层的堆叠来生成其估计值。Inception块之间的最大汇聚层可降低维度。</p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/12/OZlvEB.png"
alt="GoogLeNet架构" />
<figcaption aria-hidden="true">GoogLeNet架构</figcaption>
</figure>
<h3 id="批量规范化">批量规范化</h3>
<h4 id="批量规范化-1">批量规范化</h4>
<p>批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。
接下来，我们应用比例系数和比例偏移。</p>
<p>从形式上来说，用<span
class="math inline">\(\mathbf{x}\in\mathcal{B}\)</span>表示一个来自小批量<span
class="math inline">\(\mathcal{B}\)</span>的输入，批量规范化BN根据以下表达式转换<span
class="math inline">\(\mathbf{x}:\)</span> <span class="math display">\[
\mathrm{BN}(\mathbf{x})=\boldsymbol{\gamma}\odot\frac{\mathbf{x}-\hat{\boldsymbol{\mu}}_{\mathcal{B}}}{\hat{\boldsymbol{\sigma}}_{\mathcal{B}}}+\boldsymbol{\beta}.
\]</span> &gt; <span
class="math inline">\(\hat{\mu}_{B}\)</span>是小批量<span
class="math inline">\(\mathcal{B}\)</span>的样本均值，<span
class="math inline">\(\hat{\sigma}_{B}\)</span>是小批量<span
class="math inline">\(\mathcal{B}\)</span>的样本标准差。应用标准化后，生成的小批量的平均值为0和单位方差为1。由于单位方差
(与其他一些魔法数) 是一个主观的选择，因此我们通常包含拉伸参数(scale)
<span class="math inline">\(\gamma\)</span>和偏移参数(shift) <span
class="math inline">\(\beta\)</span>，它们的形状与x相同。<span
class="math inline">\(\gamma\)</span>和<span
class="math inline">\(\beta\)</span>是需要与其他模型参数一起学习的参数。</p>
<ul>
<li>批量规范化层和dropout层一样，在训练模式和预测模式下计算不同。</li>
</ul>
<blockquote>
<p>批量规范化层在”训练模式“（通过小批量统计数据规范化）和“预测模式”（通过数据集统计规范化）中的功能不同。
在训练过程中，我们无法得知使用整个数据集来估计平均值和方差，所以只能根据每个小批次的平均值和方差不断训练模型。
而在预测模式下，可以根据整个数据集精确计算批量规范化所需的平均值和方差。</p>
</blockquote>
<h4 id="批量规范化层">批量规范化层</h4>
<blockquote>
<p>批量规范化和其他层之间的一个关键区别是，<strong>由于批量规范化在完整的小批量上运行，因此我们不能像以前在引入其他层时那样忽略批量大小</strong>。
我们在下面讨论这两种情况：<strong>全连接层和卷积层，他们的批量规范化实现略有不同</strong>。</p>
</blockquote>
<h5 id="全连接层">全连接层</h5>
<p>将批量规范化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为x，权重参数和偏置参数分别为<span
class="math inline">\(\mathbf{W}\)</span>和b，激活函数为<span
class="math inline">\(\phi\)</span>,批量规范化的运算符为BN。那么，使用批量规范化的全连接层的输出的计算详情如下：
<span class="math display">\[
\mathbf{h}=\phi(\mathrm{BN}(\mathbf{W}\mathbf{x}+\mathbf{b})).
\]</span></p>
<h5 id="卷积层">卷积层</h5>
<p>对于卷积层，在卷积层之后和非线性激活函数之前应用批量规范化。</p>
<p>当卷积有多个输出通道时，我们需要对这些通道的“每个”输出执行批量规范化，每个通道都有自己的拉伸
(scale) 和偏移(shift)
参数，这两个参数都是标量。假设我们的小批量包含<span
class="math inline">\(m\)</span>个样本，并且对于每个通道，卷积的输出具有高度<span
class="math inline">\(p\)</span>和宽度<span
class="math inline">\(q\)</span>。那么对于卷积层，我们在每个输出通道的<span
class="math inline">\(m\cdot p\cdot
q\)</span>个元素上同时执行每个批量规范化。因此，在计算平均值和方差时，我们会收集所有空间位置的值，然后在给定通道内应用相同的均值和方差，以便在每个空间位置对值进行规范化。</p>
<h5 id="预测过程中的批量归一化">预测过程中的批量归一化</h5>
<p>通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。</p>
<h3 id="残差网络resnet">残差网络ResNet</h3>
<h4 id="残差块">残差块</h4>
<p>在残差块中，输入可通过跨层数据线路更快地向前传播。</p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/12/OZlZNg.png"
alt="一个正常块（左图）和一个残差块（右图）" />
<figcaption
aria-hidden="true">一个正常块（左图）和一个残差块（右图）</figcaption>
</figure>
<p>ResNet的残差块里：</p>
<ol type="1">
<li>首先有2个有相同输出通道数的<span
class="math inline">\(3\times3\)</span>卷积层。每个卷积层后接一个批量规范化层和ReLU激活函数。</li>
<li>然后我们通过跨层数据通路，跳过这2个卷积运算，将输入直接加在最后的ReLU激活函数前。这样要求2个卷积层的输出与输入形状一样，从而使它们可以相加。</li>
<li>如果想改变通道数，就需要引入一个额外的<span
class="math inline">\(1\times1\)</span>卷积层来将输入变换成需要的形状后再做相加运算。</li>
</ol>
<p>残差块的实现如下：</p>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/12/OZlyus.png"
alt="包含以及不包含 1×1 卷积层的残差块" />
<figcaption aria-hidden="true">包含以及不包含 1×1
卷积层的残差块</figcaption>
</figure>
<h4 id="resnet模型">ResNet模型</h4>
<ol type="1">
<li>ResNet使用了4个大模块，每个大模块使用了若干相同输出通道数的残差块。</li>
<li>第一个模块的输出通道数同输入通道数一致。
由于之前已经使用了步幅为2的最大汇聚层，所以无须减小高和宽。</li>
<li>之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。</li>
</ol>
<figure>
<img src="https://ooo.0x0.ooo/2024/01/12/OZlmFK.png"
alt="ResNet-18 架构" />
<figcaption aria-hidden="true">ResNet-18 架构</figcaption>
</figure>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>动手学深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>《动手学深度学习2.0》学习笔记（三）</title>
    <url>/2024/01/16/%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02-0%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"><h1
id="动手学深度学习2.0学习笔记三">《动手学深度学习2.0》学习笔记（三）</h1>
<p>《动手学深度学习2.0》电子书的链接地址为https://zh.d2l.ai/index.html</p>
<p>本文记录了我在学习本书8-10章节（包括循环神经网络、现代循环神经网络、注意力机制）过程中的理解和收获。</p>
<span id="more"></span>
<h2 id="循环神经网络">循环神经网络</h2>
<h3 id="引言">引言</h3>
<p>data：表格数据、图像数据、序列数据。</p>
<ul>
<li>对于表格数据和图像数据，我们默认所有样本都是独立同分布的</li>
<li>表格数据：通常利用机器学习、多层感知机处理</li>
<li>图像数据：通常利用卷积神经网络（可以有效利用空间信息）</li>
<li><strong>序列数据：循环神经网络（recurrent neural
network，RNN）</strong></li>
</ul>
<h3 id="处理序列数据的统计工具">处理序列数据的统计工具</h3>
<p>在处理序列数据时，我们面对的问题是——如何有效估计<span
class="math inline">\(P(x_t\mid
x_{t-1},\ldots,x_1)\)</span>，这需要专门的统计工具，两种较流行的选择是自回归模型和隐变量自回归模型。</p>
<h4 id="自回归模型">自回归模型</h4>
<h5 id="自回归模型-1">自回归模型</h5>
<p>第一种策略，假设在现实情况下相当长的序列<span
class="math inline">\(x_{t-1},\ldots,x_1\)</span>可能是不必要的，因此我们只需要满足某个长度为<span
class="math inline">\(\tau\)</span>的时间跨度，即使用观测序列<span
class="math inline">\(x_{t-1},\ldots,x_{t-\tau}\)</span>。当下获得的最直接的好处就是参数的数量总是不变的，至少在<span
class="math inline">\(t&gt;\tau\)</span>时如此，这就使我们能够训练一个上面提及的深度网络。这种模型被称为<strong>自回归模型
(autoregressive models)</strong> , 因为它们是对自己执行回归。</p>
<h5 id="隐变量自回归模型">隐变量自回归模型</h5>
<p>第二种策略，如图所示，是保留一些对过去观测的总结<span
class="math inline">\(h_t\)</span>, 并且同时更新预测<span
class="math inline">\(\hat{x}_{t}\)</span>和总结<span
class="math inline">\(h_{t}\)</span>。由于<span
class="math inline">\(h_t\)</span>从未被观测到，这类模型也被称为<strong>隐变量自回归模型
(latent autoregressive models)</strong> 。 <span class="math display">\[
\hat{x}_t=P(x_t\mid h_t)
\]</span></p>
<p><span class="math display">\[
h_t=g(h_{t-1},x_{t-1})
\]</span></p>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401161645545.png"
alt="隐变量自回归模型" />
<figcaption aria-hidden="true">隐变量自回归模型</figcaption>
</figure>
<p>因此，整个序列的估计值都将通过以下的方式获得：</p>
<p><span class="math display">\[
P(x_1,\ldots,x_T)=\prod_{t=1}^TP(x_t\mid x_{t-1},\ldots,x_1).
\]</span> 注意，如果我们处理的是离散的对象(如单词),
而不是连续的数字，则上述的考虑仍然有效。唯一的差别是，对于离散的对象，我们需要使用分类器而不是回归模型来估计<span
class="math inline">\(P(x_t\mid x_{t-1},\ldots,x_1)\)</span>。</p>
<h4 id="马尔可夫模型">马尔可夫模型</h4>
<p>当序列满足一阶马尔可夫模型（first-order Markov model）时，<span
class="math inline">\(P(x)\)</span>由下式给出： <span
class="math display">\[
P(x_1,\ldots,x_T)=\prod_{t=1}^TP(x_t\mid x_{t-1}){\text{当}}P(x_1\mid
x_0)=P(x_1).
\]</span> 当假设<span
class="math inline">\(x_t\)</span>仅是离散值时，可以推导出以下结论：
<span class="math display">\[
P(x_{t+1}|x_t,x_{t-1})=P(x_{t+1}|x_t)
\]</span> 含义是，在满足了前一个状态 <span
class="math inline">\(x_t\)</span> 的条件下，当前状态 <span
class="math inline">\(x_{t+1}\)</span> 的概率分布与更早的历史状态 <span
class="math inline">\(x_{t-1}\)</span>
无关。这个表达式揭示了马尔可夫性质的一种特殊情况，即当前状态的概率分布仅仅依赖于前一个状态。换句话说，给定了前一个状态
<span class="math inline">\(x_t\)</span> 后，更早的历史状态 <span
class="math inline">\(x_{t-1}\)</span> 对于预测当前状态 <span
class="math inline">\(x_{t+1}\)</span> 的概率分布没有直接的影响。</p>
<h3 id="文本预处理">文本预处理</h3>
<p>文本的常见预处理步骤包括：</p>
<ol type="1">
<li>读取数据集：将文本作为字符串加载到内存中。</li>
<li>词元化：将字符串拆分为词元（如单词和字符）。
<ul>
<li><em>语料</em>（corpus）：将训练集中的所有文档合并在一起，对它们的唯一词元进行统计，
得到的统计结果称之为<em>语料</em>（corpus）</li>
<li>未知词元：“<unk>”，用于映射语料库中不存在或已删除的任何词元</li>
<li>其他保留词元：填充词元（“<pad>”）； 序列开始词元（“<bos>”）；
序列结束词元（“<eos>”）</li>
</ul></li>
<li>建立词表：建立一个词表，将拆分的词元映射到数字索引。</li>
<li>将文本转换为数字索引序列，方便模型操作。</li>
</ol>
<h3 id="构建数据集">构建数据集</h3>
<p>对于语言建模任务，模型的目标就是预测序列中每一个位置的下一个词元。因此对于每个小批量，我们都需要产生一个源序列（X）以及相应的目标序列（Y），目标序列是源序列向右移动了一个位置的序列。下面描述两种构造数据集的策略，分别是<em>随机采样</em>（random
sampling）和 <em>顺序分区</em>（sequential partitioning）</p>
<h4 id="随机采样">随机采样</h4>
<ol type="1">
<li>选择任意偏移量来指示初始位置</li>
<li>在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">seq_data_iter_random</span>(<span class="params">corpus, batch_size, num_steps</span>): </span><br><span class="line">    <span class="comment"># 使用随机抽样生成一个小批量子序列</span></span><br><span class="line">    <span class="comment"># 首先从原始序列中随机选择一个开始点</span></span><br><span class="line">    corpus = corpus[random.randint(<span class="number">0</span>, num_steps - <span class="number">1</span>):]</span><br><span class="line">    <span class="comment"># 计算可以生成的子序列数量。减1的目的是确保有足够的字符留给标签序列</span></span><br><span class="line">    num_subseqs = (<span class="built_in">len</span>(corpus) - <span class="number">1</span>) // num_steps    <span class="comment">#子序列样本数量</span></span><br><span class="line">    <span class="comment"># 生成每个子序列的起始索引</span></span><br><span class="line">    initial_indices = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, num_subseqs * num_steps, num_steps))</span><br><span class="line">    <span class="comment"># 随机排列子序列的起始索引，打乱顺序，</span></span><br><span class="line">    <span class="comment"># 导致每个小批量中的子序列在原始序列上并不一定是相邻的</span></span><br><span class="line">    random.shuffle(initial_indices)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">data</span>(<span class="params">pos</span>):</span><br><span class="line">        <span class="comment"># 返回从pos位置开始的长度为num_steps的序列</span></span><br><span class="line">        <span class="keyword">return</span> corpus[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    num_batches = num_subseqs // batch_size    <span class="comment">#计算小批量的数量</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, batch_size * num_batches, batch_size):</span><br><span class="line">        <span class="comment"># 对所有小批量进行遍历</span></span><br><span class="line">        initial_indices_per_batch = initial_indices[i: i + batch_size]    <span class="comment">#每个小批量中的子序列的起始索引</span></span><br><span class="line">        <span class="comment"># 定义输入X，是根据initial_indices_per_batch中的索引得到的源序列</span></span><br><span class="line">        X = [data(j) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        <span class="comment"># 定义标签Y，是在源序列基础上向后偏移一个单位得到的序列</span></span><br><span class="line">        Y = [data(j + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        <span class="comment"># 每次迭代返回一个小批量的输入X和标签Y</span></span><br><span class="line">        <span class="keyword">yield</span> torch.tensor(X), torch.tensor(Y)</span><br></pre></td></tr></table></figure>
<h4 id="顺序分区">顺序分区</h4>
<ol type="1">
<li>选择任意偏移量来指示初始位置</li>
<li>在获得第一个子序列后，后面的子序列都按照顺序来获取</li>
</ol>
<p>这种方法更加能保证上下文的完整性，从而获得更好的训练效果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">seq_data_iter_sequential</span>(<span class="params">corpus, batch_size, num_steps</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="comment"># 使用顺序分区生成一个小批量子序列</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从随机偏移量开始划分序列</span></span><br><span class="line">    offset = random.randint(<span class="number">0</span>, num_steps)   </span><br><span class="line">    <span class="comment"># 计算总的有效 token 数量，这样的话，计算出的 num_tokens 能被 batch_size 整除</span></span><br><span class="line">    num_tokens = ((<span class="built_in">len</span>(corpus) - offset - <span class="number">1</span>) // batch_size) * batch_size</span><br><span class="line">    <span class="comment"># 剔除不需要的 tokens，然后将 corpus 转变为 tensor</span></span><br><span class="line">    Xs = torch.tensor(corpus[offset: offset + num_tokens])</span><br><span class="line">    <span class="comment"># 类似地，对应的标签也需要进行相同的处理。标签序列是源序列向后移动了一个位置的序列</span></span><br><span class="line">    Ys = torch.tensor(corpus[offset + <span class="number">1</span>: offset + <span class="number">1</span> + num_tokens])</span><br><span class="line">    <span class="comment"># 重塑数据的形状，使得它们可以被划分成一个个小批量</span></span><br><span class="line">    Xs, Ys = Xs.reshape(batch_size, -<span class="number">1</span>), Ys.reshape(batch_size, -<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 计算最后能生成多少个小批量</span></span><br><span class="line">    num_batches = Xs.shape[<span class="number">1</span>] // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_steps * num_batches, num_steps):</span><br><span class="line">        <span class="comment"># 每次选取一个长度为 num_steps 的子序列作为一个小批量</span></span><br><span class="line">        X = Xs[:, i: i + num_steps]</span><br><span class="line">        Y = Ys[:, i: i + num_steps]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 返回一个小批量的数据和标签</span></span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br></pre></td></tr></table></figure>
<h3 id="循环神经网络-1">循环神经网络</h3>
<h4 id="引入">引入</h4>
<ol type="1">
<li>使用隐变量模型来估计<span class="math inline">\(P(x_t\mid
x_{t-1},\ldots,x_1)\)</span>：</li>
</ol>
<p><span class="math display">\[
P(x_t\mid x_{t-1},\ldots,x_1)\approx P(x_t\mid h_{t-1}),
\]</span></p>
<p>其中<span class="math inline">\(h_{t-1}\)</span>是隐状态(hidden
state), 也称为隐藏变量 (hidden variable), 它存储了到时间步<span
class="math inline">\(t-1\)</span>的序列信息。通常，我们可以基于当前输入<span
class="math inline">\(x_t\)</span>和先前隐状态<span
class="math inline">\(h_{t-1}\)</span> 来计算时间步<span
class="math inline">\(t\)</span>处的任何时间的隐状态： <span
class="math display">\[
h_t=f(x_t,h_{t-1}).
\]</span></p>
<ol start="2" type="1">
<li><p><strong>隐藏层</strong>和<strong>隐状态</strong>指的是两个截然不同的概念。隐藏层是在从输入到输出的路径上（以观测角度来理解）的隐藏的层，
而隐状态则是在给定步骤所做的任何事情（以技术角度来定义）的<em>输入</em>，
并且这些状态只能通过先前时间步的数据来计算。</p></li>
<li><p><em>循环神经网络</em>（recurrent neural networks，RNNs）
是具有<strong>隐状态</strong>的神经网络。</p></li>
</ol>
<h4 id="有隐状态的循环神经网络">有隐状态的循环神经网络</h4>
<p>假设在时间步<span class="math inline">\(t\)</span>有小批量输入<span
class="math inline">\(\mathbf{X}_t\in\mathbb{R}^{n\times
d}\)</span>。换言之，对于<span
class="math inline">\(n\)</span>个序列样本的小批量，<span
class="math inline">\(\mathbf{X}_t\)</span>
的每一行对应于来自该序列的时间步<span
class="math inline">\(t\)</span>处的一个样本。接下来，用<span
class="math inline">\(\mathbf{H}_t\in\mathbb{R}^{n\times h}\)</span>
表示时间步<span
class="math inline">\(t\)</span>的隐藏变量。与多层感知机不同的是，我们在这里保存了前一个时间步的隐藏变量<span
class="math inline">\(\mathbf{H}_{t-1}\)</span>,并引入了一个新的权重参数<span
class="math inline">\(\mathbf{W}_{hh}\in\mathbb{R}^{h\times h}\)</span>,
来描述如何在当前时间步中使用前一个时间步的隐藏变量。具体地说，当前时间步隐藏变量由当前时间步的输入与前一个时间步的隐藏变量一起计算得出：</p>
<p><span class="math display">\[
\mathbf{H}_t=\phi(\mathbf{X}_t\mathbf{W}_{xh}+\mathbf{H}_{t-1}\mathbf{W}_{hh}+\mathbf{b}_h).
\]</span> 相比于多层感知机的计算，上式添加了一项 <span
class="math inline">\(\mathbf{H}_{t-1}\mathbf{W}_{hh}\)</span>,
从而实例化了 <span
class="math inline">\(h_t=f(x_t,h_{t-1})\)</span>。从相邻时间步的隐藏变量<span
class="math inline">\(\mathbf{H}_t\)</span>和 <span
class="math inline">\(\mathbf{H}_{t-1}\)</span>之间的关系可知，<strong>这些变量捕获并保留了序列直到其当前时间步的历史信息，就如当前时间步下神经网络的状态或记忆，因此这样的隐藏变量被称为隐状态(hidden
state)</strong>。</p>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401161645795.png"
alt="具有隐状态的循环神经网络" />
<figcaption aria-hidden="true">具有隐状态的循环神经网络</figcaption>
</figure>
<h4
id="基于循环神经网络的字符级语言模型">基于循环神经网络的字符级语言模型</h4>
<p>下图演示了如何通过基于字符级语言建模的循环神经网络，使用当前的和先前的字符预测下一个字符。设小批量大小为1，批量中的文本序列为“machine”。</p>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401161646218.png"
alt="基于循环神经网络的字符级语言模型：输入序列和标签序列分别为“machin”和“achine”" />
<figcaption
aria-hidden="true">基于循环神经网络的字符级语言模型：输入序列和标签序列分别为“machin”和“achine”</figcaption>
</figure>
<p>在训练过程中，我们对每个时间步的输出层的输出进行softmax操作，然后利用交叉熵损失计算模型输出和标签之间的误差。由于隐藏层中隐状态的循环计算，上图中的第3个时间步的输出<span
class="math inline">\(\mathbf{O}_{3}\)</span>
由文本序列"m"a"和"c"确定。由于训练数据中这个文本序列的下一个字符是"h",
因此第3个时间步的损失将取决于下一个字符的概率分布，而下一个字符是基于特征序列"n"a"c"和这个时间步的标签"h"生成的。</p>
<h4 id="困惑度perplexity">困惑度（Perplexity）</h4>
<p>可以理解为"下一个词元的实际选择数的调和平均数"，用于<strong>评价语言模型的质量</strong></p>
<ul>
<li>在最好的情况下，模型总是完美地估计标签词元的概率为1。
在这种情况下，模型的困惑度为1。</li>
<li>在最坏的情况下，模型总是预测标签词元的概率为0。
在这种情况下，困惑度是正无穷大。</li>
<li>在基线上，该模型的预测是词表的所有可用词元上的均匀分布。
在这种情况下，困惑度等于词表中唯一词元的数量。
事实上，如果我们在没有任何压缩的情况下存储序列，
这将是我们能做的最好的编码方式。 因此，这种方式提供了一个重要的上限，
而任何实际模型都必须超越这个上限。</li>
</ul>
<h4 id="从零实现循环神经网络模型">从零实现循环神经网络模型</h4>
<ol type="1">
<li><p>模型包括3个部分：</p>
<ol type="1">
<li><p>输入编码</p>
<p>将每个词元表示为更具表现力的特征向量，最简单的方式为<em>独热编码</em>（one-hot
encoding）</p></li>
<li><p>循环神经网络模型</p>
<p>实现以下公式： <span class="math display">\[
\mathbf{H}_t=\phi(\mathbf{X}_t\mathbf{W}_{xh}+\mathbf{H}_{t-1}\mathbf{W}_{hh}+\mathbf{b}_h).
\]</span></p></li>
<li><p>输出生成</p></li>
</ol></li>
<li><p>训练过程的特点：</p>
<ol type="1">
<li>隐状态初始化：选择随机采样or顺序分区</li>
<li>裁剪梯度：在更新模型参数之前需要裁剪梯度。
目的是：即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。</li>
<li>模型评估：用困惑度来评价模型，确保了不同长度的序列具有可比性。</li>
</ol></li>
</ol>
<h4 id="通过时间反向传播">通过时间反向传播</h4>
<ol type="1">
<li>“通过时间反向传播”仅仅适用于反向传播在具有隐状态的序列模型。</li>
<li>截断是计算方便性和数值稳定性的需要。截断包括：规则截断和随机截断。</li>
<li>矩阵的高次幂可能导致神经网络特征值的发散或消失，将以梯度爆炸或梯度消失的形式表现。</li>
<li>为了计算的效率，“通过时间反向传播”在计算期间会缓存中间值。</li>
</ol>
<h2 id="现代循环神经网络">现代循环神经网络</h2>
<h3 id="门控循环单元gru">门控循环单元(GRU)</h3>
<p>门控循环单元具有以下两个显著特征：</p>
<ul>
<li><strong>重置门有助于捕获序列中的短期依赖关系；</strong></li>
<li><strong>更新门有助于捕获序列中的长期依赖关系。</strong></li>
</ul>
<h4 id="门控隐状态">门控隐状态</h4>
<p>门控循环单元与普通的循环神经网络之间的关键区别在于：
前者支持隐状态的门控。
这意味着模型有专门的机制来确定应该何时更新隐状态，
以及应该何时重置隐状态，且这些机制是可学习的。</p>
<h5 id="重置门和更新门">重置门和更新门</h5>
<p>重置门允许我们控制“可能还想记住”的过去状态的数量；
更新门将允许我们控制新状态中有多少个是旧状态的副本。</p>
<p>下图描述了门控循环单元中的重置门和更新门的输入，
输入是由当前时间步的输入和前一时间步的隐状态给出。
两个门的输出是由使用sigmoid激活函数的两个全连接层给出。</p>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401161646642.png"
alt="在门控循环单元模型中计算重置门和更新门" />
<figcaption
aria-hidden="true">在门控循环单元模型中计算重置门和更新门</figcaption>
</figure>
<p>对于给定的时间步<span
class="math inline">\(t\)</span>，假设输入是一个小批量 <span
class="math inline">\(\mathbf{X}_t\in\mathbb{R}^{n\times
d}\)</span>(样本个数<span class="math inline">\(n\)</span>,输入个数<span
class="math inline">\(d\)</span>)，上一个时间步的隐状态是 <span
class="math inline">\(\mathbf{H}_{t-1}\in\mathbb{R}^{n\times
h}\quad(\)</span>隐藏单元个数<span class="math inline">\(h)\)</span>
。那么，重置门<span
class="math inline">\(\mathbf{R}_t\in\mathbb{R}^{n\times h}\)</span>和
更新门<span class="math inline">\(\mathbf{Z}_t\in\mathbb{R}^{n\times
h}\)</span>的计算如下所示：</p>
<p><span class="math display">\[
\begin{gathered}
\mathbf{R}_{t}
=\sigma(\mathbf{X}_t\mathbf{W}_{xr}+\mathbf{H}_{t-1}\mathbf{W}_{hr}+\mathbf{b}_r),
\\
\mathbf{Z}_{t}
=\sigma(\mathbf{X}_t\mathbf{W}_{xz}+\mathbf{H}_{t-1}\mathbf{W}_{hz}+\mathbf{b}_z),
\end{gathered}
\]</span></p>
<p>其中<span
class="math inline">\(\mathbf{W}_{xr},\mathbf{W}_{xz}\in\mathbb{R}^{d\times
h}\)</span>和<span
class="math inline">\(\mathbf{W}_{hr},\mathbf{W}_{hz}\in\mathbb{R}^{h\times
h}\)</span>是权重参数，<span
class="math inline">\(\mathbf{b}_r,\mathbf{b}_z\in\mathbb{R}^{1\times
h}\)</span>是偏置参数。在求和过程中会触发广播机制；我们使用sigmoid函数将输入值转换到区间(0,1)。</p>
<h5 id="候选隐状态">候选隐状态</h5>
<p>将重置门<span
class="math inline">\(R_t\)</span>与常规隐状态更新机制<span
class="math inline">\(\mathbf{H}_t=\phi(\mathbf{X}_t\mathbf{W}_{xh}+\mathbf{H}_{t-1}\mathbf{W}_{hh}+\mathbf{b}_h)\)</span>集成，得到在时间步<span
class="math inline">\(t\)</span>的<em>候选隐状态（candidate hidden
state）</em><span
class="math inline">\(\mathbf{\tilde{H}}_t\in\mathbb{R}^{n\times
h}\)</span> <span class="math display">\[
\tilde{\mathbf{H}}_t=\tanh(\mathbf{X}_t\mathbf{W}_{xh}+(\mathbf{R}_t\odot\mathbf{H}_{t-1})\mathbf{W}_{hh}+\mathbf{b}_h),
\]</span> 其中<span
class="math inline">\(\mathbf{W}_{xh}\in\mathbb{R}^{d\times
h}\)</span>和<span
class="math inline">\(\mathbf{W}_{hh}\in\mathbb{R}^{h\times
h}\)</span>是权重参数，<span
class="math inline">\(\mathbf{b}_h\in\mathbb{R}^{1\times
h}\)</span>是偏置项，符号©是Hadamard积
(按元素乘积)运算符。在这里，我们使用tanh非线性激活函数来确保候选隐状态中的值保持在区间(-1,1)中。</p>
<ul>
<li>与常规隐状态更新机制<span
class="math inline">\(\mathbf{H}_t=\phi(\mathbf{X}_t\mathbf{W}_{xh}+\mathbf{H}_{t-1}\mathbf{W}_{hh}+\mathbf{b}_h)\)</span>相比，新的候选隐状态公式中<span
class="math inline">\((\mathbf{R}_t\odot\mathbf{H}_{t-1})\)</span>可以减少以往状态的影响。</li>
<li>当重置门<span
class="math inline">\(\mathbf{R}_t\)</span>中接近1时，我们恢复一个如
<span
class="math inline">\(\mathbf{H}_t=\phi(\mathbf{X}_t\mathbf{W}_{xh}+\mathbf{H}_{t-1}\mathbf{W}_{hh}+\mathbf{b}_h)\)</span>的普通的循环神经网络<strong>=》重置门打开时，门控循环单元包含基本循环神经网络。</strong></li>
<li>对于重置门<span
class="math inline">\(\mathbf{R}_t\)</span>中所有接近0的项，候选隐状态是以<span
class="math inline">\(\mathbf{X}_t\)</span>作为输入的多层感知机的结果。因此，任何预先存在的隐状态都会被重置为默认值。</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401161646882.png"
alt="在门控循环单元模型中计算候选隐状态" />
<figcaption
aria-hidden="true">在门控循环单元模型中计算候选隐状态</figcaption>
</figure>
<h5 id="隐状态">隐状态</h5>
<p>上述的计算结果只是候选隐状态，还需要结合更新门<span
class="math inline">\(\mathbf{Z}_t\)</span>，从而确定新的隐状态<span
class="math inline">\(\mathbf{H}_t\in\mathbb{R}^{n\times h}\)</span>
在多大程度上来自旧的状态<span
class="math inline">\(\mathbf{H}_{t-1}\)</span>和 新的候选状态<span
class="math inline">\(\tilde{\mathbf{H}}_{t}\)</span> 。更新门<span
class="math inline">\(\mathbf{Z}_t\)</span>仅需要在 <span
class="math inline">\(\mathbf{H}_{t-1}\)</span>和<span
class="math inline">\(\tilde{\mathbf{H}}_t\)</span>
之间进行按元素的凸组合就可以实现这个目标。这就得出了门控循环单元的最终更新公式：</p>
<p><span class="math display">\[
\mathbf{H}_t=\mathbf{Z}_t\odot\mathbf{H}_{t-1}+(1-\mathbf{Z}_t)\odot\mathbf{\tilde{H}}_t.
\]</span></p>
<ul>
<li>当更新门<span
class="math inline">\({\mathbf{Z}}_t\)</span>接近1时，模型就倾向只保留旧状态。此时，来自<span
class="math inline">\(\mathbf{X}_t\)</span>的信息基本上被忽略，从而有效地跳过了依赖链条中的时间步<span
class="math inline">\(t\)</span>。<strong>更新门打开时，门控循环单元可以跳过子序列。</strong></li>
<li>相反，当<span
class="math inline">\({\mathbf{Z}}_t\)</span>接近0时，新的隐状态<span
class="math inline">\({\mathbf{H}}_t\)</span>就会接近候选隐状态<span
class="math inline">\(\tilde{\mathbf{H}}_{t}\)</span>。</li>
<li>这些设计可以帮助我们处理循环神经网络中的<strong>梯度消失</strong>问题，并更好地<strong>捕获时间步距离很长的序列的依赖关系</strong>。例如，如果整个子序列的所有时间步的更新门都接近于1，则无论序列的长度如何，在序列起始时间步的旧隐状态都将很容易保留并传递到序列结束。</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401161647038.png"
alt="计算门控循环单元模型中的隐状态" />
<figcaption
aria-hidden="true">计算门控循环单元模型中的隐状态</figcaption>
</figure>
<h3 id="长短期记忆网络lstm">长短期记忆网络(LSTM)</h3>
<p>长短期记忆网络的隐藏层输出包括“隐状态”和“记忆元”。<u><strong>只有隐状态会传递到输出层，而记忆元完全属于内部信息。</strong></u></p>
<ol type="1">
<li><strong>记忆元（memory cell
）</strong>：是LSTM的<strong>核心</strong>部分，它的主要功能是<strong>存储过去的信息。</strong>记忆单元内部包含一个称为“细胞状态”的向量，它可以在多个时间步之间保存和传递信息。<strong>记忆单元中的sigmoid激活门控单元（例如输入门，遗忘门和输出门）控制信息的存储、读取和遗忘，使得LSTM网络有能力选择性地记住或遗忘历史信息。</strong></li>
<li><strong>隐状态</strong>：是LSTM的<strong>输出</strong>部分。在每个时间步，LSTM都会根据当前的输入和过去的隐状态来更新其记忆元，并产生新的隐状态。这个隐状态既反映了当前时间步的输出，也存储了过去的信息，用于产生下一个时间步的输出。</li>
</ol>
<p>可以理解为：在LSTM模型中，记忆元主要负责存储长期的信息，而隐状态则主要反映短期的动态信息。</p>
<ul>
<li>记忆元中的信息可以经过多个时间步长久地保留，直到模型认为需要忘记这些信息，这就是所谓的“长期记忆”。</li>
<li>而隐状态则在每个时间步都会有所更新，既包含了一些过去的信息，又包新的输入信息，这就是所谓的“短期记忆”。只有当这种短期记忆与任务目标密切相关时，它才会被保存到长期记忆中。</li>
</ul>
<h4 id="门控记忆元">门控记忆元</h4>
<ol type="1">
<li>记忆元（memory cell 或 cell）是隐状态的一种特殊类型，
它们与隐状态具有相同的形状，其设计目的是用于记录附加的信息。</li>
<li>为了控制记忆元，我们需要许多门。
<ul>
<li><em>输出门</em>（output gate）：用来从<em>输出门</em>（output
gate）中输出条目</li>
<li><em>输入门</em>（input gate）：用来决定何时将数据读入记忆元</li>
<li><em>遗忘门</em>（forget
gate）：用来重置记忆元的内容，决定什么时候记忆或忽略记忆元中的输入</li>
</ul></li>
</ol>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401161647721.png"
alt="长短期记忆模型中的输入门、遗忘门和输出门" />
<figcaption
aria-hidden="true">长短期记忆模型中的输入门、遗忘门和输出门</figcaption>
</figure>
<h5 id="输入门遗忘门和输出门">输入门、遗忘门和输出门</h5>
<p>我们来细化一下长短期记忆网络的数学表达。假设有<span
class="math inline">\(h\)</span>个隐藏单元，批量大小为<span
class="math inline">\(n\)</span>,输入数为<span
class="math inline">\(d\)</span>。因此，输入为<span
class="math inline">\(\mathbf{X}_t\in\mathbb{R}^{n\times d}\)</span>,
前一时间步的隐状态为<span
class="math inline">\(\mathbf{H}_{t-1}\in\mathbb{R}^{n\times
h}\)</span>。相应地，时间步<span
class="math inline">\(t\)</span>的门被定义如下：输入门是<span
class="math inline">\(\mathbf{I}_t\in\mathbb{R}^{n\times h}\)</span>,
遗忘门是<span class="math inline">\(\mathbf{F}_t\in\mathbb{R}^{n\times
h}\)</span>, 输出门是<span
class="math inline">\(\mathbf{O}_t\in\mathbb{R}^{n\times
h}\)</span>。它们的计算方法如下： <span class="math display">\[
\begin{gathered}
I_{t}
=\sigma(\mathbf{X}_t\mathbf{W}_{xi}+\mathbf{H}_{t-1}\mathbf{W}_{hi}+\mathbf{b}_i),
\\
\mathbf{F}_{t}
=\sigma(\mathbf{X}_t\mathbf{W}_{xf}+\mathbf{H}_{t-1}\mathbf{W}_{hf}+\mathbf{b}_f),
\\
\mathbf{0}_{t}
=\sigma(\mathbf{X}_t\mathbf{W}_{xo}+\mathbf{H}_{t-1}\mathbf{W}_{ho}+\mathbf{b}_o),
\end{gathered}
\]</span> 其中<span
class="math inline">\(\mathbf{W}_{xi},\mathbf{W}_{xf},\mathbf{W}_{xo}\in\mathbb{R}^{d\times
h}\)</span>和<span
class="math inline">\(\mathbf{W}_{hi},\mathbf{W}_{hf},\mathbf{W}_{ho}\in\mathbb{R}^{h\times
h}\)</span>是权重参数，<span
class="math inline">\(\mathbf{b}_i,\mathbf{b}_f,\mathbf{b}_o\in\mathbb{R}^{1\times
h}\)</span>是偏置参数。</p>
<h5 id="候选记忆元">候选记忆元</h5>
<p>这里的候选记忆元对应了“短期记忆”的概念。</p>
<p>LSTM模型接收一个新的输入，并结合前一时间步的隐藏状态来更新当前的隐藏状态。这个更新过程就像是对过去信息（即前一时间步的隐藏状态）和新输入信息的一个融合，所以说隐藏状态既包含了一些过去的信息，也包含新的输入信息。这个过程对应了“短期记忆”的概念。</p>
<p>候选记忆元(candidate memory cell) <span
class="math inline">\(\tilde{\mathcal{C}}_t\in\mathbb{R}^{n\times
h}\)</span>使用tanh函数作为激活函数，函数的值范围为(-1,1)。下面导出在时间步<span
class="math inline">\(t\)</span>处的方程：</p>
<p><span class="math display">\[
\tilde{\mathbf{C}}_t=\tanh(\mathbf{X}_t\mathbf{W}_{xc}+\mathbf{H}_{t-1}\mathbf{W}_{hc}+\mathbf{b}_c),
\]</span></p>
<p>其中<span class="math inline">\(\mathbf{W}_{xc}\in\mathbb{R}^{d\times
h}\)</span>和<span
class="math inline">\(\mathbf{W}_{hc}\in\mathbb{R}^{h\times
h}\)</span>是权重参数，<span
class="math inline">\(\mathbf{b}_c\in\mathbb{R}^{1\times
h}\)</span>是偏置参数。</p>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401161648054.png"
alt="长短期记忆模型中的候选记忆元" />
<figcaption aria-hidden="true">长短期记忆模型中的候选记忆元</figcaption>
</figure>
<h5 id="记忆元">记忆元</h5>
<ul>
<li><strong>输入门</strong>决定了新的输入信息在多大程度上被写到记忆单元中。如果输入门的值接近1，意味着新的输入信息会被存储到记忆单元里，成为“长期记忆”。</li>
<li><strong>遗忘门</strong>则对前一时间步的记忆单元进行操作，决定了哪部分历史信息将被遗忘。如果遗忘门的值接近0，那么对应的历史信息就会被遗忘，不再流入下一个时间步的记忆单元。</li>
</ul>
<p>在长短期记忆网络中，利用输入门和输出门来控制输入和遗忘(或跳过)：输入门<span
class="math inline">\(\mathbf{I}_t\)</span>控制采用多少来自<span
class="math inline">\(\tilde{\mathcal{C}}_t\)</span>的新数据，而遗忘门<span
class="math inline">\(\mathbf{F}_t\)</span>控制保留多少过去的记忆元<span
class="math inline">\(\mathbf{C}_{t-1}\in\mathbb{R}^{n\times
h}\)</span>的内容。使用按元素乘法，得出：</p>
<p><span class="math display">\[
\mathbf{C}_t=\mathbf{F}_t\odot\mathbf{C}_{t-1}+\mathbf{I}_t\odot\tilde{\mathbf{C}}_t.
\]</span> 如果遗忘门<span
class="math inline">\(\mathbf{F}_t\)</span>始终为1且输入门<span
class="math inline">\(\mathbf{I}_t\)</span>始终为0，则过去的记忆元<span
class="math inline">\(\mathcal{C}_{t-1}\)</span>将随时间被保存并传递到当前时间步。引入这种设计是为了缓解梯度消失问题，并更好地捕获序列中的长距离依赖关系。</p>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401161648363.png"
alt="在长短期记忆网络模型中计算记忆元" />
<figcaption
aria-hidden="true">在长短期记忆网络模型中计算记忆元</figcaption>
</figure>
<h4 id="隐状态-1">隐状态</h4>
<ul>
<li><strong>输出门决定了多少记忆元的信息会流向隐状态。</strong>具体来说，记忆元中的信息首先会与输出门的值（一个在0到1之间的数）相乘，得到的结果就是流向隐藏状态的信息。
<ul>
<li>如果输出门的值接近1，那么记忆元中的信息就几乎完整地流向隐状态；</li>
<li>如果输出门的值接近0，那么记忆元中的信息就几乎不会流向隐状态。</li>
</ul></li>
</ul>
<blockquote>
<p>这种通过输出门控制信息流向隐状态的机制，使得LSTM模型能够对前面的信息进行选择性的忽略或者强调。例如，在一些情况下，模型可能只需要关注记忆单元中的某一部分信息，那么输出门可以通过关闭（接近0）来阻止其他不相关信息流入隐状态；而在其他情况下，如果记忆单元中的所有信息都对当前任务很重要，那么输出门可以被打开（接近1），这样所有的信息都会流向隐状态。在处理自然语言的任务（如机器翻译、文本生成等）时，通过合理调整输出门的值，可以更好地控制长短语义的流动，从而捕捉到句子中更复杂、更深层次的语义关系。</p>
</blockquote>
<p>因此，隐状态<span
class="math inline">\(\mathbf{H}_t\in\mathbb{R}^{n\times
h}\)</span>计算公式为：</p>
<p><span class="math display">\[
\mathbf{H}_t=\mathbf{O}_t\odot\tanh(\mathbf{C}_t).
\]</span></p>
<p>只要输出门接近1，我们就能够有效地将所有记忆信息传递给预测部分，而对于输出门接近0，我们只保留记忆元内的所有信息，
而不需要更新隐状态。</p>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401161648548.png"
alt="在长短期记忆模型中计算隐状态" />
<figcaption aria-hidden="true">在长短期记忆模型中计算隐状态</figcaption>
</figure>
<h3 id="深度循环神经网络">深度循环神经网络</h3>
<p>（后续需要使用再学习）</p>
<h3 id="双向循环神经网络">双向循环神经网络</h3>
<p>（后续需要使用再学习）</p>
<h3 id="编码器-解码器架构">编码器-解码器架构</h3>
<p>为了处理长度可变的输入和输出序列（如机器翻译），设计了<em>编码器-解码器</em>（encoder-decoder）架构，包含两个主要组件：</p>
<p>（1）编码器（encoder）： 接受一个长度可变的序列作为输入，
并将其转换为具有固定形状的编码状态。</p>
<p>（2）解码器（decoder）：
将固定形状的编码状态映射到长度可变的序列。</p>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401161648190.png"
alt="编码器-解码器架构" />
<figcaption aria-hidden="true">编码器-解码器架构</figcaption>
</figure>
<h3 id="序列到序列学习seq2seg">序列到序列学习(seq2seg)</h3>
<p>（后续需要使用再学习）</p>
<h2 id="注意力机制">注意力机制</h2>
<p>注意力框架=》注意力函数=》仅仅基于注意力机制的Transformer架构</p>
<h3 id="注意力提示">注意力提示</h3>
<h4 id="双组件two-component框架">双组件（two-component）框架</h4>
<p>受试者基于<em>非自主性提示</em>和<em>自主性提示</em>
有选择地引导注意力的焦点。（心理学）</p>
<ul>
<li>非自主性提示：基于环境中物体的突出性和易见性</li>
<li>自主性提示：依赖于认知和意识的控制</li>
</ul>
<h4 id="查询键和值">查询、键和值</h4>
<ul>
<li>在注意力机制的背景下，自主性提示被称为<em>查询</em>（query）</li>
<li>感官输入被称为<em>值</em>（value）</li>
<li>每个值都与一个<em>键</em>（key）配对，
这可以想象为感官输入的非自主提示。</li>
</ul>
<p>给定任何查询，注意力机制通过<em>注意力汇聚</em>（attention pooling）
将选择引导至<em>感官输入</em>（sensory inputs，例如中间特征表示）。</p>
<p>如下图所示，可以通过设计注意力汇聚的方式，
便于给定的查询（自主性提示）与键（非自主性提示）进行匹配，
这将引导得出最匹配的值（感官输入）。</p>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401161649624.png"
alt="注意力机制通过注意力汇聚将查询（自主性提示）和键（非自主性提示）结合在一起，实现对值（感官输入）的选择倾向" />
<figcaption
aria-hidden="true">注意力机制通过注意力汇聚将<em>查询</em>（自主性提示）和<em>键</em>（非自主性提示）结合在一起，实现对<em>值</em>（感官输入）的选择倾向</figcaption>
</figure>
<p>查询（自主提示）和键（非自主提示）之间的交互形成了注意力汇聚；
注意力汇聚有选择地聚合了值（感官输入）以生成最终的输出</p>
<h4 id="可视化注意力">可视化注意力</h4>
<p>注意力机制的可视化一般是通过<u>在源序列和目标序列之间展示一个<strong>热图</strong></u>，该热图的每个单元格<code>[i][j]</code>对应源序列中的第<code>i</code>个元素和目标序列中的第<code>j</code>个词的<strong><u>注意力分数</u></strong>。<u><strong>这个分数表明在生成目标序列的第j个词时模型对源序列中第i个词的关注程度</strong></u>。我们根据注意力分数，画出对应的颜色，数值越高，对应的颜色越明显（或者说越热）。</p>
<h3 id="注意力汇聚">注意力汇聚</h3>
<h4 id="非参数注意力汇聚">非参数注意力汇聚</h4>
<h5 id="nadaraya-watson核回归">Nadaraya-Watson核回归</h5>
<p>Nadaraya和 Watson提出根据输入的位置对输出<span
class="math inline">\(y_i\)</span>进行加权，来实现注意力汇聚。其中，K是核（kernel）。
<span class="math display">\[
f(x)=\sum_{i=1}^n\frac{K(x-x_i)}{\sum_{j=1}^nK(x-x_j)}y_i,
\]</span></p>
<h5
id="通用的注意力汇聚attention-pooling公式">通用的注意力汇聚（attention
pooling）公式</h5>
<p>由上述公式启发，写出更加<strong>通用的注意力汇聚（attention
pooling）公式：</strong> <span class="math display">\[
f(x)=\sum_{i=1}^n\alpha(x,x_i)y_i
\]</span> 其中，<span class="math inline">\(x\)</span>是查询，<span
class="math inline">\((x_i,y_i)\)</span>是键值对，将查询<span
class="math inline">\(x\)</span>和键<span
class="math inline">\(x_i\)</span>之间的关系建模为 注意力权重 (attention
weight) <span
class="math inline">\(\alpha(x,x_i)\)</span>，这个权重将被分配给每一个对应值<span
class="math inline">\(y_{i}\)</span>，因此注意力汇聚<span
class="math inline">\(f(x)\)</span>是<span
class="math inline">\(y_i\)</span>的加权平均。对于任何查询，模型在所有键值的注意力权重都是一个有效的概率分布：它们是非负的，并且总和为1。</p>
<h5 id="nadaraya-watson高斯核回归">Nadaraya-Watson高斯核回归</h5>
<p><strong>为了更好地理解注意力汇聚，考虑一个高斯核(Gaussian
kernel)，其定义为：</strong> <span class="math display">\[
K(u)=\frac1{\sqrt{2\pi}}\exp(-\frac{u^2}2).
\]</span> 将高斯核代上述"Nadaraya-Watson核回归公式"可以得到： <span
class="math display">\[
\begin{aligned}
f(x)&amp; =\sum_{i=1}^n\alpha(x,x_i)y_i  \\
&amp;=\sum_{i=1}^n\frac{\exp\left(-\frac12(x-x_i)^2\right)}{\sum_{j=1}^n\exp\left(-\frac12(x-x_j)^2\right)}y_i
\\
&amp;=\sum_{i=1}^n\text{softmax}\left(-\frac12(x-x_i)^2\right)y_i.
\end{aligned}
\]</span> 分析上式，可以得出：<strong>如果一个键<span
class="math inline">\(x_i\)</span>越是接近给定的查询<span
class="math inline">\(x\)</span>, 那么分配给这个键对应值<span
class="math inline">\(y_i\)</span>的注意力权重就会越大，也就“获得了更多的注意力”。</strong></p>
<blockquote>
<p>原因是：如果键<span
class="math inline">\(x_i\)</span>与查询x越接近，<span
class="math inline">\((x - x_i)^2\)</span>就越小，那么分子<span
class="math inline">\(\exp\left(-\frac12(x-x_i)^2\right)\)</span>的值就越大。虽然分母<span
class="math inline">\(\sum_{i=1}^n\exp\left(-\frac12(x-x_j)^2\right)\)</span>的值也会随着分子的变大而变大。然而，那么这个根据糖水不等式可知，最终该项的分数整体会更大，也就是分配给这个键对应值<span
class="math inline">\(y_i\)</span>的注意力权重就会越大，也就“获得了更多的注意力”。</p>
</blockquote>
<h4 id="带参数的注意力汇聚">带参数的注意力汇聚</h4>
<p>在下面的查询<span class="math inline">\(x\)</span>和键<span
class="math inline">\(x_i\)</span>之间的距离乘以可学习参数<span
class="math inline">\(w\)</span>： <span class="math display">\[
\begin{aligned}
f(x)&amp; =\sum_{i=1}^n\alpha(x,x_i)y_i  \\
&amp;=\sum_{i=1}^n\frac{\exp\left(-\frac12((x-x_i)w)^2\right)}{\sum_{j=1}^n\exp\left(-\frac12((x-x_j)w)^2\right)}y_i
\\
&amp;=\sum_{i=1}^n\text{softmax}\left(-\frac12((x-x_i)w)^2\right)y_i.
\end{aligned}
\]</span></p>
<h5 id="定义模型">定义模型</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入需要的模块</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NWKernelRegression</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 初始化函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 创建一个模型参数w，初始化为随机数，需要求梯度</span></span><br><span class="line">        self.w = nn.Parameter(torch.rand((<span class="number">1</span>,), requires_grad=<span class="literal">True</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 前向传播函数，计算模型的输出</span></span><br><span class="line">    <span class="comment"># 输入参数是一组查询、键和对应的值</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values</span>):</span><br><span class="line">        <span class="comment"># 将queries张量扩展并重组其形状，使其与keys张量的形状匹配，这主要是为了计算每个查询与所有键的差异</span></span><br><span class="line">        queries = queries.repeat_interleave(keys.shape[<span class="number">1</span>]).reshape((-<span class="number">1</span>, keys.shape[<span class="number">1</span>]))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算attention weights，使用softmax函数将权重标准化到0-1范围内，并且保证所有权重之和为1</span></span><br><span class="line">        self.attention_weights = nn.functional.softmax(</span><br><span class="line">            -((queries - keys) * self.w)**<span class="number">2</span> / <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 使用attention weights对值进行加权平均</span></span><br><span class="line">        <span class="comment"># 首先，我们使用unsqueeze函数为values和attention weights增加一个维度</span></span><br><span class="line">        <span class="comment"># 这样，我们可以借助torch.bmm函数，对每一对查询和键-值对进行加权平均</span></span><br><span class="line">        <span class="comment"># 最后，使用reshape函数将结果张量的形状重新调整为与查询相同的形状</span></span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.attention_weights.unsqueeze(<span class="number">1</span>),</span><br><span class="line">                         values.unsqueeze(-<span class="number">1</span>)).reshape(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4
id="对比非参数和带参数的注意力汇聚">对比非参数和带参数的注意力汇聚</h4>
<p><strong>非参数的注意力汇聚</strong>
如果有足够的数据，模型会收敛到最优结果。</p>
<p><strong>带参数的注意力汇聚</strong>在尝试拟合带噪声的训练数据时，
预测结果不如之前非参数模型的平滑。</p>
<h3 id="注意力评分函数">注意力评分函数</h3>
<h4 id="引入-1">引入</h4>
<p>【注意力汇聚】小节<strong>使用了高斯核来对查询和键之间的关系建模</strong>。
<span class="math display">\[
\begin{aligned}
f(x)
&amp;=\sum_{i=1}^n\frac{\exp\left(-\frac12((x-x_i)w)^2\right)}{\sum_{j=1}^n\exp\left(-\frac12((x-x_j)w)^2\right)}y_i
\\
&amp;=\sum_{i=1}^n\text{softmax}\left(-\frac12((x-x_i)w)^2\right)y_i.
\end{aligned}
\]</span></p>
<ul>
<li>其中的高斯核指数部分<span
class="math inline">\(-\frac12((x-x_i)w)^2\)</span>可以视为<em>注意力评分函数</em>（attention
scoring function），简称<em>评分函数</em>（scoring function）</li>
<li>然后，把这个函数的输出结果输入到softmax函数中进行运算，将得到与键对应的值的概率分布（即注意力权重）</li>
<li>最后，注意力汇聚的输出就是基于这些注意力权重的值的加权和</li>
</ul>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401161649078.png"
alt="计算注意力汇聚的输出为值的加权和" />
<figcaption
aria-hidden="true">计算注意力汇聚的输出为值的加权和</figcaption>
</figure>
<h4 id="数学描述">数学描述</h4>
<blockquote>
<p>用数学语言描述，假设有一个查询<span
class="math inline">\(\mathbf{q}\in\mathbb{R}^q\)</span>和 <span
class="math inline">\(m\)</span>个“键-值"对 <span
class="math inline">\((\mathbf{k}_1,\mathbf{v}_1),\ldots,(\mathbf{k}_m,\mathbf{v}_m)\)</span>,
其中<span class="math inline">\(\mathbf{k}_i\in\mathbb{R}^k\)</span>,
<span
class="math inline">\(\mathbf{v}_i\in\mathbb{R}^v\)</span>。注意力汇聚函数<span
class="math inline">\(f\)</span>就被表示成值的加权和：</p>
</blockquote>
<p><span class="math display">\[
f(\mathbf{q},(\mathbf{k}_1,\mathbf{v}_1),\ldots,(\mathbf{k}_m,\mathbf{v}_m))=\sum_{i=1}^m\alpha(\mathbf{q},\mathbf{k}_i)\mathbf{v}_i\in\mathbb{R}^v,
\]</span></p>
<ul>
<li>观察公式，理解为：通过查询<span
class="math inline">\(q\)</span>和键<span
class="math inline">\(k_i\)</span>的匹配度，得出不同的注意力权重<span
class="math inline">\(\alpha\)</span>，从而对与键<span
class="math inline">\(k_i\)</span>对应的值<span
class="math inline">\(v_i\)</span>产生不同的注意力</li>
</ul>
<blockquote>
<p>其中查询<span class="math inline">\(\mathbf{q}\)</span>和键<span
class="math inline">\(\mathbf{k}_i\)</span>的注意力权重(标量)
是通过注意力评分函数<span
class="math inline">\(a\)</span>将两个向量映射成标量，再经过softmax运算得到的：</p>
</blockquote>
<p><span class="math display">\[
\alpha(\mathbf{q},\mathbf{k}_i)=\mathrm{softmax}(a(\mathbf{q},\mathbf{k}_i))=\frac{\exp(a(\mathbf{q},\mathbf{k}_i))}{\sum_{j=1}^m\exp(a(\mathbf{q},\mathbf{k}_j))}\in\mathbb{R}.
\]</span></p>
<p><strong>因此，选择不同的注意力评分函数<span
class="math inline">\(a\)</span>会导致不同的注意力汇聚操作</strong>。<u>下面将介绍两个流行的评分函数，
稍后将用他们来实现更复杂的注意力机制。</u></p>
<h4 id="常用评分函数">常用评分函数</h4>
<h5 id="加性注意力">加性注意力</h5>
<p>一般来说，当查询和键是不同长度的矢量时，可以使用加性注意力作为评分函数。给定查询<span
class="math inline">\(\mathbf{q}\in\mathbb{R}^q\)</span>和 键<span
class="math inline">\(\mathbf{k}\in\mathbb{R}^k\)</span>, 加性注意力
(additive attention) 的评分函数为</p>
<p><span class="math display">\[
a(\mathbf{q},\mathbf{k})=\mathbf{w}_v^\top\mathrm{tanh}(\mathbf{W}_q\mathbf{q}+\mathbf{W}_k\mathbf{k})\in\mathbb{R},
\]</span></p>
<ul>
<li>其中可学习的参数是<span
class="math inline">\(\mathbf{W}_q\in\mathbb{R}^{h\times
q}\)</span>、<span
class="math inline">\(\mathbf{W}_k\in\mathbb{R}^{h\times
k}\)</span>和<span
class="math inline">\(\mathbf{w}_v\in\mathbb{R}^h\)</span>。</li>
<li>公式解释：将查询和键连结起来后输入到一个多层感知机(MLP)
中，感知机包含一个隐藏层，其隐藏单元数是一个超参数<span
class="math inline">\(h\)</span>，使用tanh作为激活函数，并且禁用偏置项。</li>
</ul>
<h5 id="缩放点积注意力">缩放点积注意力</h5>
<p><strong>查询和键具有相同的长度<span
class="math inline">\(d\)</span>（前提条件），</strong>可以使用点积这一计算效率更高的评分函数。</p>
<blockquote>
<p>假设查询和键的所有元素都是独立的随机变量，并且都满足零均值和单位方差，那么两个向量的点积的均值为0，方差为<span
class="math inline">\(d\)</span>
。为确保无论向量长度如何，点积的方差在不考虑向量长度的情况下仍然是1，我们再将点积除以<span
class="math inline">\(\sqrt d\)</span>，则缩放点积注意力 (scaled
dot-product attention) 评分函数为：</p>
</blockquote>
<p><span class="math display">\[
a(\mathbf{q},\mathbf{k})=\mathbf{q}^\top\mathbf{k}/\sqrt d.
\]</span></p>
<blockquote>
<p>在实践中，我们通常从小批量的角度来考虑提高效率，例如基于<span
class="math inline">\(n\)</span>个查询和<span
class="math inline">\(m\)</span>个键一值对计算注意力，其中查询和键的长度为<span
class="math inline">\(d\)</span>,值的长度为<span
class="math inline">\(v\)</span>。查询<span
class="math inline">\(\mathbf{Q}\in\mathbb{R}^{n\times
d}\)</span>、键<span
class="math inline">\(\mathbf{K}\in\mathbb{R}^{m\times d}\)</span>和
值<span class="math inline">\(\mathbf{V}\in\mathbb{R}^{m\times
v}\)</span>的缩放点积注意力是：</p>
</blockquote>
<p><span class="math display">\[
\operatorname{softmax}\left(\frac{\mathbf{QK}^\top}{\sqrt{d}}\right)\mathbf{V}\in\mathbb{R}^{n\times
v}.
\]</span></p>
<h3 id="bahdanau-注意力">Bahdanau 注意力</h3>
<h4 id="引入-2">引入</h4>
<p>在机器翻译问题中：通过设计一个基于两个循环神经网络的编码器-解码器架构，
用于序列到序列学习。</p>
<p>具体来说，循环神经网络<u>编码器</u>将长度可变的序列转换为<strong><u>固定形状的上下文变量</u></strong>，
然后循环神经网络<u>解码器</u>根据<u>生成的词元和上下文变量</u>
按词元生成输出（目标）序列词元。然而，并非所有输入（源）词元都对解码某个词元有用，
那么<u><strong>在每个解码步骤中使用编码不同的上下文变量</strong></u>是否会更合理呢？</p>
<p>在这里，我们可以将<strong>上下文变量</strong>视为注意力集中的输出。
<u>在预测词元时，如果不是所有输入词元都相关，模型将仅对齐输入序列中与当前预测相关的部分。</u></p>
<h4 id="bahdanau-注意力模型">Bahdanau 注意力模型</h4>
<blockquote>
<p>原来的机器翻译问题中，解码器的隐藏层的变换为： <span
class="math display">\[
\mathbf{s}_{t^{\prime}}=g(y_{t^{\prime}-1},\mathbf{c},\mathbf{s}_{t^{\prime}-1}).
\]</span> 在输出序列上的任意时间步<span
class="math inline">\(t^{\prime}\)</span>,
循环神经网络将来自上一时间步的输出<span
class="math inline">\(y_{t^{\prime}-1}\)</span> 和上下文变量<span
class="math inline">\(c\)</span>作为其输入，然后在当前时间步将它们和上一隐状态
<span class="math inline">\(\mathfrak{s}_{t^{\prime}-1}\)</span>转换为
隐状态<span class="math inline">\(s_{t^{\prime}}\)</span>。</p>
</blockquote>
<p>在理解上述<strong>【引入】</strong>部分的思路后，假设输入序列中有<span
class="math inline">\(T\)</span>个词元，解码时间步<span
class="math inline">\(t^{\prime}\)</span>的上下文变量是注意力集中的输出：
<span class="math display">\[
\mathbf{c}_{t^{\prime}}=\sum_{t=1}^T\alpha(\mathbf{s}_{t^{\prime}-1},\mathbf{h}_t)\mathbf{h}_t,
\]</span> 其中，时间步<span
class="math inline">\(t^{\prime}-1\)</span>时的解码器隐状态<span
class="math inline">\(\mathfrak{s}_{t^{\prime}-1}\)</span>是查询，编码器隐状态<span
class="math inline">\(\mathfrak{h}_t\)</span>既是键，也是值，注意力权重<span
class="math inline">\(\alpha\)</span>是使用加性注意力打分函数计算的。</p>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401161649438.png"
alt="一个带有Bahdanau注意力的循环神经网络编码器-解码器模型" />
<figcaption
aria-hidden="true">一个带有Bahdanau注意力的循环神经网络编码器-解码器模型</figcaption>
</figure>
<h4 id="定义注意力解码器">定义注意力解码器</h4>
<p>定义Bahdanau注意力，实现循环神经网络编码器-解码器。</p>
<ol type="1">
<li>首先，初始化解码器的状态，需要下面的输入：
<ul>
<li><strong>用</strong><u>编码器在所有时间步的最终层隐状态</u><strong>作为</strong><u>注意力的键和值</u></li>
<li><strong>用</strong><u>上一时间步的编码器全层隐状态</u><strong>作为</strong><u>初始化解码器的隐状态</u></li>
<li>编码器有效长度（排除在注意力池中填充词元）</li>
</ul></li>
<li>其次，进行查询
<ul>
<li>在每个解码时间步骤中，解码器上一个时间步的最终层隐状态将用作查询。</li>
</ul></li>
</ol>
<h3 id="多头注意力">多头注意力</h3>
<h4 id="引入-3">引入</h4>
<blockquote>
<p>在实践中，当给定相同的查询、键和值的集合时，我们希望模型可以基于相同的注意力机制学习到不同的行为，然后将不同的行为作为知识组合起来，捕获序列内各种范围的依赖关系
(例如，短距离依赖和长距离依赖关系)。因此，允许注意力机制组合使用查询、键和值的不同
子空间表示 (representation subspaces) 可能是有益的。</p>
</blockquote>
<blockquote>
<p>为此，与其只使用单独一个注意力汇聚，我们可以用独立学习得到的<span
class="math inline">\(h\)</span>组不同的 线性投影 (linear projections)
来变换查询、键和值。然后，这<span
class="math inline">\(h\)</span>组变换后的查询、键和值将并行地送到注意力汇聚中。最后，将这<span
class="math inline">\(h\)</span>个注意力汇聚的输出拼接在一起，并且通过另一个可以学习的线性投影进行变换，以产生最终输出。</p>
</blockquote>
<blockquote>
<p>这种设计被称为多头注意力 (multihead attention) 。对于<span
class="math inline">\(h\)</span>个注意力汇聚输出，
每一个注意力汇聚都被称作一个头(head)。多头注意力融合了来自于多个注意力汇聚的不同知识，这些知识的不同来源于相同的查询、键和值的不同的子空间表示。</p>
</blockquote>
<blockquote>
<p>下图展示了使用全连接层来实现可学习的线性变换的多头注意力。</p>
</blockquote>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401161649564.png"
alt="多头注意力：多个头连结然后线性变换" />
<figcaption
aria-hidden="true">多头注意力：多个头连结然后线性变换</figcaption>
</figure>
<h4 id="数学描述-1">数学描述</h4>
<p>用数学语言描述上述多头注意力模型：</p>
<blockquote>
<p>给定查询<span
class="math inline">\(\mathbf{q}\in\mathbb{R}^{d_q}\)</span>、键<span
class="math inline">\(\mathbf{k}\in\mathbb{R}^{d_k}\)</span>和 值<span
class="math inline">\(\mathbf{v}\in\mathbb{R}^{d_v}\)</span>,
每个注意力头<span
class="math inline">\(\mathbf{h}_i\quad(i=1,\ldots,h)\)</span>
的计算方法为：</p>
</blockquote>
<p><span class="math display">\[
\mathbf{h}_i=f(\mathbf{W}_i^{(q)}\mathbf{q},\mathbf{W}_i^{(k)}\mathbf{k},\mathbf{W}_i^{(v)}\mathbf{v})\in\mathbb{R}^{p_v},
\]</span></p>
<blockquote>
<p>其中，可学习的参数包括<span
class="math inline">\(\mathbf{W}_i^{(q)}\in\mathbb{R}^{p_q\times
d_q}\text{、}\mathbf{W}_i^{(k)}\in\mathbb{R}^{p_k\times d_k}\text{和
}\mathbf{W}_i^{(v)}\in\mathbb{R}^{p_v\times
d_v}\)</span>，以及代表注意力汇聚的函数<span
class="math inline">\(f\)</span>。<u><span
class="math inline">\(f\)</span>可以是加性注意力和缩放点积注意力。</u></p>
</blockquote>
<blockquote>
<p>多头注意力的输出需要经过另一个线性转换，它对应着<span
class="math inline">\(h\)</span>个头连结后的结果，因此其可学习参数是
<span class="math inline">\(\mathbf{W}_o\in\mathbb{R}^{p_o\times
hp_v}:\)</span></p>
</blockquote>
<p><span class="math display">\[
\mathbf{W}_o\begin{bmatrix}\mathbf{h}_1\\\vdots\\\mathbf{h}_h\end{bmatrix}\in\mathbb{R}^{p_o}.
\]</span>
基于这种设计，<strong>每个头都可能会关注输入的不同部分</strong>，从而可以表示比简单加权平均值更复杂的函数。</p>
<h3 id="自注意力和位置编码">自注意力和位置编码</h3>
<h4 id="引入-4">引入</h4>
<p>在深度学习中，经常使用卷积神经网络(CNN)或循环神经网络(RNN)对序列进行编码。<u><strong>有了注意力机制之后，我们将词元序列输入注意力池化中，以便同一组词元同时充当查询、键和值。</strong></u><u>具体来说，每个查询都会关注所有的键一值对并生成一个注意力输出</u>。<strong><u>由于查询、键和值来自同一组输入，因此被称为
自注意力 (self-attention)</u></strong> ，也被称为内部注意力
(intra-attention)。</p>
<h4 id="自注意力">自注意力</h4>
<p>给定一个由词元组成的输入序列<span
class="math inline">\(\mathbf{x}_1,\ldots,\mathbf{x}_n\)</span>,
其中任意<span
class="math inline">\(\mathbf{x}_i\in\mathbb{R}^d\quad(1\leq i\leq
n)\)</span> 。该序列的自注意力输出为一个长度相同的序列 <span
class="math inline">\(\mathbf{y}_1,\ldots,\mathbf{y}_n\)</span>,其中：</p>
<p><span class="math display">\[
\mathbf{y}_i=f(\mathbf{x}_i,(\mathbf{x}_1,\mathbf{x}_1),\ldots,(\mathbf{x}_n,\mathbf{x}_n))\in\mathbb{R}^d
\]</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">attention = d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,</span><br><span class="line">                                   num_hiddens, num_heads, <span class="number">0.5</span>)</span><br><span class="line">attention(X, X, X, valid_lens).shape</span><br></pre></td></tr></table></figure>
<p>MultiHeadAttention模型在接收输入时，传入的查询（query）、键（key）和值（value）都是相同的张量X。这意味着模型在计算注意力分数时，<strong><u>输入序列的各个元素同时扮演了查询、键和值的角色</u></strong>，也就是自己对自己进行注意力计算，因此被称为"自注意力"。</p>
<h4
id="比较卷积神经网络循环神经网络和自注意力">比较卷积神经网络、循环神经网络和自注意力</h4>
<p>比较卷积神经网络、循环神经网络和自注意力这3种架构的计算复杂性、顺序操作和最大路径长度。</p>
<ol type="1">
<li><strong>计算复杂性：</strong>
指进行运算所需要的计算量。我们通常希望计算复杂性越低越好，因为这意味着需要的计算资源更少，训练和预测的速度也更快。
<ul>
<li>卷积神经网络：计算复杂性为<span
class="math inline">\(O(knd^2)\)</span>，其中k是卷积核的大小，n是序列长度，d是输入和输出的通道数。换言之，卷积神经网络所需要的计算量与卷积核的大小、序列长度以及输入和输出的通道数有关。</li>
<li>循环神经网络：计算复杂性为<span
class="math inline">\(O(nd^2)\)</span>，其中n是序列长度，d是隐藏状态的维度。因此，循环神经网络所需要的计算量与序列长度和隐藏状态的维度有关。</li>
<li>自注意力机制：计算复杂性为<span
class="math inline">\(O(n^2d)\)</span>，其中n是序列长度，d是输入输出的维度。对于自注意力机制，计算量与序列长度的平方和输入输出的维度有关。</li>
</ul></li>
<li><strong>顺序操作：</strong>
顺序操作是指必须按照特定顺序进行的操作。顺序操作的数量越多，越难以进行并行计算，因此我们通常希望顺序操作越少越好。
<ul>
<li><strong>卷积神经网络和自注意力机制的顺序操作数量都为$ O
(1)$，也就是说，几乎不存在顺序操作，计算可以高度并行化。</strong></li>
<li>循环神经网络的顺序操作数量为<span class="math inline">\(O
(n)\)</span>，也就是说，它需要按照序列数据（长度为n）的顺序，一个接一个地处理每个元素——先处理第一个元素，然后处理第二个元素，然后处理第三个元素，依此类推。也就是说，无法同时处理所有的序列元素，因为每个步骤都依赖于前一个步骤的输出，因此无法进行完全的并行计算。这就导致了RNN的主要缺点——在处理长序列时可能会很慢，因为必须等待所有的顺序操作都完成才能得到最终的输出。</li>
</ul></li>
<li><strong>最大路径长度：</strong>
最大路径长度是指在网络中，从一个节点到另一个节点经过的最大路径长度。<strong>路径越短，网络学习序列中远距离依赖关系的能力越好。</strong>
<ul>
<li>卷积神经网络的最大路径长度为 <span class="math inline">\(O
(n/k)\)</span>，n是序列长度，k是卷积核的大小。</li>
<li>循环神经网络的最大路径长度为 <span class="math inline">\(O
(n)\)</span>，n是序列长度。</li>
<li><strong>自注意力机制的最大路径长度为 <span class="math inline">\(O
(1)\)</span>，也就是说，任何两个节点之间的距离都为 1。</strong></li>
</ul></li>
</ol>
<p>总结一下，<strong>卷积神经网络和自注意力机制可以进行高度的并行计算，而自注意力机制的最大路径长度最短。然而，自注意力机制的计算复杂性与序列长度的平方成正比，因此在处理长序列时可能会很慢。</strong></p>
<h4 id="位置编码">位置编码</h4>
<h5 id="引入-5">引入</h5>
<p>在处理词元序列时，循环神经网络是逐个的重复地处理词元的，而自注意力则因为并行计算而放弃了顺序操作。<strong>为了使用序列的顺序信息，通过在输入表示中添加
位置编码 (positional encoding)
来注入绝对的或相对的位置信息。</strong>位置编码可以<u>通过学习得到</u>也可以<u>直接固定得到</u>。接下来介绍基于正弦函数和余弦函数的固定位置编码</p>
<h5
id="基于正弦函数和余弦函数的固定位置编码">基于正弦函数和余弦函数的固定位置编码</h5>
<blockquote>
<p>假设输入表示<span
class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times
d}\)</span>包含一个序列中<span
class="math inline">\(n\)</span>个词元的<span
class="math inline">\(d\)</span>维嵌入表示。位置编码使用相同形状的位置嵌入矩阵
<span class="math inline">\(\mathbf{P}\in\mathbb{R}^{n\times
d}\)</span>输出<span
class="math inline">\(\mathbf{X}+\mathbf{P}\)</span>, 矩阵第<span
class="math inline">\(i\)</span>行、第<span
class="math inline">\(2j\)</span>列和<span
class="math inline">\(2j+1\)</span>列上的元素为：</p>
</blockquote>
<p><span class="math display">\[
\begin{gathered}
p_{i,2j} =\sin\left(\frac i{10000^{2j/d}}\right), \\
p_{i,2j+1} =\cos\left(\frac i{10000^{2j/d}}\right).
\end{gathered}
\]</span>
这里利用了正弦函数和余弦函数的连续性和周期性，来为每个位置生成一个固定的编码。具体来说，对于每一个位置i（也就是序列中的第i个词元）：</p>
<ul>
<li>我们为它的每一个维度（指该词元对应的词嵌入向量的每一个元素）生成一个位置编码。</li>
<li>如果这个维度是偶数（<span
class="math inline">\(2j\)</span>），那么使用正弦函数生成位置编码</li>
<li>如果这个维度是奇数（<span
class="math inline">\(2j+1\)</span>），那么使用余弦函数生成位置编码。</li>
<li>函数的输入是位置i和维度j，通过这种方法可以为每个位置生成一个唯一的编码。</li>
</ul>
<p>之所以使用正弦函数和余弦函数主要归功于它们的周期性和连续性。周期性是指函数的值会在每个完整的周期内重复。连续性是指在实数范围内，函数的图像是无间断的。</p>
<p><strong>当用于位置编码时，正弦和余弦函数可以非常好地表达出序列中单词的相对位置关系</strong>。</p>
<ul>
<li>相邻的单词位置的差距较小，对应的正弦或余弦值的变化也相近（连续性）</li>
<li>相反，序列中距离较远的单词对应的输入数值差距较大，他们的正弦和余弦值的变化也会相对较大（实际中并不直接使用单一的正弦或余弦函数来表达位置信息，而是使用了一系列不同波长的正弦和余弦函数，使得任意两个位置的编码都是唯一的，即使是在很远的序列位置，也能得到有区别性的编码。）</li>
</ul>
<p>正弦和余弦函数的这些特性<strong>让编码的变化能够反映出单词在序列中位置的改变，这种改变可能是相邻位置的微小变动，也可能是较长距离的较大变动</strong>。这就使得模型可以<strong>捕捉到语言序列中单词之间的相对位置关系</strong>，这对于理解和生成语言非常重要。</p>
<h3 id="transformer">Transformer</h3>
<p>自注意力同时具有<u>并行计算</u>和<u>最短的最大路径长度</u>这两个优势，因此，使用自注意力来设计深度架构很有吸引力。对比之前仍然依赖循环神经网络实现输入表示的自注意力模型，<strong>Transformer模型完全基于注意力机制，没有任何卷积层或循环神经网络层。</strong></p>
<h4 id="模型">模型</h4>
<blockquote>
<p>Transformer作为编码器一解码器架构的一个实例，其整体架构图如下图所示。与基于Bahdanau注意力实现的序列到序列的学习相比，Transformer的编码器和解码器是基于自注意力的模块叠加而成的，源(输入)
序列和目标(输出)序列的嵌入(embedding) 表示将加上位置编码 (positional
encoding)，再分别输入到编码器和解码器中。</p>
</blockquote>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401161650780.png"
alt="transformer架构" />
<figcaption aria-hidden="true">transformer架构</figcaption>
</figure>
<ul>
<li><p>Transformer的<strong>编码器</strong>是由多个相同的层叠加而成的，每个层都有两个子层
(子层表示为sublayer)。</p>
<ul>
<li><p>子层一：多头自注意力 (multi-head self-attention) 汇聚层。</p>
<p>该层将输入进行了分组处理，每一组分别通过自注意力机制处理后，多个结果又会合并为一组新的编码输出。这个子层主要负责提取输入中的全局依赖关系。</p></li>
<li><p>子层二：位置前馈网络 (positionwise feed-forward network)层。</p>
<p>该层会对每个输入位置对应的编码进行独立的处理，频繁地学习到输入的非线性表示。</p></li>
<li><p>残差连接 (residual
connection)：这种机制引入了“短路”机制，使得神经网络的每层的输入都能直接影响每层的输出，并传播到更深的层次。这种机制有效地缓解了梯度消失/梯度爆炸问题。</p></li>
<li><p>层规范化 (layer normalization):
这个操作将得到的残差结果进行了规范化处理，以调整它们的值范围，从而提升模型训练的稳定性。</p></li>
</ul></li>
<li><p>Transformer解码器也是由多个相同的层叠加而成的，并且层中使用了残差连接和层规范化。</p>
<ul>
<li>除了编码器中描述的两个子层之外，<strong>解码器还在这两个子层之间插入了第三个子层</strong>（通常被隐式地包含在解码器中的“自注意力（Self-Attention）”），称为
<strong>编码器一解码器注意力 (encoder-decoder attention)
层。在<u>编码器—解码器注意力中，查询来自前一个解码器层的输出，而键和值来自整个编码器的输出。</u></strong></li>
<li>解码器的自注意力层使用了一个掩蔽机制，以防止预测位置看到后面的位置，这就保证了解码的自回归性质，使得当前输出仅依赖于产生的前面的输出。<strong><u>在解码器自注意力中，查询、键和值都来自上一个解码器层的输出</u></strong>。</li>
</ul></li>
</ul>
<h4 id="位置前馈网络">位置前馈网络</h4>
<p>这里的"前馈网络"其实只是一个特别普通的多层感知机，而"位置前馈网络"特殊就特殊在这个"位置"，<strong>这个"位置"是指这个"前馈网络"会被独立地应用于输入序列的每一个元素</strong>。也就是说，对于输入序列中的每一个位置，我们都会独立地应用同一个前馈网络。从而实现——将元素对应的原有的词向量<strong>变换为</strong>多层感知机（MLP）处理后的新的词向量。</p>
<p>这一变换的意义是：</p>
<ul>
<li>首先，<u>原有的词向量通常是词的静态表示</u>，在特定上下文中，一个词的含义可能会有所变化。<strong><u>通过这种变换，我们可以得到词在特定上下文中的动态表示</u></strong>，这对于理解和生成上下文相关的句子非常有帮助。</li>
<li>另外，这种变换也是<strong>增加模型复杂性</strong>的一种方式，可以使模型具有更强的表达能力。多层感知机（MLP）可以理解为对输入数据的一种非线性变换，通过多个这样的非线性变换，我们可以得到输入数据的一个更抽象、更复杂的表示。就好像我们看一个物体，可以从不同的角度（即语境）获得关于这个物体的不同信息，这种变化使我们对物体的理解更深入、更全面。</li>
<li><strong>这是一种一对一的映射关系，为每个位置提供了一个密集的变换</strong>。在这一点上，它类似于卷积层，因为卷积层也会对输入数据的每一个位置分别进行映射，不过它的映射函数是依赖于邻域的，而前馈网络则更加简单，它的映射函数仅依赖于那一个位置的数据。</li>
</ul>
<h4 id="残差连接和层规范化">残差连接和层规范化</h4>
<h5 id="残差连接">残差连接</h5>
<blockquote>
<p>残差连接（Residual
Connection）也被称为跳过连接，是一种让前面层的信息能够直接流向后面层的方法。在Transformer中，每一层的输入都会与该层的输出进行元素级别的相加操作，这种操作就构成了一种残差连接。残差连接的主要作用是缓解梯度消失问题，使得深层网络的训练变得更加稳定。当网络深度较大时，前向传播和反向传播都可能会遇到数值不稳定性问题，残差连接可以允许梯度直接反向传播到任何一层。</p>
</blockquote>
<h5 id="层规范化">层规范化</h5>
<blockquote>
<p>层规范化（Layer
Normalization）是一种对神经网络中的隐藏层进行规范化的方法，并且层规范在一个样本内部进行，这意味着对于同一层的每个神经元来说，其规范化的统计量（均值、方差）是相同的。主要有两个作用：一是加速模型收敛速度，稳定模型训练；二是具有一定的正则化效果，避免模型过拟合。在Transformer中，将层规范化应用于残差连接后的结果，有助于网络更好地学习和适应数据集的特性。</p>
</blockquote>
<h4 id="编码器">编码器</h4>
<p>编码器<code>EncoderBlock</code>类包含两个子层：多头自注意力和基于位置的前馈网络，这两个子层都使用了残差连接和紧随的层规范化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer编码器块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span><br><span class="line"><span class="params">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span><br><span class="line"><span class="params">                 dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderBlock, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 多头自注意力</span></span><br><span class="line">        self.attention = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout,</span><br><span class="line">            use_bias)</span><br><span class="line">        <span class="comment"># 残差连接后进行层规范化</span></span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        <span class="comment"># 位置前馈网络</span></span><br><span class="line">        self.ffn = PositionWiseFFN(</span><br><span class="line">            ffn_num_input, ffn_num_hiddens, num_hiddens)</span><br><span class="line">        <span class="comment"># 残差连接后进行层规范化</span></span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, valid_lens</span>):</span><br><span class="line">        <span class="comment"># 多头自注意力 + 残差连接后进行层规范化</span></span><br><span class="line">        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))</span><br><span class="line">        <span class="comment"># 位置前馈网络 + 残差连接后进行层规范化</span></span><br><span class="line">        <span class="keyword">return</span> self.addnorm2(Y, self.ffn(Y))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AddNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;残差连接后进行层规范化&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, normalized_shape, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AddNorm, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.ln = nn.LayerNorm(normalized_shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, Y</span>):</span><br><span class="line">        <span class="keyword">return</span> self.ln(self.dropout(Y) + X)</span><br></pre></td></tr></table></figure>
<h4 id="解码器">解码器</h4>
<p>在解码器<code>DecoderBlock</code>类中实现的每个层包含了三个子层：解码器自注意力、“编码器-解码器”注意力和基于位置的前馈网络。这些子层也都被残差连接和紧随的层规范化围绕。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;解码器中第i个块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span><br><span class="line"><span class="params">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span><br><span class="line"><span class="params">                 dropout, i, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.i = i</span><br><span class="line">        <span class="comment"># 多头自注意力</span></span><br><span class="line">        self.attention1 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        <span class="comment"># 残差连接后进行层规范化</span></span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        <span class="comment"># 编码器－解码器注意力</span></span><br><span class="line">        self.attention2 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        <span class="comment"># 残差连接后进行层规范化</span></span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line">        <span class="comment"># 位置前馈网络</span></span><br><span class="line">        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                                   num_hiddens)</span><br><span class="line">        <span class="comment"># 残差连接后进行层规范化</span></span><br><span class="line">        self.addnorm3 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        enc_outputs, enc_valid_lens = state[<span class="number">0</span>], state[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 训练阶段，输出序列的所有词元都在同一时间处理，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]初始化为None。</span></span><br><span class="line">        <span class="comment"># 预测阶段，输出序列是通过词元一个接着一个解码的，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示</span></span><br><span class="line">        <span class="keyword">if</span> state[<span class="number">2</span>][self.i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key_values = X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            key_values = torch.cat((state[<span class="number">2</span>][self.i], X), axis=<span class="number">1</span>)</span><br><span class="line">        state[<span class="number">2</span>][self.i] = key_values</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            batch_size, num_steps, _ = X.shape</span><br><span class="line">            <span class="comment"># dec_valid_lens的开头:(batch_size,num_steps),</span></span><br><span class="line">            <span class="comment"># 其中每一行是[1,2,...,num_steps]</span></span><br><span class="line">            dec_valid_lens = torch.arange(</span><br><span class="line">                <span class="number">1</span>, num_steps + <span class="number">1</span>, device=X.device).repeat(batch_size, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dec_valid_lens = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 多头自注意力 + 残差连接后进行层规范化</span></span><br><span class="line">        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)</span><br><span class="line">        Y = self.addnorm1(X, X2)</span><br><span class="line">        <span class="comment"># 编码器－解码器注意力 + 残差连接后进行层规范化</span></span><br><span class="line">        <span class="comment"># enc_outputs的开头:(batch_size,num_steps,num_hiddens)</span></span><br><span class="line">        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">        Z = self.addnorm2(Y, Y2)</span><br><span class="line">        <span class="comment"># 位置前馈网络 + 残差连接后进行层规范化</span></span><br><span class="line">        <span class="keyword">return</span> self.addnorm3(Z, self.ffn(Z)), state</span><br></pre></td></tr></table></figure>
<h4 id="小结">小结</h4>
<ul>
<li>Transformer是编码器－解码器架构的一个实践，尽管在实际情况中编码器或解码器可以单独使用。</li>
<li>在Transformer中，多头自注意力用于表示输入序列和输出序列，不过解码器必须通过掩蔽机制来保留自回归属性。</li>
<li>Transformer中的残差连接和层规范化是训练非常深度模型的重要工具。</li>
<li>Transformer模型中基于位置的前馈网络使用同一个多层感知机，作用是对所有序列位置的表示进行转换。</li>
</ul>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>动手学深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>《动手学深度学习2.0》学习笔记（四）</title>
    <url>/2024/01/17/%E3%80%8A%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A02-0%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css"><h1
id="动手学深度学习2.0学习笔记四">《动手学深度学习2.0》学习笔记（四）</h1>
<p>《动手学深度学习2.0》电子书的链接地址为https://zh.d2l.ai/index.html</p>
<p>本文记录了我在学习本书第13章节（计算机视觉）过程中的理解和收获。</p>
<span id="more"></span>
<h1 id="计算机视觉">计算机视觉</h1>
<p>计算机视觉任务包括<em>图像分类</em>（Image
classification）、<em>目标检测</em>（object
detection）、<em>语义分割</em>（semantic
segmentation）和<em>样式迁移</em>（style transfer）等。</p>
<h2 id="改进泛化的方法">改进泛化的方法</h2>
<p>改进模型泛化的方法包括<strong>图像增广</strong>和<strong>微调</strong></p>
<h3 id="图像增广">图像增广</h3>
<p><strong>为什么要进行图像增广？</strong></p>
<ul>
<li><strong>扩大训练集的规模：</strong>大型数据集是成功应用深度神经网络的先决条件。
图像增广在对训练图像进行一系列的随机变化之后，生成相似但不同的训练样本，从而扩大了训练集的规模。</li>
<li><strong>提高模型的泛化能力</strong>：随机改变训练样本可以减少模型对某些属性的依赖，从而提高模型的泛化能力。</li>
</ul>
<p><strong>如何进行图像增广？</strong></p>
<ol type="1">
<li>翻转和裁剪</li>
<li>改变颜色</li>
<li>混合应用1和2</li>
</ol>
<h3 id="微调">微调</h3>
<p>迁移学习（transfer
learning）指<u>将从<em>源数据集</em>学到的知识迁移到<em>目标数据集</em></u>，其中的常见技巧：<strong>微调（fine-tuning）</strong>。</p>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401172133866.png"
alt="微调" />
<figcaption aria-hidden="true">微调</figcaption>
</figure>
<p><strong>应用条件：</strong>当<u><strong>目标数据集比源数据集小得多</strong></u>时，微调有助于提高模型的泛化能力。</p>
<p><strong>实现步骤：</strong></p>
<ol type="1">
<li>在源数据集（例如ImageNet数据集）上<u>预训练</u>神经网络模型，即<strong><em>源模型</em></strong>。</li>
<li>创建一个新的神经网络模型，即<strong><em>目标模型</em></strong>。<strong><u>它将复制源模型上的所有模型设计及其参数（输出层除外）</u></strong>。
<ul>
<li>假设1：这些模型参数包含从源数据集中学到的<u>知识</u>，这些知识也将适用于目标数据集。</li>
<li>假设2：源模型的输出层与源数据集的<u>标签</u>密切相关，因此不在目标模型中使用该层。</li>
</ul></li>
<li><u>向目标模型添加输出层</u>，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。</li>
<li><u>在目标数据集（如椅子数据集）上训练目标模型</u>。
<ul>
<li><u>输出层</u>将<strong>从头</strong>开始进行训练</li>
<li><u>所有其他层</u>的参数将根据源模型的参数进行<strong>微调</strong></li>
</ul></li>
</ol>
<p><strong>特点：</strong></p>
<ol type="1">
<li>除输出层外，目标模型从源模型中复制所有模型设计及其参数，并根据目标数据集对这些参数进行微调。但是，<strong><u>目标模型的输出层需要从头开始训练</u></strong>。</li>
<li>通常，<strong><u>微调参数使用较小的学习率，而从头开始训练输出层可以使用更大的学习率</u></strong>。</li>
</ol>
<h2 id="目标检测和边界框">目标检测和边界框</h2>
<h3 id="目标检测">目标检测</h3>
<p><strong>图像分类和目标检测的区别</strong></p>
<ol type="1">
<li>在<strong>图像分类</strong>任务中：我们假设图像中只有<strong>一个主要物体对象</strong>，我们只关注如何识别其<strong>类别</strong>。</li>
<li>在<strong>目标检测</strong>任务中：图像里有<strong>多个我们感兴趣的目标</strong>，我们不仅想知道它们的<strong>类别</strong>，还想得到它们在图像中的<strong>具体位置</strong>。</li>
</ol>
<h3 id="边界框">边界框</h3>
<blockquote>
<p>在目标检测中，我们通常使用边界框 (bounding box)
来描述对象的空间位置。边界框是矩形的。表示方式有2种：</p>
<ol type="1">
<li><p>由矩形左上角的以及右下角的<span
class="math inline">\(x\)</span>和<span
class="math inline">\(y\)</span>坐标</p></li>
<li><p>边界框中心的<span
class="math inline">\((x,y)\)</span>轴坐标以及框的宽度和高度</p></li>
</ol>
</blockquote>
<h2 id="锚框">锚框</h2>
<blockquote>
<p>目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边界从而更准确地预测目标的<em>真实边界框</em>（ground-truth
bounding box）。 不同的模型使用的区域采样方法可能不同。</p>
</blockquote>
<p>这里介绍其中一种方法：以每个像素为中心，生成多个缩放比和宽高比（aspect
ratio）不同的边界框。 这些边界框被称为<em>锚框</em>（anchor box）。</p>
<p><strong>！！！这里介绍的锚框是在原始的输入图像上生成的，然而到了后面的【多尺度目标检测】，我们的锚框就是在特征图上生成的。！！！</strong></p>
<h3 id="生成多个锚框">生成多个锚框</h3>
<p>假设<strong>输入图像</strong>的高度为<span
class="math inline">\(h\)</span>, 宽度为<span
class="math inline">\(w\)</span>。我们以图像的每个像素为中心生成不同形状的锚框：缩放比为<span
class="math inline">\(s\in(0,1]\)</span>，宽高比为<span
class="math inline">\(r&gt;0\)</span>。那么锚框的宽度和高度分别是<span
class="math inline">\(ws\sqrt{r}\)</span>和<span
class="math inline">\(hs/\sqrt{r}\)</span>。</p>
<blockquote>
<p>缩放比 (Scale)
在这里是指生成锚框的大小与原始图像大小的比例。例如，如果图像的高度为<span
class="math inline">\(h\)</span>, 缩放比为<span
class="math inline">\(s\)</span>,那么生成的锚框的高度就是<span
class="math inline">\(hs\)</span>, 在这里<span
class="math inline">\(s\in(0,1]\)</span>。这样可以确保生成的锚框的大小都在图像的范围内。</p>
</blockquote>
<blockquote>
<p>锚框的宽度和高度是怎么计算的？</p>
<ol type="1">
<li><p>尺度（即宽度和高度）有预设的缩放比s（<span
class="math inline">\(s=锚框高度/图像高度h\)</span> or <span
class="math inline">\(s=锚框宽度/图像宽度w\)</span>） =》
因此锚框高度变为hs，宽度变为ws</p></li>
<li><p>宽高比<span class="math inline">\(r\)</span>
定义为宽度和高度的比值<span
class="math inline">\(r=\frac{width}{height}\)</span>​，要想按照特定的宽高比来调整锚框的尺寸，需要分别对原始宽度和高度进行适当的调整。</p>
<p>为何宽度需要乘以 <span class="math inline">\(\sqrt{r}\)</span>
呢？</p>
<ul>
<li>因为如果按照宽高比 <span class="math inline">\(r\)</span>
(宽度/高度)
直接调整锚框可能会导致宽度或高度的变化过大或过小，而影响到目标检测的精度。</li>
<li>通过使用<span class="math inline">\(\sqrt{r}\)</span>和<span
class="math inline">\(1/\sqrt{r}\)</span>来分别调整宽度和高度，我们可以更平稳地改变宽度和高度，同时保持宽高比为<span
class="math inline">\(r\)</span> 。</li>
</ul></li>
<li><p>因此，锚框的宽度和高度分别是<span
class="math inline">\(ws\sqrt{r}\)</span>和<span
class="math inline">\(hs/\sqrt{r}\)</span>。</p></li>
</ol>
</blockquote>
<p>要生成多个不同形状的锚框，让我们设置许多缩放比 (scale) 取值<span
class="math inline">\(s_1,\ldots,s_n\)</span>和许多宽高比 (aspect ratio)
取值 <span
class="math inline">\(r_1,\ldots,r_m\)</span>。当使用这些比例和长宽比的所有组合以每个像素为中心时，输入图像将总共有<span
class="math inline">\(whnm\)</span>个锚框。</p>
<p>尽管这些锚框可能会覆盖所有真实边界框，但计算复杂性很容易过高。在实践中，我们只考虑包含<span
class="math inline">\(s_{1}\)</span>或<span
class="math inline">\(r_{1}\)</span>的组合： <span
class="math display">\[
(s_1,r_1),(s_1,r_2),\ldots,(s_1,r_m),(s_2,r_1),(s_3,r_1),\ldots,(s_n,r_1).
\]</span> 也就是说，以同一像素为中心的锚框的数量是<span
class="math inline">\(n+m-1\)</span>。对于整个输入图像，将共生成<span
class="math inline">\(wh(n+m-1)\)</span>个锚框。</p>
<h3 id="交并比">交并比</h3>
<h3 id="在训练数据中标注锚框">在训练数据中标注锚框</h3>
<blockquote>
<ul>
<li>在训练集中，我们将每个锚框视为一个训练样本。
为了训练目标检测模型，我们需要每个锚框的<em>类别</em>（class）和<em>偏移量</em>（offset）标签，其中前者是与锚框相关的对象的类别，后者是真实边界框相对于锚框的偏移量。</li>
<li>在预测时，我们为每个图像生成多个锚框，预测所有锚框的类别和偏移量，根据预测的偏移量调整它们的位置以获得预测的边界框，最后只输出符合特定条件的预测边界框。</li>
</ul>
</blockquote>
<p>现有条件：目标检测训练集有<u>真实边界框的位置</u>及其包围<u>物体类别</u>的标签。</p>
<p>目的：为生成的每个锚框标注其类别（class）和偏移量（offset），其中前者是与锚框相关的对象的类别，后者是真实边界框相对于锚框的偏移量。</p>
<p>思路：为锚框分配最接近它的真实边界框的位置和类别标签</p>
<p><strong>（1）将真实边界框分配给锚框</strong></p>
<p>给定图像，假设锚框是<span
class="math inline">\(A_1,A_2,\ldots,A_{n_a}\)</span>, 真实边界框是<span
class="math inline">\(B_1,B_2,\ldots,B_{n_b}\)</span>,其中<span
class="math inline">\(n_a\geq n_b\)</span>。让我们定义一个矩阵<span
class="math inline">\(\mathbf{X}\in\mathbb{R}^{n_a\times
n_b}\)</span>,其中第<span class="math inline">\(i\)</span>行、第<span
class="math inline">\(j\)</span>列的元素<span
class="math inline">\(x_{ij}\)</span>是锚框<span
class="math inline">\(A_i\)</span>和真实边界框<span
class="math inline">\(B_j\)</span>的loU。该算法包含以下步骤。</p>
<ol type="1">
<li><p>在矩阵<span
class="math inline">\(\mathbf{X}\)</span>中找到最大的元素，并将它的行索引和列索引分别表示为<span
class="math inline">\(i_{1}\)</span>和<span
class="math inline">\(j_{1}\)</span>。然后将真实边界框<span
class="math inline">\(B_{j_1}\)</span>分配给锚框<span
class="math inline">\(A_{i_1}\)</span>。这很直观，因为<span
class="math inline">\(A_{i_1}\)</span>和<span
class="math inline">\(B_{j_1}\)</span>是所有锚框和真实边界框配对中最相近的。在第一个分配完成后，丢弃矩阵中<span
class="math inline">\(i_{1}^\mathrm{th}\)</span>行和<span
class="math inline">\(j_{1}^\mathrm{th}\)</span>列中的所有元素<span
class="math inline">\(_{\mathrm{o}}\)</span></p></li>
<li><p>在矩阵<span
class="math inline">\(\mathbf{X}\)</span>中找到剩余元素中最大的元素，并将它的行索引和列索引分别表示为<span
class="math inline">\(i_{2}\)</span>和<span
class="math inline">\(j_{2}\)</span>。我们将真实边界框<span
class="math inline">\(B_{j_2}\)</span>分配给锚框<span
class="math inline">\(A_{i_2}\)</span>,并丢弃矩阵中<span
class="math inline">\(i_2^\mathrm{th}\)</span>行和<span
class="math inline">\(j_2^\mathrm{th}\)</span>列中的所有元素。</p></li>
<li><p>此时，矩阵<span
class="math inline">\(\mathbf{X}\)</span>中两行和两列中的元素已被丢弃。我们继续，直到丢弃掉矩阵<span
class="math inline">\(\mathbf{X}\)</span>中<span
class="math inline">\(n_b\)</span>列中的所有元素。此时已经为这<span
class="math inline">\(n_b\)</span>个锚框各自分配了一个真实边界框。</p></li>
<li><p>只遍历剩下的<span
class="math inline">\(n_a-n_b\)</span>个锚框。例如，给定任何锚框<span
class="math inline">\(A_i\)</span>, 在矩阵<span
class="math inline">\(\mathbf{X}\)</span>的第<span
class="math inline">\(i^\mathrm{th}\)</span>行中找到与<span
class="math inline">\(A_i\)</span>的IoU 最大的真实边界框<span
class="math inline">\(B_j\)</span>,只有当此loU大于预定义的阈值时，才将<span
class="math inline">\(B_j\)</span>分配给<span
class="math inline">\(A_{i}\)</span></p></li>
</ol>
<blockquote>
<p>下面举例说明上述算法。如下图(左)所示，假设矩阵<span
class="math inline">\(\mathbf{X}\)</span>中的最大值为<span
class="math inline">\(x_{23}\)</span>,我们将真实边界框<span
class="math inline">\(B_{3}\)</span>分配给锚框<span
class="math inline">\(A_{2}\)</span>。
然后，我们丢弃矩阵第2行和第3列中的所有元素，在剩余元素(阴影区域)
中找到最大的<span
class="math inline">\(x_{71}\)</span>,然后将真实边界框<span
class="math inline">\(B_{1}\)</span>分配给锚框<span
class="math inline">\(A_{7}\)</span>。接下来，如下图中)
所示，丢弃矩阵第7行和第1列中的所有元素，在剩余元素 (阴影区域)
中找到最大的<span
class="math inline">\(x_{54}\)</span>,然后将真实边界框<span
class="math inline">\(B_{4}\)</span>分配给锚框<span
class="math inline">\(A_{5}\)</span>。
最后，如下图(右)所示，丢弃矩阵第5行和第4列中的所有元素，在剩余元素
(阴影区域) 中找到最大的<span
class="math inline">\(x_{92}\)</span>,然后将真实边界框<span
class="math inline">\(B_{2}\)</span>分配给锚框<span
class="math inline">\(A_{9}\)</span>。之后，我们只需要遍历剩余的锚框<span
class="math inline">\(A_1,A_3,A_4,A_6,A_8\)</span>,然后根据阈值确定是否为它们分配真实边界框。</p>
</blockquote>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401172133758.png"
alt="将真实边界框分配给锚框" />
<figcaption aria-hidden="true">将真实边界框分配给锚框</figcaption>
</figure>
<p><strong>（2）标记类别和偏移量</strong></p>
<p>现在我们可以为每个锚框标记类别和偏移量了。假设一个锚框<span
class="math inline">\(A\)</span>被分配了一个真实边界框<span
class="math inline">\(B\)</span>。</p>
<ul>
<li>锚框<span class="math inline">\(A\)</span>的类别将被标记为与<span
class="math inline">\(B\)</span>相同</li>
<li>锚框<span class="math inline">\(A\)</span>的偏移量将根据<span
class="math inline">\(B\)</span>和<span
class="math inline">\(A\)</span>中心坐标的相对位置以及这两个框的相对大小进行标记</li>
</ul>
<p>鉴于数据集内不同的框的位置和大小不同，我们可以<strong><u>对这些相对位置和大小应用变换，使其获得分布更均匀且易于拟合的偏移量。</u></strong></p>
<p>这里介绍一种常见的变换。</p>
<blockquote>
<p>给定框<span class="math inline">\(A\)</span>和<span
class="math inline">\(B\)</span>,中心坐标分别为<span
class="math inline">\((x_a,y_a)\)</span>和<span
class="math inline">\((x_b,y_b)\)</span>,宽度分别为<span
class="math inline">\(w_a\)</span>和<span
class="math inline">\(w_b\)</span>, 高度分别为<span
class="math inline">\(h_a\)</span>和<span
class="math inline">\(h_b\)</span>, 可以将<span
class="math inline">\(A\)</span>的偏移量标记为：</p>
</blockquote>
<p><span class="math display">\[
\left(\frac{\frac{x_b-x_a}{w_a}-\mu_x}{\sigma_x},\frac{\frac{y_b-y_a}{h_a}-\mu_y}{\sigma_y},\frac{\log\frac{w_b}{w_a}-\mu_w}{\sigma_w},\frac{\log\frac{h_b}{h_a}-\mu_h}{\sigma_h}\right),
\]</span></p>
<blockquote>
<p>其中常量的默认值为 <span
class="math inline">\(\mu_x=\mu_y=\mu_w=\mu_h=0,\sigma_x=\sigma_y=0.1,\quad\sigma_w=\sigma_h=0.2\)</span>。</p>
</blockquote>
<p>如果一个锚框没有被分配真实边界框，我们只需将锚框的类别标记为背景
(background)。背景类别的锚框通常被称为负类锚框，其余的被称为正类锚框。</p>
<h3 id="使用非极大值抑制预测边界框">使用非极大值抑制预测边界框</h3>
<blockquote>
<p>当有许多锚框时，可能会围绕着同一目标、输出许多相似的具有明显重叠的预测边界框。为了简化输出，我们可以使用非极大值抑制
(non-maximum suppression, NMS)
<strong>合并</strong>属于<strong>同一目标</strong>的类似的<strong>预测</strong>边界框。</p>
</blockquote>
<p><strong>工作原理：</strong></p>
<p>对于一个预测边界框<span class="math inline">\(B\)</span>,
目标检测模型会计算每个类别的预测概率。假设最大的预测概率为<span
class="math inline">\(p\)</span>, 则该概率所对应的类别<span
class="math inline">\(B\)</span>即为预测的类别。具体来说，我们将<span
class="math inline">\(p\)</span>称为预测边界框<span
class="math inline">\(B\)</span>的置信度(confidence)。<u>在同一张图像中，所有预测的非背景边界框都按置信度降序排序，以生成列表<span
class="math inline">\(L\)</span>。然后我们通过以下步骤操作排序列表<span
class="math inline">\(L\)</span>。</u></p>
<ol type="1">
<li><p>从<span
class="math inline">\(L\)</span>中选取置信度最高的预测边界框<span
class="math inline">\(B_{1}\)</span>作为基准，然后将所有与<span
class="math inline">\(B_{1}\)</span>的loU超过预定阈值<span
class="math inline">\(\epsilon\)</span>的非基准预测边界框从<span
class="math inline">\(L\)</span>中移除。这时，<span
class="math inline">\(L\)</span>保留了置信度最高的预测边界框，去除了与其太过相似的其他预测边界框。简而言之，那些具有
非极大值置信度的边界框被抑制了。</p></li>
<li><p>从<span
class="math inline">\(L\)</span>中选取置信度第二高的预测边界框<span
class="math inline">\(B_{2}\)</span>作为又一个基准，然后将所有与<span
class="math inline">\(B_{2}\)</span>的loU大于<span
class="math inline">\(\epsilon\)</span>的非基准预测边界框从<span
class="math inline">\(L\)</span>中移除。</p></li>
<li><p>重复上述过程，直到<span
class="math inline">\(L\)</span>中的所有预测边界框都曾被用作基准。此时，<span
class="math inline">\(L\)</span>中任意一对预测边界框的loU都小于阈值<span
class="math inline">\(\epsilon\)</span>，因此，没有一对边界框过于相似。</p></li>
<li><p>输出列表<span
class="math inline">\(L\)</span>中的所有预测边界框。</p></li>
</ol>
<h2 id="多尺度目标检测">多尺度目标检测</h2>
<p>注意这里的尺度有两种不同的含义</p>
<ol type="1">
<li>对于每个中心点要生成的锚框来说，它有不同的缩放比（scales），<strong>不同的缩放比意味着不同尺度的锚框</strong>。</li>
<li>对于不同层次的特征图来说，特征图上的一个空间位置在输入图像上分别拥有大小不同的感受野，即时对于相同缩放比的锚框来说，它在不同层的特征图上也具有不同大小的感受野，可以用于检测不同大小的目标。因此我们可以认为<strong>特征图也是多尺度的</strong>。</li>
</ol>
<h3 id="多尺度锚框">多尺度锚框</h3>
<p><strong>（1）思路</strong></p>
<p>在上一节【锚框】中，我们以输入图像的每个像素为中心，生成了多个锚框，这些锚框代表了图像不同区域的样本。
然而，如果为每个像素都生成的锚框，我们最终可能会得到太多需要计算的锚框。
因此接下来的思路就是要减少图像上的锚框数量=》我们可以在输入图像中均匀采样一小部分像素，并以它们为中心生成锚框.</p>
<p><strong>（2）具体操作</strong></p>
<p><strong>我们在特征图 (fmap) 上生成锚框 (anchors) ，每个单位(像素)
作为锚框的中心。</strong></p>
<ul>
<li>由于锚框中的<span class="math inline">\((x,y)\)</span>轴坐标值
(anchors) 已经被除以特征图 (fmap)
的宽度和高度，因此这些值介于0和1之间，表示特征图中锚框的相对位置。</li>
<li>给定特征图的宽度和高度fmap_w和fmap_h,
我们将均匀地对任何输入图像中fmap_h行和fmap_w列中的像素进行采样。以这些均匀采样的像素为中心，将会生成尺度为s
且宽高比 (ratios) 不同的锚框。</li>
</ul>
<h3 id="多尺度检测">多尺度检测</h3>
<p><strong>（1）内容</strong></p>
<p>假设CNN基于输入图像的正向传播算法获得<strong>有c张形状为ℎ×w的特征图</strong>的中间输出，既然每张特征图上都有ℎw个不同的空间位置，那么相同空间位置可以看作含有c个单元。</p>
<p>我们可以将特征图在同一空间位置的c个单元变换为使用此空间位置生成的a个锚框类别和偏移量。
本质上，我们用输入图像在某个感受野区域内的信息，来预测输入图像上与该区域位置相近的锚框类别和偏移量。</p>
<p><strong>（2）特点</strong></p>
<ul>
<li>不同层的特征图在输入图像上分别拥有不同大小的感受野，可以用于检测不同大小的目标。简言之，我们可以利用深层神经网络在多个层次上对图像进行分层表示，从而实现多尺度目标检测。</li>
</ul>
<h2 id="单发多框检测ssd">单发多框检测（SSD）</h2>
<p>单发多框检测（SSD）是一个多尺度目标检测模型。</p>
<h3 id="模型">模型</h3>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401172133548.png"
alt="单发多框检测模型主要由一个基础网络块和若干多尺度特征块串联而成" />
<figcaption
aria-hidden="true">单发多框检测模型主要由一个基础网络块和若干多尺度特征块串联而成</figcaption>
</figure>
<p>单发多框检测模型的结构如上图所示，主要由基础网络块+几个多尺度特征块（也叫高宽减半块）组成。</p>
<ul>
<li>基础网络块：用于从输入图像中提取特征，如在分类层之前截断的VGG或ResNet。</li>
<li>多尺度特征块：将上一层提供的特征图的高和宽缩小 (如减半) ,
使得特征图中每个单元在输入图像上的感受野变得更广阔。</li>
</ul>
<p>设计思路：</p>
<ol type="1">
<li>首先要让基础网络块输出的高和宽较大，这样基于该特征图生成的锚框数量较多，可以用来检测尺寸较小的目标。</li>
<li>之后的每个多尺度特征块将上一层提供的特征图的高和宽缩小 (如减半) ,
使得特征图中每个单元在输入图像上的感受野变得更广阔。</li>
<li>通过多尺度特征块，单发多框检测模型可以生成不同大小的锚框，并通过预测边界框的类别和偏移量来检测大小不同的目标，因此这是一个多尺度目标检测模型。</li>
<li>顶部的多尺度特征图较小，但具有较大的感受野，适合检测较少但较大的物体</li>
</ol>
<h3 id="类别预测层">类别预测层</h3>
<p>设目标类别的数量为<span
class="math inline">\(q\)</span>。这样一来，锚框有<span
class="math inline">\(q+1\)</span>个类别，其中0类是背景。在某个尺度下，设特征图的高和宽分别为<span
class="math inline">\(h\)</span>和<span
class="math inline">\(w\)</span>。如果以其中每个单元为中心生成<span
class="math inline">\(a\)</span>个锚框，那么我们需要对<span
class="math inline">\(hwa\)</span>个锚框进行分类。如果使用全连接层作为输出，很容易导致模型参数过多。因此单发多框检测模型（SSD）使用卷积层的通道来输出类别预测，从而降低模型复杂度。</p>
<p>具体来说，<strong>类别预测层使用一个保持输入高和宽的卷积层</strong>。这样，输出和输入在特征图宽和高上的空间坐标一一对应。也就是说，<strong>输出特征图上的（x、y）就包含了以输入特征图（x、y）为中心的所有锚框的类别预测，只不过这众多类别是通过增加通道数来保存的。</strong></p>
<h3 id="边界框预测层">边界框预测层</h3>
<p>边界框预测层的设计与类别预测层的设计类似。
唯一不同的是，这里需要为每个锚框预测4个偏移量，而不是q+1个类别。</p>
<h3 id="完整模型">完整模型</h3>
<p>完整的单发多框检测模型（SSD）由五个模块组成。每个块生成的特征图既用于生成锚框，又用于预测这些锚框的类别和偏移量。</p>
<ul>
<li><p>模块一：基本网络块</p></li>
<li><p>模块二~模块四：高和宽减半块</p></li>
<li><p>模块五：使用全局最大池将高度和宽度都降到1</p>
<p>其中，第二到第五个模块都是模型架构图中的多尺度特征块。</p></li>
</ul>
<p>与图像分类任务不同，此处的输出包括：CNN特征图<code>Y</code>；在当前尺度下根据<code>Y</code>生成的锚框；预测的这些锚框的类别和偏移量（基于<code>Y</code>）。</p>
<h3 id="损失函数">损失函数</h3>
<p>目标检测有两种类型的损失。</p>
<ul>
<li>有关锚框类别的损失：复用图像分类问题种的交叉熵损失函数；</li>
<li>有关正类锚框偏移量的损失：预测偏移量是一个回归问题。
但是，对于这个回归问题，我们在这里不使用平方损失（即L2范数），而是使用L1范数损失，即预测值和真实值之差的绝对值。
掩码变量<code>bbox_masks</code>令负类锚框和填充锚框不参与损失的计算。</li>
<li>最后，我们将锚框类别和偏移量的损失相加，以获得模型的最终损失函数。</li>
</ul>
<h3 id="评价函数">评价函数</h3>
<p>预测锚框类别：沿用<strong>准确率</strong>来评价分类结果。</p>
<p>预测边界框：由于偏移量使用了L1范数损失，我们使用<strong>平均绝对误差</strong>来评价边界框的预测结果。</p>
<h2 id="区域卷积神经网络r-cnn系列">区域卷积神经网络（R-CNN系列）</h2>
<h3 id="r-cnn">R-CNN</h3>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401172133755.png"
alt="R-CNN模型" />
<figcaption aria-hidden="true">R-CNN模型</figcaption>
</figure>
<h4 id="步骤">步骤</h4>
<ol type="1">
<li>对输入图像使用<strong>选择性搜索</strong>来选取多个高质量的提议区域。
<ul>
<li>提议区域通常是在多个尺度下选取的，并具有不同的形状和大小</li>
<li>每个提议区域都将被标注类别和真实边界框</li>
</ul></li>
<li>选择一个预训练的卷积神经网络，并将其在输出层之前截断。
<ul>
<li>将每个提议区域变形为网络需要的输入尺寸，并通过前向传播输出抽取的提议区域特征。</li>
</ul></li>
<li>将每个提议区域的特征连同其标注的类别作为一个样本。
<ul>
<li>训练多个支持向量机对目标分类，其中每个支持向量机用来判断样本是否属于某一个类别；</li>
<li>训练线性回归模型来预测真实边界框。</li>
</ul></li>
</ol>
<h4 id="优缺点">优缺点</h4>
<p>优点：R-CNN模型通过预训练的卷积神经网络有效地抽取了图像特征</p>
<p>缺点：速度很慢。
网络需要从一张图像中选出上千个提议区域，对每个提议区域都要进行一次前向传播来执行目标检测，计算量庞大。</p>
<h3 id="fast-r-cnn">Fast R-CNN</h3>
<p><strong>R-CNN的主要性能瓶颈在于，对每个提议区域，卷积神经网络的前向传播是独立的，而没有共享计算。
由于这些区域通常有重叠，独立的特征抽取会导致重复的计算。 <em>Fast
R-CNN</em>对R-CNN的主要改进之一，是仅在整张图象上执行卷积神经网络的前向传播。</strong></p>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401172133123.png"
alt="Fast R-CNN模型" />
<figcaption aria-hidden="true">Fast R-CNN模型</figcaption>
</figure>
<h4 id="步骤-1">步骤</h4>
<ol type="1">
<li><p><strong>与R-CNN相比，Fast
R-CNN用来提取特征的卷积神经网络的输入是整个图像，而不是各个提议区域。</strong>此外，这个网络通常会参与训练。设输入为一张图像，将卷积神经网络的输出的形状记为<span
class="math inline">\(1\times c\times h_1\times w_1;\)</span></p></li>
<li><p>假设选择性搜索生成了<span
class="math inline">\(n\)</span>个<strong>提议区域</strong>。</p>
<ul>
<li>首先，这些形状各异的提议区域在<u>卷积神经网络的输出上</u>分别标出了形状各异的<strong>兴趣区域</strong>。</li>
<li>然后，这些<strong>感兴趣的区域</strong>需要进一步抽取出<strong>形状相同的特征
(比如指定高度<span class="math inline">\(h_{2}\)</span>和宽度<span
class="math inline">\(w_{2})\)</span></strong>
，以便于<u>连结后输出</u>。为了实现这一目标，Fast
R-CNN引入了<strong>兴趣区域汇聚层 (Rol pooling)</strong> :
将卷积神经网络的输出和提议区域作为输入，输出连结后的各个提议区域抽取的特征，形状为<span
class="math inline">\(n\times c\times h_2\times w_2;\)</span></li>
</ul></li>
<li><p>通过全连接层将输出形状变换为<span class="math inline">\(n\times
d\)</span>， 其中超参数<span
class="math inline">\(d\)</span>取决于模型设计；</p></li>
<li><p>预测<span
class="math inline">\(n\)</span>个提议区域中每个区域的类别和边界框。</p>
<ul>
<li>在预测类别时，将全连接层的输出转换为形状为<span
class="math inline">\(n\times q\)</span>(<span
class="math inline">\(q\)</span>是类别的数量)
的输出，然后使用softmax回归。</li>
<li>在预测边界框时，将全连接层的输出转换为形状为<span
class="math inline">\(n\times4\)</span>的输出。</li>
</ul></li>
</ol>
<h4 id="主要特色">主要特色</h4>
<p>在Fast R-CNN中提出的兴趣区域汇聚层与普通的汇聚层有所不同。</p>
<ul>
<li><p>在普通汇聚层中，我们通过设置汇聚窗口、填充和步幅的大小来间接控制输出形状。</p></li>
<li><p><strong>而兴趣区域汇聚层对每个区域的输出形状是可以直接指定的。</strong></p>
<blockquote>
<p>例如，指定每个区域输出的高和宽分别为<span
class="math inline">\(h_{2}\)</span>和<span
class="math inline">\(w_{2}\)</span>。对于任何形状为<span
class="math inline">\(h\times
w\)</span>的兴趣区域窗口，该窗口将被划分为<span
class="math inline">\(h_2\times
w_2\)</span>子窗口网格，其中每个子窗口的大小约为<span
class="math inline">\((h/h_2)\times(w/w_2)\)</span>。在实践中，任何子窗口的高度和宽度都应向上取整，其中的最大元素作为该子窗口的输出。因此，兴趣区域汇聚层可从形状各异的兴趣区域中均抽取出形状相同的特征。</p>
<p>以下图为例，在<span
class="math inline">\(4\times4\)</span>的输入中，我们选取了左上角<span
class="math inline">\(3\times3\)</span>的兴趣区域。对于该兴趣区域，我们通过<span
class="math inline">\(2\times2\)</span>的兴趣区域汇聚层得到一个<span
class="math inline">\(2\times2\)</span>的输出。请注意，四个划分后的子窗口中分别含有元素0、1、4、5(5最大);2、6(6最大);8、9(9最大);以及10。</p>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401172133000.png"
alt="一个2×2的兴趣区域汇聚层" />
<figcaption aria-hidden="true">一个2×2的兴趣区域汇聚层</figcaption>
</figure>
<ul>
<li>兴趣区域窗口的形状<span
class="math inline">\(h*w=(3*3)\)</span></li>
<li>经兴趣区域汇聚后输出窗口的形状为<span
class="math inline">\(h_2*w_2=(2*2)\)</span></li>
<li>由上述两个形状推导出的池化窗口的形状（向上取整）为<span
class="math inline">\((h/h_2)*(w/w_2)=(3/2)*(3/2)=(2*2)\)</span></li>
</ul>
</blockquote></li>
</ul>
<h3 id="faster-r-cnn">Faster R-CNN</h3>
<p><strong>为了较精确地检测目标结果，Fast
R-CNN模型通常需要在选择性搜索中生成大量的提议区域。 Faster R-CNN
提出将选择性搜索替换为<em>区域提议网络</em>（region proposal
network），从而减少提议区域的生成数量，并保证目标检测的精度。</strong></p>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401172133582.png"
alt="Faster R-CNN 模型" />
<figcaption aria-hidden="true">Faster R-CNN 模型</figcaption>
</figure>
<h4 id="步骤-2">步骤</h4>
<p><strong>与Fast R-CNN相比，Faster
R-CNN只将生成提议区域的方法从选择性搜索改为了区域提议网络，模型的其余部分保持不变。</strong>模型步骤如下：</p>
<ol type="1">
<li>使用填充为1的<span
class="math inline">\(3\times3\)</span>的卷积层变换卷积神经网络的输出，并将输出通道数记为<span
class="math inline">\(c\)</span>。这样，卷积神经网络为图像抽取的特征图中的每个单元均得到一个长度为<span
class="math inline">\(c\)</span>的新特征。</li>
<li>以特征图的每个像素为中心，生成多个不同大小和宽高比的锚框并标注它们。</li>
<li>使用锚框中心单元长度为<span
class="math inline">\(c\)</span>的特征，分别预测该锚框的二元类别
(含目标还是背景) 和边界框。</li>
<li>使用非极大值抑制，从预测类别为目标的预测边界框中移除相似的结果。最终输出的预测边界框即是兴趣区域汇聚层所需的提议区域。</li>
</ol>
<h4 id="优点">优点</h4>
<p>区域提议网络（region proposal network）作为Faster
R-CNN模型的一部分，是和整个模型一起训练得到的。</p>
<p>换句话说，Faster
R-CNN的目标函数不仅包括目标检测中的类别和边界框预测，还包括区域提议网络中锚框的二元类别和边界框预测。</p>
<p>作为端到端训练的结果，<strong>区域提议网络能够学习到如何生成高质量的提议区域，从而在减少了从数据中学习的提议区域的数量的情况下，仍保持目标检测的精度。</strong></p>
<h3 id="mask-rcnn">Mask-RCNN</h3>
<p><strong>如果在训练集中还标注了每个目标在图像上的像素级位置</strong>，那么<em>Mask
R-CNN</em>
能够有效地利用这些详尽的标注信息进一步提升目标检测的精度。</p>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401172133205.png"
alt="Mask R-CNN模型" />
<figcaption aria-hidden="true">Mask R-CNN模型</figcaption>
</figure>
<p>Mask R-CNN是基于Faster R-CNN修改而来的。</p>
<p>具体来说，<strong>Mask R-CNN将兴趣区域汇聚层替换为了
<em>兴趣区域对齐</em> 层</strong>，使用<em>双线性插值</em>（bilinear
interpolation）来保留特征图上的空间信息，从而更适于像素级预测。</p>
<ul>
<li>兴趣区域对齐层的输出包含了所有与兴趣区域的形状相同的特征图。
它们不仅被用于预测每个兴趣区域的类别和边界框，还通过额外的全卷积网络预测目标的像素级位置。</li>
</ul>
<h3 id="总结">总结</h3>
<ul>
<li>R-CNN对图像选取若干提议区域，使用卷积神经网络对每个提议区域执行前向传播以抽取其特征，然后再用这些特征来预测提议区域的类别和边界框。</li>
<li>Fast
R-CNN对R-CNN的一个主要改进：只对整个图像做卷积神经网络的前向传播。
<ul>
<li>它还引入了兴趣区域汇聚层，从而为具有不同形状的兴趣区域抽取相同形状的特征。</li>
</ul></li>
<li>Faster R-CNN将Fast
R-CNN中使用的选择性搜索替换为参与训练的区域提议网络，这样后者可以在减少提议区域数量的情况下仍保证目标检测的精度。</li>
<li>Mask R-CNN在Faster
R-CNN的基础上引入了一个全卷积网络，从而借助目标的像素级位置进一步提升目标检测的精度。</li>
</ul>
<h2 id="语义分割和数据集">语义分割和数据集</h2>
<p><em>语义分割</em>（semantic
segmentation）问题重点关注<strong>如何将图像分割成属于不同语义类别的区域</strong>。</p>
<ul>
<li>与目标检测不同，<strong>语义分割可以识别并理解图像中每一个像素的内容</strong>：其语义区域的标注和预测是像素级的。</li>
<li>与目标检测相比，语义分割标注的像素级的边框显然更加精细。</li>
</ul>
<p>下图展示了语义分割中图像有关狗、猫和背景的标签。</p>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401172133094.png"
alt="语义分割中图像有关狗、猫和背景的标签" />
<figcaption
aria-hidden="true">语义分割中图像有关狗、猫和背景的标签</figcaption>
</figure>
<h3 id="概念区分">概念区分</h3>
<h4 id="图像分割">图像分割</h4>
<blockquote>
<p>将图像划分为若干组成区域，这类问题的方法通常利用图像中像素之间的相关性。它在训练时不需要有关图像像素的标签信息，在预测时也无法保证分割出的区域具有我们希望得到的语义。以上图中的图像作为输入，图像分割可能会将狗分为两个区域：一个覆盖以黑色为主的嘴和眼睛，另一个覆盖以黄色为主的其余部分身体。</p>
</blockquote>
<h4 id="语义分割">语义分割</h4>
<h4 id="实例分割">实例分割</h4>
<blockquote>
<p><em>实例分割</em>也叫<em>同时检测并分割</em>（simultaneous detection
and
segmentation），它研究如何识别图像中各个目标实例的像素级区域。与语义分割不同，实例分割不仅需要区分语义，还要区分不同的目标实例。例如，如果图像中有两条狗，则实例分割需要区分像素属于的两条狗中的哪一条。</p>
</blockquote>
<p><strong>由图像分割、到语义分割，再到实例分割，对分割任务的要求越来越高。</strong></p>
<h3 id="数据集">数据集</h3>
<p>最重要的语义分割数据集之一是<strong>Pascal VOC2012</strong>。</p>
<h4 id="预处理数据">预处理数据</h4>
<p>为了使输入图像符合模型的输入形状，我们可以对原始图像做一些预处理——<strong>缩放图像
or 裁剪图像</strong>。</p>
<blockquote>
<p>在语义分割任务中，如果采用缩放原始图像的方式，在预测时需要将预测的像素类别重新映射回原始尺寸的输入图像。这样的映射可能不够精确，尤其在不同语义的分割区域。</p>
<p>为了避免这个问题，我们将图像裁剪为固定尺寸，而不是缩放。具体来说，我们使用图像增广中的随机裁剪，裁剪输入图像和标签的相同区域。</p>
</blockquote>
<h4 id="自定义语义分割数据集类">自定义语义分割数据集类</h4>
<p>通过继承高级API提供的<code>Dataset</code>类，自定义了一个语义分割数据集类<code>VOCSegDataset</code>。
通过实现<code>__getitem__</code>函数，我们可以任意访问数据集中索引为<code>idx</code>的输入图像及其每个像素的类别索引。
由于数据集中有些图像的尺寸可能小于随机裁剪所指定的输出尺寸，这些样本可以通过自定义的<code>filter</code>函数移除掉。
此外，我们还定义了<code>normalize_image</code>函数，从而对输入图像的RGB三个通道的值分别做标准化。</p>
<h4 id="读取数据集">读取数据集</h4>
<h2 id="转置卷积">转置卷积</h2>
<blockquote>
<p>补充一些基础：</p>
<p>下采样：</p>
<ul>
<li>定义：指在进行一些操作时（如卷积操作或汇聚操作）降低图像的空间分辨率（高和宽）。</li>
<li>目的：减少计算量和模型参数的数量，也有助于将低层细节特征抽象为高层语义特征。</li>
<li>典型例子：卷积神经网络中的最大池化（Max pooling）。</li>
</ul>
<p>上采样：</p>
<ul>
<li>定义：指进行一些操作时（如转置卷积）增大图像的空间分辨率。</li>
<li>目的：在一些需要更精细处理图像的任务中，对中间层或低层的特征图进行上采样操作，以获得更多的空间信息或恢复部分丢失的空间信息。</li>
<li>主要用于语义分割、超分辨率等任务。</li>
</ul>
</blockquote>
<h3 id="基本操作">基本操作</h3>
<p>到目前为止，我们所见到的卷积神经网络层，例如卷积层和汇聚层，通常会减少下采样输入图像的空间维度（高和宽）。然而<strong>如果输入和输出图像的空间维度相同，在以像素级分类的语义分割中将会很方便。
例如，输出像素所处的通道维可以保有输入像素在同一位置上的分类结果。</strong></p>
<p>为了实现这一点，尤其是在空间维度被卷积神经网络层缩小后，我们可以使用另一种类型的卷积神经网络层，它可以增<u>加上采样中间层特征图的空间维度</u>。</p>
<p><strong><em>转置卷积</em>（transposed
convolution）可以将较小的特征图上采样为较大的尺寸，从而逆转下采样操作导致的空间尺寸减小。</strong></p>
<p>下图解释了如何为2×2的输入张量计算卷积核为2×2的转置卷积，设步幅为1且没有填充。</p>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401172134512.png"
alt="卷积核为2×2的转置卷积。阴影部分是中间张量的一部分，也是用于计算的输入和卷积核张量元素" />
<figcaption
aria-hidden="true">卷积核为2×2的转置卷积。阴影部分是中间张量的一部分，也是用于计算的输入和卷积核张量元素</figcaption>
</figure>
<blockquote>
<p>与通过卷积核“减少”输入元素的常规卷积相比，转置卷积通过卷积核“广播”输入元素，从而产生大于输入的输出。</p>
</blockquote>
<h3 id="填充步幅和多通道">填充、步幅和多通道</h3>
<h4 id="填充">填充</h4>
<p>与常规卷积不同，在转置卷积中，填充被应用于的输出（常规卷积将填充应用于输入）。</p>
<ul>
<li>例如，当将高和宽两侧的填充数指定为1时，转置卷积的输出中将删除第一和最后的行与列。</li>
</ul>
<h4 id="步幅">步幅</h4>
<p>在转置卷积中，步幅被指定为中间结果（输出），而不是输入。</p>
<p>使用上图中相同输入和卷积核张量，将步幅从1更改为2会增加中间张量的高和权重，因此输出张量如下图所示</p>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401172134447.png"
alt="卷积核为2×2，步幅为2的转置卷积。阴影部分是中间张量的一部分，也是用于计算的输入和卷积核张量元素。" />
<figcaption
aria-hidden="true">卷积核为2×2，步幅为2的转置卷积。阴影部分是中间张量的一部分，也是用于计算的输入和卷积核张量元素。</figcaption>
</figure>
<h4 id="多通道">多通道</h4>
<p>对于多个输入和输出通道，转置卷积与常规卷积以相同方式运作。假设输入有<span
class="math inline">\(c_i\)</span>个通道，且转置卷积为每个输入通道分配了一个<span
class="math inline">\(k_h\times
k_w\)</span>的卷积核张量。当指定多个输出通道时，每个输出通道将有一个<span
class="math inline">\(c_i\times k_h\times k_w\)</span>的卷积核。</p>
<h2 id="全卷积网络">全卷积网络</h2>
<p>语义分割是对图像中的每个像素分类。</p>
<p><em>全卷积网络</em>（fully convolutional
network，FCN）采用卷积神经网络实现了从图像像素到像素类别的变换。</p>
<p>与我们之前在图像分类或目标检测部分介绍的卷积神经网络不同，<strong>全卷积网络将中间层特征图的高和宽变换回输入图像的尺寸：这是通过引入转置卷积（transposed
convolution）实现的</strong>。</p>
<p><strong>因此，输出的类别预测与输入图像在像素级别上具有一一对应关系：通道维的输出即该位置对应像素的类别预测。</strong></p>
<p>全卷积网络模型如下图所示：</p>
<ol type="1">
<li><p>首先使用卷积神经网络抽取图像特征</p></li>
<li><p>然后通过1×1卷积层将通道数变换为类别个数</p></li>
<li><p>最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸</p>
<p>因此，模型输出与输入图像的高和宽相同，且最终输出通道包含了该空间位置像素的类别预测。</p></li>
</ol>
<figure>
<img
src="https://raw.githubusercontent.com/ValoraY/blog-imgs/main/img/202401172134666.png"
alt="全卷积网络" />
<figcaption aria-hidden="true">全卷积网络</figcaption>
</figure>
<h3 id="初始化转置卷积层">初始化转置卷积层</h3>
<p>应用<em>双线性插值</em></p>
<p>为了解释双线性插值，假设给定输入图像，我们想要计算上采样输出图像上的每个像素。</p>
<ol type="1">
<li><p>将输出图像的坐标<span
class="math inline">\((x,y)\)</span>映射到输入图像的坐标<span
class="math inline">\((x^{\prime},y^{\prime})\)</span>上。例如，根据输入与输出的尺寸之比来映射。请注意，映射后的<span
class="math inline">\(x\prime\)</span>和<span
class="math inline">\(y\prime\)</span>是实数。</p></li>
<li><p>在输入图像上找到离坐标<span
class="math inline">\((x^{\prime},y^{\prime})\)</span>最近的4个像素。</p></li>
<li><p>输出图像在坐标<span
class="math inline">\((x,y)\)</span>上的像素依据输入图像上这4个像素及其与<span
class="math inline">\((x^{\prime},y^{\prime})\)</span>的相对距离来计算。</p></li>
</ol>
<h2 id="风格迁移">风格迁移</h2>
<p>（需要用到再学习）</p>
]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>动手学深度学习</tag>
      </tags>
  </entry>
</search>
